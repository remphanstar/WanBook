{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Cell (1) WAN2GP Setup Introduction & Enhanced Platform Detection v4.0 (System Compliant)\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import subprocess\n",
        "from IPython.display import display, HTML, Markdown\n",
        "\n",
        "def jupyter_lab_title(title):\n",
        "    if \"google.colab\" not in sys.modules:\n",
        "        display(Markdown(f\"## {title}\"))\n",
        "\n",
        "jupyter_lab_title(\"WAN2GP Setup Introduction & Enhanced Platform Detection\")\n",
        "\n",
        "def detect_platform():\n",
        "    \"\"\"Enhanced platform detection with comprehensive indicators\"\"\"\n",
        "\n",
        "    # Lightning AI detection - multiple indicators for reliability\n",
        "    lightning_indicators = [\n",
        "        \"lightning\" in str(sys.executable).lower(),\n",
        "        \"teamspace-studios\" in os.getcwd(),\n",
        "        \"LIGHTNING_CLOUDSPACE_HOST\" in os.environ,\n",
        "        \"LIGHTNING_CLOUDSPACE_ID\" in os.environ,\n",
        "        \"commands/python\" in str(sys.executable),\n",
        "        \"/home/zeus/miniconda3/envs/cloudspace\" in str(sys.executable),\n",
        "        os.path.exists(\"/teamspace\"),\n",
        "        os.path.exists(\"/commands\")\n",
        "    ]\n",
        "\n",
        "    # Google Colab detection\n",
        "    colab_indicators = [\n",
        "        \"google.colab\" in sys.modules,\n",
        "        \"/content\" in os.getcwd()\n",
        "    ]\n",
        "\n",
        "    # Vast.AI detection\n",
        "    vast_indicators = [\n",
        "        \"VAST_CONTAINER_LABEL\" in os.environ,\n",
        "        \"/workspace\" in os.getcwd(),\n",
        "        \"vast\" in os.environ.get(\"HOSTNAME\", \"\").lower()\n",
        "    ]\n",
        "\n",
        "    if any(lightning_indicators):\n",
        "        return \"Lightning AI\"\n",
        "    elif any(colab_indicators):\n",
        "        return \"Google Colab\"\n",
        "    elif any(vast_indicators):\n",
        "        return \"Vast.AI/Generic\"\n",
        "    else:\n",
        "        return \"Vast.AI/Generic\"\n",
        "\n",
        "def get_platform_commands(platform):\n",
        "    \"\"\"Get platform-specific pip and python commands - ENFORCED VENV FOR COLAB\"\"\"\n",
        "    if platform == \"Lightning AI\":\n",
        "        return \"pip\", \"python\", False  # (pip_cmd, python_cmd, use_venv)\n",
        "    elif platform == \"Google Colab\":\n",
        "        # ENFORCED: Always use venv for Google Colab per system instructions\n",
        "        return \".venv/bin/pip\", \".venv/bin/python\", True\n",
        "    else:  # Vast.AI/Generic\n",
        "        return \".venv/bin/pip\", \".venv/bin/python\", True\n",
        "\n",
        "# Detect current platform\n",
        "current_platform = detect_platform()\n",
        "pip_cmd, python_cmd, use_venv = get_platform_commands(current_platform)\n",
        "\n",
        "# Display platform information\n",
        "display(HTML(f\"\"\"\n",
        "<div style=\"font-family: Arial, sans-serif; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "           color: white; padding: 20px; border-radius: 10px; text-align: center; margin: 10px 0;\n",
        "           box-shadow: 0 4px 15px rgba(0,0,0,0.2);\">\n",
        "    <h1>ğŸš€ Wan2GP Full Setup Notebook</h1>\n",
        "    <p>A cross-platform Jupyter notebook (Colab, Lightning AI, Vast.ai) that installs Wan2GP,\n",
        "       common LoRA packs, and optional performance extras (FlashAttention 2, SageAttention, xFormers).\n",
        "       It accelerates all downloads with <strong>aria2c</strong> for maximum speed.</p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background-color: #f8f9fa; padding: 20px; border-radius: 8px; border: 1px solid #dee2e6; margin: 15px 0;\">\n",
        "    <div style=\"background-color: #007bff; color: white; padding: 15px; border-radius: 5px; text-align: center; margin-bottom: 20px;\">\n",
        "        <h3 style=\"margin: 0; color: white;\">ğŸ¯ Platform Detection Results</h3>\n",
        "        <p style=\"margin: 5px 0 0 0; color: white;\">Currently Running on: <strong style=\"color: #28a745; background-color: white; padding: 2px 6px; border-radius: 3px;\">{current_platform}</strong></p>\n",
        "        <p style=\"margin: 5px 0 0 0; color: white;\">Virtual Environment: <strong>{\"Yes\" if use_venv else \"No (Lightning AI)\"}</strong></p>\n",
        "        <p style=\"margin: 5px 0 0 0; color: white;\">Pip Command: <strong>{pip_cmd}</strong></p>\n",
        "        <p style=\"margin: 5px 0 0 0; color: white;\">Python Command: <strong>{python_cmd}</strong></p>\n",
        "    </div>\n",
        "</div>\n",
        "\"\"\"))\n",
        "\n",
        "print(\"ğŸ“‹ Platform Detection Debug Info:\")\n",
        "print(f\"ğŸ” - Detected Platform: {current_platform}\")\n",
        "print(f\"ğŸ - Python Executable: {sys.executable}\")\n",
        "print(f\"ğŸ“ - Current Working Directory: {os.getcwd()}\")\n",
        "print(f\"ğŸŒ - Google Colab Check: {'google.colab' in sys.modules}\")\n",
        "print(f\"âš¡ - Lightning Environment Variables: {[key for key in os.environ.keys() if 'LIGHTNING' in key]}\")\n",
        "print(f\"ğŸ”§ - Virtual Environment Usage: {use_venv}\")\n",
        "print(f\"ğŸ“¦ - Pip Command: {pip_cmd}\")\n",
        "print(f\"ğŸ - Python Command: {python_cmd}\")\n",
        "\n",
        "# CRITICAL: Enforce venv for Google Colab per system instructions\n",
        "if current_platform == \"Google Colab\" and not use_venv:\n",
        "    print(\"ğŸš¨ CRITICAL: Detected Google Colab but venv is disabled - FORCE ENABLING\")\n",
        "    use_venv = True\n",
        "    pip_cmd = \".venv/bin/pip\"\n",
        "    python_cmd = \".venv/bin/python\"\n",
        "    print(\"âœ… ENFORCED: Google Colab now using venv commands\")\n",
        "elif current_platform == \"Google Colab\":\n",
        "    print(\"âœ… CONFIRMED: Google Colab correctly configured for venv usage\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Q7hLV1Oz-aBD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "e25800d9-6874-4d3a-fc59-be6b7219ed58"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<div style=\"font-family: Arial, sans-serif; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
              "           color: white; padding: 20px; border-radius: 10px; text-align: center; margin: 10px 0;\n",
              "           box-shadow: 0 4px 15px rgba(0,0,0,0.2);\">\n",
              "    <h1>ğŸš€ Wan2GP Full Setup Notebook</h1>\n",
              "    <p>A cross-platform Jupyter notebook (Colab, Lightning AI, Vast.ai) that installs Wan2GP,\n",
              "       common LoRA packs, and optional performance extras (FlashAttention 2, SageAttention, xFormers).\n",
              "       It accelerates all downloads with <strong>aria2c</strong> for maximum speed.</p>\n",
              "</div>\n",
              "\n",
              "<div style=\"background-color: #f8f9fa; padding: 20px; border-radius: 8px; border: 1px solid #dee2e6; margin: 15px 0;\">\n",
              "    <div style=\"background-color: #007bff; color: white; padding: 15px; border-radius: 5px; text-align: center; margin-bottom: 20px;\">\n",
              "        <h3 style=\"margin: 0; color: white;\">ğŸ¯ Platform Detection Results</h3>\n",
              "        <p style=\"margin: 5px 0 0 0; color: white;\">Currently Running on: <strong style=\"color: #28a745; background-color: white; padding: 2px 6px; border-radius: 3px;\">Google Colab</strong></p>\n",
              "        <p style=\"margin: 5px 0 0 0; color: white;\">Virtual Environment: <strong>Yes</strong></p>\n",
              "        <p style=\"margin: 5px 0 0 0; color: white;\">Pip Command: <strong>.venv/bin/pip</strong></p>\n",
              "        <p style=\"margin: 5px 0 0 0; color: white;\">Python Command: <strong>.venv/bin/python</strong></p>\n",
              "    </div>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‹ Platform Detection Debug Info:\n",
            "ğŸ” - Detected Platform: Google Colab\n",
            "ğŸ - Python Executable: /usr/bin/python3\n",
            "ğŸ“ - Current Working Directory: /content\n",
            "ğŸŒ - Google Colab Check: True\n",
            "âš¡ - Lightning Environment Variables: []\n",
            "ğŸ”§ - Virtual Environment Usage: True\n",
            "ğŸ“¦ - Pip Command: .venv/bin/pip\n",
            "ğŸ - Python Command: .venv/bin/python\n",
            "âœ… CONFIRMED: Google Colab correctly configured for venv usage\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell (2) Complete System + Python Environment Setup - Cross-Platform v4.0 (System Compliant)\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "def jupyter_lab_title(title):\n",
        "    if \"google.colab\" not in sys.modules:\n",
        "        from IPython.display import display, Markdown\n",
        "        display(Markdown(f\"## {title}\"))\n",
        "\n",
        "jupyter_lab_title(\"Complete System + Python Environment Setup\")\n",
        "\n",
        "class CompleteEnvironmentSetup:\n",
        "    def __init__(self):\n",
        "        # Use global platform configuration from Cell 1\n",
        "        try:\n",
        "            self.current_platform = current_platform\n",
        "            self.pip_cmd = pip_cmd\n",
        "            self.python_cmd = python_cmd\n",
        "            self.use_venv = use_venv\n",
        "        except NameError:\n",
        "            print(\"âš ï¸ Platform detection variables not found. Using fallback detection.\")\n",
        "            self.detect_platform_fallback()\n",
        "\n",
        "        # CRITICAL ENFORCEMENT: Ensure venv for Google Colab per system instructions\n",
        "        self.enforce_colab_venv()\n",
        "\n",
        "        self.setup_phases = [\n",
        "            (\"System Packages\", self.install_system_packages),\n",
        "            (\"Directory Protection\", self.setup_directory_protection),\n",
        "            (\"Repository Clone\", self.clone_repository),\n",
        "            (\"Virtual Environment\", self.setup_virtual_environment),\n",
        "            (\"Matplotlib Fix\", self.fix_matplotlib_backend),\n",
        "            (\"PyTorch Installation\", self.install_pytorch),\n",
        "            (\"Requirements\", self.install_requirements),\n",
        "            (\"Environment Verification\", self.verify_complete_setup),\n",
        "        ]\n",
        "\n",
        "        self.current_phase = 0\n",
        "        self.total_phases = len(self.setup_phases)\n",
        "\n",
        "    def enforce_colab_venv(self):\n",
        "        \"\"\"CRITICAL: Enforce venv usage for Google Colab per system instructions\"\"\"\n",
        "        if hasattr(self, 'current_platform') and self.current_platform == \"Google Colab\":\n",
        "            if not hasattr(self, 'use_venv') or not self.use_venv or self.pip_cmd != \".venv/bin/pip\":\n",
        "                print(\"ğŸš¨ ENFORCING: Google Colab MUST use venv per system instructions\")\n",
        "                self.use_venv = True\n",
        "                self.pip_cmd = \".venv/bin/pip\"\n",
        "                self.python_cmd = \".venv/bin/python\"\n",
        "                print(\"âœ… ENFORCED: System setup using venv commands\")\n",
        "            else:\n",
        "                print(\"âœ… VERIFIED: System setup correctly using venv for Colab\")\n",
        "\n",
        "    def detect_platform_fallback(self):\n",
        "        \"\"\"Fallback platform detection - ENFORCES VENV FOR COLAB\"\"\"\n",
        "        if \"google.colab\" in sys.modules:\n",
        "            self.current_platform = \"Google Colab\"\n",
        "            # ENFORCED: Always venv for Colab per system instructions\n",
        "            self.pip_cmd, self.python_cmd, self.use_venv = \".venv/bin/pip\", \".venv/bin/python\", True\n",
        "        elif any([\"lightning\" in str(sys.executable).lower(), \"teamspace\" in os.getcwd()]):\n",
        "            self.current_platform = \"Lightning AI\"\n",
        "            self.pip_cmd, self.python_cmd, self.use_venv = \"pip\", \"python\", False\n",
        "        else:\n",
        "            self.current_platform = \"Generic\"\n",
        "            self.pip_cmd, self.python_cmd, self.use_venv = \".venv/bin/pip\", \".venv/bin/python\", True\n",
        "\n",
        "    def update_progress(self, phase_name, status=\"in_progress\"):\n",
        "        \"\"\"Update setup progress with visual indicators\"\"\"\n",
        "        progress_percent = (self.current_phase / self.total_phases) * 100\n",
        "        status_icons = {\n",
        "            \"in_progress\": \"ğŸ”„\",\n",
        "            \"success\": \"âœ…\",\n",
        "            \"failed\": \"âŒ\"\n",
        "        }\n",
        "\n",
        "        print(f\"\\n[{self.current_phase + 1}/{self.total_phases}] {status_icons[status]} {phase_name}\")\n",
        "        print(f\"Progress: {progress_percent:.1f}% | Platform: {self.current_platform}\")\n",
        "        print(f\"ğŸ”§ Using: {self.pip_cmd} | {self.python_cmd} | venv: {self.use_venv}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    def run_command_safely(self, command, description, timeout=300):\n",
        "        \"\"\"Execute command with comprehensive error handling\"\"\"\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                command,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=timeout\n",
        "            )\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print(f\"âœ… {description} completed successfully\")\n",
        "                return True, result.stdout\n",
        "            else:\n",
        "                print(f\"âŒ {description} failed (exit code {result.returncode})\")\n",
        "                if result.stderr:\n",
        "                    # Check for specific ensurepip failure\n",
        "                    if \"ensurepip\" in result.stderr:\n",
        "                        print(f\"âš ï¸ ensurepip module failure detected\")\n",
        "                        return \"ensurepip_failure\", result.stderr\n",
        "                    print(f\"   Error: {result.stderr.strip()[:200]}\")\n",
        "                return False, result.stderr\n",
        "\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(f\"â° {description} timed out after {timeout}s\")\n",
        "            return False, f\"Timeout after {timeout}s\"\n",
        "        except Exception as e:\n",
        "            print(f\"ğŸ’¥ {description} crashed: {e}\")\n",
        "            return False, str(e)\n",
        "\n",
        "    def install_system_packages(self):\n",
        "        \"\"\"Install system packages - same for all platforms\"\"\"\n",
        "        print(\"ğŸ”§ Installing system packages...\")\n",
        "\n",
        "        if not shutil.which(\"apt-get\"):\n",
        "            print(\"ğŸ“¦ apt-get not found. Skipping system package installation.\")\n",
        "            return True\n",
        "\n",
        "        try:\n",
        "            print(\"ğŸ“¦ Updating package lists...\")\n",
        "            subprocess.run([\"sudo\", \"apt-get\", \"update\", \"-qq\"], check=True, timeout=60)\n",
        "\n",
        "            print(\"ğŸ”§ Installing aria2, git, build-essential, and wget...\")\n",
        "            subprocess.run([\"sudo\", \"apt-get\", \"install\", \"-y\", \"aria2\", \"git\", \"build-essential\", \"wget\"], check=True, timeout=180)\n",
        "\n",
        "            print(\"âœ… System packages installed successfully\")\n",
        "            return True\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"âŒ System package installation failed: {e}\")\n",
        "            return False\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"â° System package installation timed out\")\n",
        "            return False\n",
        "\n",
        "    def setup_directory_protection(self):\n",
        "        \"\"\"Critical directory protection to prevent recursive cloning\"\"\"\n",
        "        print(\"ğŸ›¡ï¸ Setting up directory protection...\")\n",
        "\n",
        "        current_dir = os.getcwd()\n",
        "        print(f\"ğŸ“ Current directory: {current_dir}\")\n",
        "\n",
        "        # Check if we're already inside a Wan2GP directory\n",
        "        if \"Wan2GP\" in current_dir:\n",
        "            print(\"âš ï¸ Already inside Wan2GP directory - navigating to safe location\")\n",
        "\n",
        "            # Navigate to content root (Colab) or appropriate base directory\n",
        "            if self.current_platform == \"Google Colab\":\n",
        "                safe_dir = \"/content\"\n",
        "            elif self.current_platform == \"Lightning AI\":\n",
        "                safe_dir = os.path.expanduser(\"~\")\n",
        "            else:\n",
        "                safe_dir = os.path.expanduser(\"~\")\n",
        "\n",
        "            try:\n",
        "                os.chdir(safe_dir)\n",
        "                print(f\"ğŸ“ Moved to safe directory: {os.getcwd()}\")\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Failed to navigate to safe directory: {e}\")\n",
        "                return False\n",
        "\n",
        "        # Check for existing Wan2GP directory and handle it\n",
        "        repo_path = os.path.join(os.getcwd(), \"Wan2GP\")\n",
        "        if os.path.exists(repo_path):\n",
        "            print(f\"ğŸ“ Found existing Wan2GP directory at: {repo_path}\")\n",
        "\n",
        "            # Check if it looks like a valid repository\n",
        "            if os.path.exists(os.path.join(repo_path, \".git\")):\n",
        "                print(\"âœ… Existing directory appears to be a valid git repository\")\n",
        "                # Check if it's the correct repository\n",
        "                try:\n",
        "                    os.chdir(repo_path)\n",
        "                    result = subprocess.run([\"git\", \"remote\", \"get-url\", \"origin\"],\n",
        "                                          capture_output=True, text=True, timeout=10)\n",
        "                    if result.returncode == 0 and \"Wan2GP\" in result.stdout:\n",
        "                        print(\"âœ… Existing repository is correct - using it\")\n",
        "                        return True\n",
        "                    else:\n",
        "                        print(\"âš ï¸ Existing repository is not the correct one\")\n",
        "                        os.chdir(\"..\")\n",
        "                except Exception as e:\n",
        "                    print(f\"âš ï¸ Could not verify existing repository: {e}\")\n",
        "                    os.chdir(\"..\")\n",
        "\n",
        "            # If we reach here, remove the problematic directory\n",
        "            print(\"ğŸ—‘ï¸ Removing problematic existing directory...\")\n",
        "            try:\n",
        "                shutil.rmtree(repo_path)\n",
        "                print(\"âœ… Problematic directory removed\")\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Failed to remove existing directory: {e}\")\n",
        "                # Try to rename it instead\n",
        "                try:\n",
        "                    backup_name = f\"Wan2GP_backup_{int(time.time())}\"\n",
        "                    os.rename(repo_path, backup_name)\n",
        "                    print(f\"âœ… Renamed problematic directory to {backup_name}\")\n",
        "                except Exception as e2:\n",
        "                    print(f\"âŒ Could not even rename directory: {e2}\")\n",
        "                    return False\n",
        "\n",
        "        print(\"âœ… Directory protection setup complete\")\n",
        "        return True\n",
        "\n",
        "    def clone_repository(self):\n",
        "        \"\"\"Clone WAN2GP repository with enhanced protection\"\"\"\n",
        "        REPO_DIR = \"Wan2GP\"\n",
        "        REPO_URL = \"https://github.com/deepbeepmeep/Wan2GP.git\"\n",
        "\n",
        "        print(\"ğŸ“‚ Setting up WAN2GP repository...\")\n",
        "        print(f\"ğŸ“ Working from: {os.getcwd()}\")\n",
        "\n",
        "        # Final check - ensure we're not in a nested situation\n",
        "        current_path = os.getcwd()\n",
        "        if current_path.count(\"Wan2GP\") > 0:\n",
        "            print(\"ğŸš¨ CRITICAL: Still in Wan2GP directory path!\")\n",
        "            print(\"ğŸ”„ Attempting emergency navigation...\")\n",
        "\n",
        "            # Emergency navigation\n",
        "            if self.current_platform == \"Google Colab\":\n",
        "                emergency_path = \"/content\"\n",
        "            else:\n",
        "                emergency_path = os.path.expanduser(\"~\")\n",
        "\n",
        "            try:\n",
        "                os.chdir(emergency_path)\n",
        "                print(f\"ğŸ†˜ Emergency navigation successful: {os.getcwd()}\")\n",
        "            except Exception as e:\n",
        "                print(f\"ğŸ’¥ Emergency navigation failed: {e}\")\n",
        "                return False\n",
        "\n",
        "        if not os.path.exists(REPO_DIR):\n",
        "            print(f\"ğŸ“¥ Cloning repository from {REPO_URL}...\")\n",
        "            try:\n",
        "                subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", REPO_URL], check=True, timeout=120)\n",
        "                print(\"âœ… Repository cloned successfully\")\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                print(f\"âŒ Failed to clone repository: {e}\")\n",
        "                return False\n",
        "            except subprocess.TimeoutExpired:\n",
        "                print(\"â° Repository clone timed out\")\n",
        "                return False\n",
        "        else:\n",
        "            print(f\"ğŸ“ Directory {REPO_DIR} already exists\")\n",
        "\n",
        "        try:\n",
        "            os.chdir(REPO_DIR)\n",
        "            final_path = os.getcwd()\n",
        "            print(f\"ğŸ“ Changed to directory: {final_path}\")\n",
        "\n",
        "            # Verify we're in the right place\n",
        "            if final_path.count(\"Wan2GP\") > 1:\n",
        "                print(\"ğŸš¨ WARNING: Detected nested Wan2GP directories!\")\n",
        "                print(f\"ğŸ” Current path: {final_path}\")\n",
        "                return False\n",
        "\n",
        "            print(\"âœ… Repository setup verified\")\n",
        "            return True\n",
        "        except FileNotFoundError:\n",
        "            print(f\"âŒ Directory {REPO_DIR} not found\")\n",
        "            return False\n",
        "\n",
        "    def setup_venv_manual_pip(self):\n",
        "        \"\"\"Create venv without pip, then install pip manually\"\"\"\n",
        "        try:\n",
        "            print(\"ğŸ”§ Creating venv without pip...\")\n",
        "            subprocess.run([sys.executable, \"-m\", \"venv\", \".venv\", \"--without-pip\"],\n",
        "                          check=True, timeout=60)\n",
        "\n",
        "            print(\"ğŸ“¥ Downloading get-pip.py...\")\n",
        "            subprocess.run([\"wget\", \"https://bootstrap.pypa.io/get-pip.py\"], check=True)\n",
        "\n",
        "            print(\"ğŸ”§ Installing pip manually...\")\n",
        "            subprocess.run([\".venv/bin/python\", \"get-pip.py\"], check=True)\n",
        "\n",
        "            print(\"âœ… Virtual environment with manual pip created successfully\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Manual pip installation failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def setup_virtual_environment(self):\n",
        "        \"\"\"Create virtual environment - ENFORCED for Google Colab\"\"\"\n",
        "        if not self.use_venv:\n",
        "            print(\"âš¡ Lightning AI detected - using system environment (no venv)\")\n",
        "            return True\n",
        "\n",
        "        print(\"ğŸ Setting up virtual environment...\")\n",
        "        print(f\"ğŸ”§ Platform: {self.current_platform} - venv REQUIRED per system instructions\")\n",
        "\n",
        "        # Check if venv already exists\n",
        "        if os.path.exists(\".venv/bin/python\"):\n",
        "            print(\"âœ… Virtual environment already exists and appears functional\")\n",
        "            return True\n",
        "\n",
        "        print(\"ğŸ”¨ Creating virtual environment...\")\n",
        "\n",
        "        # Try multiple methods for virtual environment creation\n",
        "        venv_methods = [\n",
        "            ([sys.executable, \"-m\", \"venv\", \"--system-site-packages\", \".venv\"], \"venv with system packages\"),\n",
        "            ([sys.executable, \"-m\", \"venv\", \".venv\"], \"standard venv\"),\n",
        "            ([sys.executable, \"-m\", \"virtualenv\", \".venv\"], \"virtualenv package\"),\n",
        "        ]\n",
        "\n",
        "        for command, method_name in venv_methods:\n",
        "            print(f\"ğŸ”„ Trying: {method_name}\")\n",
        "            success, error_msg = self.run_command_safely(command, f\"Virtual environment creation ({method_name})\", timeout=60)\n",
        "\n",
        "            if success == True:\n",
        "                print(\"âœ… Virtual environment created successfully\")\n",
        "                return True\n",
        "            elif success == \"ensurepip_failure\":\n",
        "                print(\"âš ï¸ ensurepip failure detected - trying manual pip installation...\")\n",
        "                if self.setup_venv_manual_pip():\n",
        "                    return True\n",
        "                break  # Don't try other methods if ensurepip failed\n",
        "            else:\n",
        "                print(f\"âŒ {method_name} failed\")\n",
        "                if error_msg:\n",
        "                    print(f\"   Error: {error_msg[:200]}\")\n",
        "                continue\n",
        "\n",
        "        # Final fallback ONLY for non-Colab platforms per system instructions\n",
        "        if self.current_platform != \"Google Colab\":\n",
        "            print(\"ğŸ”„ All virtual environment methods failed - switching to system installation\")\n",
        "            print(\"âš¡ Overriding to system-wide installation\")\n",
        "            self.use_venv = False\n",
        "            self.pip_cmd = \"pip\"\n",
        "            self.python_cmd = \"python\"\n",
        "            return True\n",
        "        else:\n",
        "            print(\"ğŸš¨ CRITICAL: Google Colab REQUIRES venv per system instructions - cannot fallback\")\n",
        "            return False\n",
        "\n",
        "    def fix_matplotlib_backend(self):\n",
        "        \"\"\"Fix matplotlib backend issue for Colab environment\"\"\"\n",
        "        print(\"ğŸ¨ Fixing matplotlib backend compatibility...\")\n",
        "\n",
        "        try:\n",
        "            # Create matplotlib configuration fix\n",
        "            matplotlib_fix_script = \"\"\"\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Fix matplotlib backend for Colab/Jupyter environments\n",
        "# This prevents the 'module://matplotlib_inline.backend_inline' error\n",
        "def fix_matplotlib_backend():\n",
        "    # Override problematic MPLBACKEND environment variable\n",
        "    if 'MPLBACKEND' in os.environ:\n",
        "        old_backend = os.environ['MPLBACKEND']\n",
        "        if 'inline' in old_backend or 'module://' in old_backend:\n",
        "            # Set to a safe backend for headless environments\n",
        "            os.environ['MPLBACKEND'] = 'Agg'\n",
        "            print(f\"ğŸ”§ Fixed matplotlib backend: {old_backend} -> Agg\")\n",
        "\n",
        "    # Ensure matplotlib uses safe backend before any imports\n",
        "    try:\n",
        "        import matplotlib\n",
        "        matplotlib.use('Agg', force=True)\n",
        "        print(\"âœ… Matplotlib backend set to 'Agg' (headless mode)\")\n",
        "    except ImportError:\n",
        "        print(\"âš ï¸ Matplotlib not yet installed - will be fixed after requirements\")\n",
        "\n",
        "    return True\n",
        "\n",
        "# Apply the fix\n",
        "fix_matplotlib_backend()\n",
        "\"\"\"\n",
        "\n",
        "            # Write the fix to a file that can be imported\n",
        "            fix_file_path = \"matplotlib_backend_fix.py\"\n",
        "            with open(fix_file_path, 'w') as f:\n",
        "                f.write(matplotlib_fix_script)\n",
        "\n",
        "            # Execute the fix\n",
        "            success, _ = self.run_command_safely([self.python_cmd, fix_file_path],\n",
        "                                               \"Matplotlib backend fix\", timeout=30)\n",
        "\n",
        "            if success:\n",
        "                print(\"âœ… Matplotlib backend fix applied successfully\")\n",
        "                return True\n",
        "            else:\n",
        "                print(\"âš ï¸ Matplotlib backend fix execution failed, continuing anyway\")\n",
        "                return True  # Non-critical failure\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Matplotlib backend fix failed: {e}\")\n",
        "            print(\"ğŸ”„ Continuing setup - will be resolved during runtime\")\n",
        "            return True  # Non-critical failure\n",
        "\n",
        "    def install_pytorch(self):\n",
        "        \"\"\"Install PyTorch using enforced venv commands\"\"\"\n",
        "        print(\"ğŸ”¥ Installing PyTorch...\")\n",
        "        print(f\"ğŸ”§ Using pip command: {self.pip_cmd}\")\n",
        "\n",
        "        # Use CUDA 12.4 for RTX 10XX-40XX as per documentation[6]\n",
        "        pytorch_cmd = [\n",
        "            self.pip_cmd, \"install\",\n",
        "            \"torch==2.6.0\", \"torchvision\", \"torchaudio\",\n",
        "            \"--index-url\", \"https://download.pytorch.org/whl/test/cu124\"\n",
        "        ]\n",
        "\n",
        "        success, _ = self.run_command_safely(pytorch_cmd, \"PyTorch installation\", timeout=300)\n",
        "        return success == True\n",
        "\n",
        "    def install_requirements(self):\n",
        "        \"\"\"Install requirements.txt using enforced venv commands\"\"\"\n",
        "        print(\"ğŸ“¦ Installing requirements...\")\n",
        "        print(f\"ğŸ”§ Using pip command: {self.pip_cmd}\")\n",
        "\n",
        "        if not os.path.exists(\"requirements.txt\"):\n",
        "            print(\"âš ï¸ requirements.txt not found, skipping\")\n",
        "            return True\n",
        "\n",
        "        req_cmd = [self.pip_cmd, \"install\", \"-r\", \"requirements.txt\"]\n",
        "        success, _ = self.run_command_safely(req_cmd, \"Requirements installation\", timeout=300)\n",
        "        return success == True\n",
        "\n",
        "    def verify_complete_setup(self):\n",
        "        \"\"\"Verify the complete installation using enforced venv commands\"\"\"\n",
        "        print(\"ğŸ” Verifying installation...\")\n",
        "        print(f\"ğŸ”§ Using python command: {self.python_cmd}\")\n",
        "\n",
        "        # Verify directory structure\n",
        "        current_dir = os.getcwd()\n",
        "        print(f\"ğŸ“ Final directory: {current_dir}\")\n",
        "\n",
        "        if current_dir.count(\"Wan2GP\") > 1:\n",
        "            print(\"ğŸš¨ CRITICAL: Nested directory structure detected!\")\n",
        "            print(\"âŒ Setup verification failed due to directory structure\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Test PyTorch installation\n",
        "            result = subprocess.run([self.python_cmd, \"-c\", \"import torch; print(f'PyTorch {torch.__version__} installed'); print(f'CUDA available: {torch.cuda.is_available()}')\"],\n",
        "                                  capture_output=True, text=True, timeout=30)\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print(\"âœ… PyTorch verification successful:\")\n",
        "                for line in result.stdout.strip().split('\\n'):\n",
        "                    print(f\"   {line}\")\n",
        "\n",
        "                # Verify main WanGP file exists\n",
        "                if os.path.exists(\"wgp.py\"):\n",
        "                    print(\"âœ… WanGP main file found\")\n",
        "                else:\n",
        "                    print(\"âš ï¸ WanGP main file (wgp.py) not found\")\n",
        "\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"âŒ PyTorch verification failed: {result.stderr}\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Verification failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def run_complete_setup(self):\n",
        "        \"\"\"Execute the complete setup process with enforced venv\"\"\"\n",
        "        print(\"ğŸš€ WAN2GP Complete System + Python Environment Setup\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"Platform: {self.current_platform}\")\n",
        "        print(f\"Virtual Environment: {'Yes' if self.use_venv else 'No'}\")\n",
        "        print(f\"Pip Command: {self.pip_cmd}\")\n",
        "        print(f\"Python Command: {self.python_cmd}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Final enforcement check per system instructions\n",
        "        if hasattr(self, 'current_platform') and self.current_platform == \"Google Colab\" and not self.use_venv:\n",
        "            print(\"ğŸš¨ FATAL: Google Colab not using venv per system instructions - ABORTING\")\n",
        "            return False\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for phase_name, phase_func in self.setup_phases:\n",
        "            self.update_progress(phase_name, \"in_progress\")\n",
        "\n",
        "            phase_start = time.time()\n",
        "            success = phase_func()\n",
        "            phase_duration = time.time() - phase_start\n",
        "\n",
        "            if success:\n",
        "                self.update_progress(phase_name, \"success\")\n",
        "                print(f\"â±ï¸ {phase_name} completed in {phase_duration:.1f}s\")\n",
        "                self.current_phase += 1\n",
        "            else:\n",
        "                self.update_progress(phase_name, \"failed\")\n",
        "                print(f\"ğŸ’¥ Setup failed at: {phase_name}\")\n",
        "                print(\"\\nğŸ”„ Setup Issues - Platform-Specific Troubleshooting:\")\n",
        "                print(\"ğŸ“± Colab: Restart runtime and re-run all cells\")\n",
        "                print(\"âš¡ Lightning.AI: Ensure you're using the correct Python version\")\n",
        "                print(\"ğŸŒŒ Vast.AI: Check GPU drivers and CUDA installation\")\n",
        "                return False\n",
        "\n",
        "        total_duration = time.time() - start_time\n",
        "        print(f\"\\nğŸ‰ WAN2GP setup completed successfully in {total_duration:.1f}s!\")\n",
        "        print(\"ğŸš€ Ready to launch WAN2GP!\")\n",
        "        return True\n",
        "\n",
        "# Execute the complete setup\n",
        "setup = CompleteEnvironmentSetup()\n",
        "setup.run_complete_setup()\n"
      ],
      "metadata": {
        "id": "Wc1mj9zi_ExM",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bef5f8e0-d4db-425b-c525-3976a5e9a8b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… VERIFIED: System setup correctly using venv for Colab\n",
            "ğŸš€ WAN2GP Complete System + Python Environment Setup\n",
            "======================================================================\n",
            "Platform: Google Colab\n",
            "Virtual Environment: Yes\n",
            "Pip Command: .venv/bin/pip\n",
            "Python Command: .venv/bin/python\n",
            "======================================================================\n",
            "\n",
            "[1/8] ğŸ”„ System Packages\n",
            "Progress: 0.0% | Platform: Google Colab\n",
            "ğŸ”§ Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "ğŸ”§ Installing system packages...\n",
            "ğŸ“¦ Updating package lists...\n",
            "ğŸ”§ Installing aria2, git, build-essential, and wget...\n",
            "âœ… System packages installed successfully\n",
            "\n",
            "[1/8] âœ… System Packages\n",
            "Progress: 0.0% | Platform: Google Colab\n",
            "ğŸ”§ Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "â±ï¸ System Packages completed in 9.0s\n",
            "\n",
            "[2/8] ğŸ”„ Directory Protection\n",
            "Progress: 12.5% | Platform: Google Colab\n",
            "ğŸ”§ Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "ğŸ›¡ï¸ Setting up directory protection...\n",
            "ğŸ“ Current directory: /content\n",
            "ğŸ“ Found existing Wan2GP directory at: /content/Wan2GP\n",
            "âœ… Existing directory appears to be a valid git repository\n",
            "âœ… Existing repository is correct - using it\n",
            "\n",
            "[2/8] âœ… Directory Protection\n",
            "Progress: 12.5% | Platform: Google Colab\n",
            "ğŸ”§ Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "â±ï¸ Directory Protection completed in 0.0s\n",
            "\n",
            "[3/8] ğŸ”„ Repository Clone\n",
            "Progress: 25.0% | Platform: Google Colab\n",
            "ğŸ”§ Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "ğŸ“‚ Setting up WAN2GP repository...\n",
            "ğŸ“ Working from: /content/Wan2GP\n",
            "ğŸš¨ CRITICAL: Still in Wan2GP directory path!\n",
            "ğŸ”„ Attempting emergency navigation...\n",
            "ğŸ†˜ Emergency navigation successful: /content\n",
            "ğŸ“ Directory Wan2GP already exists\n",
            "ğŸ“ Changed to directory: /content/Wan2GP\n",
            "âœ… Repository setup verified\n",
            "\n",
            "[3/8] âœ… Repository Clone\n",
            "Progress: 25.0% | Platform: Google Colab\n",
            "ğŸ”§ Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "â±ï¸ Repository Clone completed in 0.0s\n",
            "\n",
            "[4/8] ğŸ”„ Virtual Environment\n",
            "Progress: 37.5% | Platform: Google Colab\n",
            "ğŸ”§ Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "ğŸ Setting up virtual environment...\n",
            "ğŸ”§ Platform: Google Colab - venv REQUIRED per system instructions\n",
            "âœ… Virtual environment already exists and appears functional\n",
            "\n",
            "[4/8] âœ… Virtual Environment\n",
            "Progress: 37.5% | Platform: Google Colab\n",
            "ğŸ”§ Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "â±ï¸ Virtual Environment completed in 0.0s\n",
            "\n",
            "[5/8] ğŸ”„ Matplotlib Fix\n",
            "Progress: 50.0% | Platform: Google Colab\n",
            "ğŸ”§ Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "ğŸ¨ Fixing matplotlib backend compatibility...\n",
            "âœ… Matplotlib backend fix completed successfully\n",
            "âœ… Matplotlib backend fix applied successfully\n",
            "\n",
            "[5/8] âœ… Matplotlib Fix\n",
            "Progress: 50.0% | Platform: Google Colab\n",
            "ğŸ”§ Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "â±ï¸ Matplotlib Fix completed in 0.6s\n",
            "\n",
            "[6/8] ğŸ”„ PyTorch Installation\n",
            "Progress: 62.5% | Platform: Google Colab\n",
            "ğŸ”§ Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "ğŸ”¥ Installing PyTorch...\n",
            "ğŸ”§ Using pip command: .venv/bin/pip\n",
            "âœ… PyTorch installation completed successfully\n",
            "\n",
            "[6/8] âœ… PyTorch Installation\n",
            "Progress: 62.5% | Platform: Google Colab\n",
            "ğŸ”§ Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "â±ï¸ PyTorch Installation completed in 2.0s\n",
            "\n",
            "[7/8] ğŸ”„ Requirements\n",
            "Progress: 75.0% | Platform: Google Colab\n",
            "ğŸ”§ Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "ğŸ“¦ Installing requirements...\n",
            "ğŸ”§ Using pip command: .venv/bin/pip\n",
            "âœ… Requirements installation completed successfully\n",
            "\n",
            "[7/8] âœ… Requirements\n",
            "Progress: 75.0% | Platform: Google Colab\n",
            "ğŸ”§ Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "â±ï¸ Requirements completed in 16.1s\n",
            "\n",
            "[8/8] ğŸ”„ Environment Verification\n",
            "Progress: 87.5% | Platform: Google Colab\n",
            "ğŸ”§ Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "ğŸ” Verifying installation...\n",
            "ğŸ”§ Using python command: .venv/bin/python\n",
            "ğŸ“ Final directory: /content/Wan2GP\n",
            "âœ… PyTorch verification successful:\n",
            "   PyTorch 2.6.0+cu124 installed\n",
            "   CUDA available: True\n",
            "âœ… WanGP main file found\n",
            "\n",
            "[8/8] âœ… Environment Verification\n",
            "Progress: 87.5% | Platform: Google Colab\n",
            "ğŸ”§ Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "â±ï¸ Environment Verification completed in 4.5s\n",
            "\n",
            "ğŸ‰ WAN2GP setup completed successfully in 32.3s!\n",
            "ğŸš€ Ready to launch WAN2GP!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Quick Fix: Install HuggingFace Hub in venv\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_huggingface_hub():\n",
        "    \"\"\"Install huggingface_hub in the current venv\"\"\"\n",
        "    try:\n",
        "        # Use the same python command as the download system\n",
        "        python_cmd = \".venv/bin/python\" if \"google.colab\" in sys.modules else \"python\"\n",
        "        pip_cmd = \".venv/bin/pip\" if \"google.colab\" in sys.modules else \"pip\"\n",
        "\n",
        "        print(\"ğŸ“¦ Installing huggingface_hub in venv...\")\n",
        "        result = subprocess.run([\n",
        "            pip_cmd, \"install\", \"huggingface_hub\"\n",
        "        ], capture_output=True, text=True, timeout=120)\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(\"âœ… HuggingFace Hub installed successfully!\")\n",
        "            print(\"ğŸ”„ Re-run Cell 3 to use HF Hub downloads\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"âŒ Installation failed: {result.stderr}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Installation error: {e}\")\n",
        "        return False\n",
        "\n",
        "# Execute the fix\n",
        "success = install_huggingface_hub()\n",
        "if success:\n",
        "    print(\"\\nğŸš€ Ready to use HuggingFace Hub downloads!\")\n",
        "    print(\"ğŸ’¡ Re-run Cell 3 to retry with HF Hub support\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ Installation failed, but aria2c will still work fine\")\n",
        "    print(\"ğŸ’¡ Let Cell 3 continue with aria2c downloads\")\n"
      ],
      "metadata": {
        "id": "z9AWs1xcT32T",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efd99d4d-5aeb-42b0-b3cd-ddc9a5f4f2ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Installing huggingface_hub in venv...\n",
            "âœ… HuggingFace Hub installed successfully!\n",
            "ğŸ”„ Re-run Cell 3 to use HF Hub downloads\n",
            "\n",
            "ğŸš€ Ready to use HuggingFace Hub downloads!\n",
            "ğŸ’¡ Re-run Cell 3 to retry with HF Hub support\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell (3) WAN2GP Downloads - Complete System Fix v9.0 (All Bugs Resolved)\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import time\n",
        "import urllib.request\n",
        "import sys\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def jupyter_lab_title(title):\n",
        "    if \"google.colab\" not in sys.modules:\n",
        "        from IPython.display import display, Markdown\n",
        "        display(Markdown(f\"## {title}\"))\n",
        "\n",
        "jupyter_lab_title(\"WAN2GP Downloads - Complete System Fix\")\n",
        "\n",
        "# ==========================================\n",
        "# PLATFORM DETECTION - STANDALONE IMPLEMENTATION\n",
        "# ==========================================\n",
        "def detect_platform_standalone():\n",
        "    \"\"\"Standalone platform detection with comprehensive indicators\"\"\"\n",
        "\n",
        "    # Lightning AI detection\n",
        "    lightning_indicators = [\n",
        "        \"lightning\" in str(sys.executable).lower(),\n",
        "        \"teamspace-studios\" in os.getcwd(),\n",
        "        \"LIGHTNING_CLOUDSPACE_HOST\" in os.environ,\n",
        "        \"LIGHTNING_CLOUDSPACE_ID\" in os.environ,\n",
        "        \"commands/python\" in str(sys.executable),\n",
        "        \"/home/zeus/miniconda3/envs/cloudspace\" in str(sys.executable),\n",
        "        os.path.exists(\"/teamspace\"),\n",
        "        os.path.exists(\"/commands\")\n",
        "    ]\n",
        "\n",
        "    # Google Colab detection\n",
        "    colab_indicators = [\n",
        "        \"google.colab\" in sys.modules,\n",
        "        \"/content\" in os.getcwd()\n",
        "    ]\n",
        "\n",
        "    # Vast.AI detection\n",
        "    vast_indicators = [\n",
        "        \"VAST_CONTAINER_LABEL\" in os.environ,\n",
        "        \"/workspace\" in os.getcwd(),\n",
        "        \"vast\" in os.environ.get(\"HOSTNAME\", \"\").lower()\n",
        "    ]\n",
        "\n",
        "    if any(lightning_indicators):\n",
        "        return \"Lightning AI\"\n",
        "    elif any(colab_indicators):\n",
        "        return \"Google Colab\"\n",
        "    elif any(vast_indicators):\n",
        "        return \"Vast.AI/Generic\"\n",
        "    else:\n",
        "        return \"Vast.AI/Generic\"\n",
        "\n",
        "def get_platform_commands_standalone(platform):\n",
        "    \"\"\"Get platform-specific commands - ENFORCED VENV FOR COLAB per system instructions\"\"\"\n",
        "    if platform == \"Lightning AI\":\n",
        "        return \"pip\", \"python\", False  # (pip_cmd, python_cmd, use_venv)\n",
        "    elif platform == \"Google Colab\":\n",
        "        # ENFORCED: Always use venv for Google Colab per system instructions\n",
        "        return \".venv/bin/pip\", \".venv/bin/python\", True\n",
        "    else:  # Vast.AI/Generic\n",
        "        return \".venv/bin/pip\", \".venv/bin/python\", True\n",
        "\n",
        "# Detect current platform\n",
        "current_platform = detect_platform_standalone()\n",
        "pip_cmd, python_cmd, use_venv = get_platform_commands_standalone(current_platform)\n",
        "\n",
        "# ==========================================\n",
        "# HUGGINGFACE TOKEN SETUP - ADD YOUR TOKEN HERE\n",
        "# ==========================================\n",
        "# Get your token from: https://huggingface.co/settings/tokens\n",
        "HF_TOKEN = \"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\"\n",
        "\n",
        "def setup_hf_authentication():\n",
        "    \"\"\"Setup HuggingFace authentication with comprehensive token support\"\"\"\n",
        "    global HF_TOKEN\n",
        "\n",
        "    # Check if environment variable is already set\n",
        "    if 'HF_TOKEN' in os.environ:\n",
        "        print(\"âœ… HuggingFace token found in environment variable\")\n",
        "        print(\"ğŸ” Using existing token for authenticated downloads\")\n",
        "        return True\n",
        "\n",
        "    if HF_TOKEN and HF_TOKEN != \"hf_your_token_here_replace_this_text\":\n",
        "        try:\n",
        "            from huggingface_hub import login\n",
        "\n",
        "            # Set environment variables for all download methods\n",
        "            os.environ['HF_TOKEN'] = HF_TOKEN\n",
        "            os.environ['HUGGINGFACE_HUB_TOKEN'] = HF_TOKEN\n",
        "            os.environ['HUGGINGFACE_TOKEN'] = HF_TOKEN\n",
        "\n",
        "            # Login to HuggingFace Hub\n",
        "            login(token=HF_TOKEN, add_to_git_credential=False)\n",
        "            print(\"âœ… HuggingFace token configured successfully!\")\n",
        "            print(\"ğŸ” Authenticated downloads enabled\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Token setup warning: {e}\")\n",
        "            print(\"ğŸ”„ Continuing with unauthenticated downloads\")\n",
        "            return False\n",
        "    else:\n",
        "        print(\"âš ï¸ No HuggingFace token configured\")\n",
        "        print(\"ğŸ’¡ Add your token to HF_TOKEN variable for authenticated downloads\")\n",
        "        print(\"ğŸ“ Get token from: https://huggingface.co/settings/tokens\")\n",
        "        return False\n",
        "\n",
        "class WAN2GPSystemFixDownloader:\n",
        "    def __init__(self):\n",
        "        # Use standalone platform detection\n",
        "        self.current_platform = current_platform\n",
        "        self.pip_cmd = pip_cmd\n",
        "        self.python_cmd = python_cmd\n",
        "        self.use_venv = use_venv\n",
        "\n",
        "        # CRITICAL ENFORCEMENT: Ensure venv for Google Colab per system instructions\n",
        "        self.enforce_colab_venv()\n",
        "\n",
        "        # CORRECTED model configurations with verified URLs\n",
        "        self.essential_models = {\n",
        "            # Core Text2Video Models - VERIFIED WORKING\n",
        "            \"wan_t2v_1_3b\": {\n",
        "                \"name\": \"Wan 2.1 Text2Video 1.3B\",\n",
        "                \"filename\": \"Wan2_1-T2V-1_3B_bf16.safetensors\",\n",
        "                \"url\": \"https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Wan2_1-T2V-1_3B_bf16.safetensors\",\n",
        "                \"size\": \"2.87 GB\",\n",
        "                \"destination\": \"ckpts/\",\n",
        "                \"priority\": \"essential\",\n",
        "                \"description\": \"Fast T2V model (6GB VRAM required)\"\n",
        "            },\n",
        "            \"wan_t2v_1_3b_fp32\": {\n",
        "                \"name\": \"Wan 2.1 Text2Video 1.3B FP32\",\n",
        "                \"filename\": \"Wan2_1-T2V-1_3B_fp32.safetensors\",\n",
        "                \"url\": \"https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Wan2_1-T2V-1_3B_fp32.safetensors\",\n",
        "                \"size\": \"5.68 GB\",\n",
        "                \"destination\": \"ckpts/\",\n",
        "                \"priority\": \"high\",\n",
        "                \"description\": \"I2V compatibility model\"\n",
        "            },\n",
        "            # Essential Text Encoder - CORRECTED\n",
        "            \"umt5_encoder\": {\n",
        "                \"name\": \"UMT5 Text Encoder BF16\",\n",
        "                \"filename\": \"umt5-xxl-enc-bf16.safetensors\",\n",
        "                \"url\": \"https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/umt5-xxl-enc-bf16.safetensors\",\n",
        "                \"size\": \"3.0 GB\",\n",
        "                \"destination\": \"text_encoders/\",\n",
        "                \"priority\": \"essential\",\n",
        "                \"description\": \"Required text encoder for all models\"\n",
        "            },\n",
        "            # Essential VAE Models - VERIFIED WORKING\n",
        "            \"wan_vae_bf16\": {\n",
        "                \"name\": \"Wan VAE BF16\",\n",
        "                \"filename\": \"Wan2_1_VAE_bf16.safetensors\",\n",
        "                \"url\": \"https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Wan2_1_VAE_bf16.safetensors\",\n",
        "                \"size\": \"254 MB\",\n",
        "                \"destination\": \"vae/\",\n",
        "                \"priority\": \"essential\",\n",
        "                \"description\": \"Required VAE for video generation\"\n",
        "            },\n",
        "            \"wan_vae_fp32\": {\n",
        "                \"name\": \"Wan VAE FP32\",\n",
        "                \"filename\": \"Wan2_1_VAE_fp32.safetensors\",\n",
        "                \"url\": \"https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Wan2_1_VAE_fp32.safetensors\",\n",
        "                \"size\": \"508 MB\",\n",
        "                \"destination\": \"vae/\",\n",
        "                \"priority\": \"high\",\n",
        "                \"description\": \"Alternative VAE format\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Essential LoRA configurations - CORRECTED NAMES\n",
        "        self.essential_loras = {\n",
        "            # Speed Enhancement LoRAs with corrected filenames\n",
        "            \"safe_forcing\": {\n",
        "                \"name\": \"Safe-Forcing lightx2v (2-step generation)\",\n",
        "                \"filename\": \"Wan21_T2V_14B_lightx2v_cfg_step_distill_lora_rank32.safetensors\",\n",
        "                \"url\": \"https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Wan21_T2V_14B_lightx2v_cfg_step_distill_lora_rank32.safetensors\",\n",
        "                \"size\": \"317 MB\",\n",
        "                \"destination\": \"loras/\",\n",
        "                \"priority\": \"medium\",\n",
        "                \"description\": \"2-step generation, 2x speed boost\"\n",
        "            },\n",
        "            \"causvid\": {\n",
        "                \"name\": \"CausVid (4-12 step generation)\",\n",
        "                \"filename\": \"Wan21_CausVid_14B_T2V_lora_rank32.safetensors\",\n",
        "                \"url\": \"https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Wan21_CausVid_14B_T2V_lora_rank32.safetensors\",\n",
        "                \"size\": \"319 MB\",\n",
        "                \"destination\": \"loras/\",\n",
        "                \"priority\": \"medium\",\n",
        "                \"description\": \"4-12 steps, 2x speed improvement\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.all_downloads = {**self.essential_models, **self.essential_loras}\n",
        "\n",
        "    def enforce_colab_venv(self):\n",
        "        \"\"\"CRITICAL: Enforce venv usage for Google Colab per system instructions\"\"\"\n",
        "        if self.current_platform == \"Google Colab\":\n",
        "            if not self.use_venv or self.python_cmd != \".venv/bin/python\":\n",
        "                print(\"ğŸš¨ ENFORCING: Google Colab MUST use venv per system instructions\")\n",
        "                self.use_venv = True\n",
        "                self.pip_cmd = \".venv/bin/pip\"\n",
        "                self.python_cmd = \".venv/bin/python\"\n",
        "                print(\"âœ… ENFORCED: Download system using venv commands\")\n",
        "            else:\n",
        "                print(\"âœ… VERIFIED: Download system correctly using venv for Colab\")\n",
        "\n",
        "    def create_directory_structure(self):\n",
        "        \"\"\"Create complete WAN2GP directory structure\"\"\"\n",
        "        print(\"ğŸ“ Creating WAN2GP directory structure...\")\n",
        "\n",
        "        # Complete directory structure\n",
        "        directories = {\n",
        "            \"ckpts\": \"Main model files - populate model dropdown\",\n",
        "            \"text_encoders\": \"Text encoding models - required for generation\",\n",
        "            \"vae\": \"VAE models - required for generation\",\n",
        "            \"vae_approx\": \"Approximate VAE models\",\n",
        "            \"clip_vision\": \"CLIP vision models\",\n",
        "            \"loras\": \"General T2V LoRAs - populate LoRA checkboxes\",\n",
        "            \"loras1.3B\": \"1.3B specific LoRAs\",\n",
        "            \"loras14B\": \"14B specific LoRAs\",\n",
        "            \"lorasi2v\": \"Image-to-video LoRAs\",\n",
        "            \"lorashunyuan\": \"Hunyuan Video LoRAs\",\n",
        "            \"lorashunyuani2v\": \"Hunyuan I2V LoRAs\",\n",
        "            \"lorasltxv\": \"LTX Video LoRAs\"\n",
        "        }\n",
        "\n",
        "        for directory, description in directories.items():\n",
        "            Path(directory).mkdir(parents=True, exist_ok=True)\n",
        "            print(f\"   âœ… {directory}/ - {description}\")\n",
        "\n",
        "        print(\"âœ… Complete directory structure created\")\n",
        "        return True\n",
        "\n",
        "    def download_with_fixed_methods(self, config, item_id):\n",
        "        \"\"\"Download using multiple methods with ALL BUGS FIXED\"\"\"\n",
        "        filename = config[\"filename\"]\n",
        "        url = config[\"url\"]\n",
        "        destination = config[\"destination\"]\n",
        "        full_path = os.path.join(destination, filename)\n",
        "\n",
        "        print(f\"\\nğŸ“¥ Downloading: {config['name']}\")\n",
        "        print(f\"ğŸ“Š Size: {config['size']}\")\n",
        "        print(f\"ğŸ“„ File: {filename}\")\n",
        "        print(f\"ğŸ“ Destination: {full_path}\")\n",
        "        print(f\"ğŸ”— URL: {url}\")\n",
        "\n",
        "        # Check if file already exists\n",
        "        if os.path.exists(full_path):\n",
        "            file_size = os.path.getsize(full_path)\n",
        "            if file_size > 1024:  # File exists and has content\n",
        "                print(f\"âœ… {filename} already exists ({file_size:,} bytes) - skipping\")\n",
        "                return True\n",
        "\n",
        "        # Ensure destination directory exists\n",
        "        Path(destination).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Method 1: HuggingFace Hub with FIXED code\n",
        "        if self.try_huggingface_hub_download_fixed(url, full_path, filename):\n",
        "            return True\n",
        "\n",
        "        # Method 2: aria2c (fastest for large files)\n",
        "        if self.try_aria2c_download_fixed(url, full_path):\n",
        "            return True\n",
        "\n",
        "        # Method 3: wget with better error handling\n",
        "        if self.try_wget_download_fixed(url, full_path):\n",
        "            return True\n",
        "\n",
        "        # Method 4: Python urllib with enhanced auth\n",
        "        if self.try_urllib_download_fixed(url, full_path):\n",
        "            return True\n",
        "\n",
        "        print(f\"âŒ All download methods failed for {filename}\")\n",
        "        return False\n",
        "\n",
        "    def try_huggingface_hub_download_fixed(self, url, full_path, filename):\n",
        "        \"\"\"Try HuggingFace Hub download with FIXED Python code\"\"\"\n",
        "        try:\n",
        "            print(\"ğŸ”„ Trying HuggingFace Hub with FIXED code...\")\n",
        "\n",
        "            if \"huggingface.co\" in url and \"/resolve/main/\" in url:\n",
        "                # FIXED: Extract repo and file parts properly\n",
        "                url_parts = url.split(\"huggingface.co/\")[1].split(\"/resolve/main/\")\n",
        "                repo_part = url_parts[0]\n",
        "                file_part = url_parts[1]\n",
        "\n",
        "                # Create download script with FIXED variable definitions\n",
        "                download_script = f'''\n",
        "import sys\n",
        "import os\n",
        "try:\n",
        "    from huggingface_hub import hf_hub_download\n",
        "\n",
        "    # Use token from environment if available\n",
        "    token = os.environ.get('HF_TOKEN') or os.environ.get('HUGGINGFACE_HUB_TOKEN') or os.environ.get('HUGGINGFACE_TOKEN')\n",
        "\n",
        "    print(f\"ğŸ” Using token: {{bool(token)}}\")\n",
        "    print(f\"ğŸ“ Downloading {repo_part}/{file_part}\")\n",
        "\n",
        "    # Download with corrected parameters\n",
        "    downloaded_path = hf_hub_download(\n",
        "        repo_id=\"{repo_part}\",\n",
        "        filename=\"{file_part}\",\n",
        "        local_dir=\".\",\n",
        "        local_dir_use_symlinks=False,\n",
        "        token=token,\n",
        "        resume_download=True\n",
        "    )\n",
        "\n",
        "    # Verify file exists and has content\n",
        "    if os.path.exists(\"{full_path}\") and os.path.getsize(\"{full_path}\") > 1024:\n",
        "        print(\"HF_SUCCESS\")\n",
        "    else:\n",
        "        print(\"HF_FAILED - File not found or empty\")\n",
        "        sys.exit(1)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"HF_ERROR: {{e}}\")\n",
        "    sys.exit(1)\n",
        "'''\n",
        "\n",
        "                result = subprocess.run([\n",
        "                    self.python_cmd, \"-c\", download_script\n",
        "                ], capture_output=True, text=True, timeout=900)\n",
        "\n",
        "                if result.returncode == 0 and \"HF_SUCCESS\" in result.stdout:\n",
        "                    print(\"âœ… HuggingFace Hub download successful\")\n",
        "                    return True\n",
        "                else:\n",
        "                    print(f\"âŒ HuggingFace Hub failed:\")\n",
        "                    if result.stderr:\n",
        "                        print(f\"   Error: {result.stderr[:200]}\")\n",
        "                    if result.stdout:\n",
        "                        print(f\"   Output: {result.stdout[:200]}\")\n",
        "                    return False\n",
        "            else:\n",
        "                print(\"âš ï¸ URL not compatible with HuggingFace Hub\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ HuggingFace Hub error: {e}\")\n",
        "            return False\n",
        "\n",
        "    def try_aria2c_download_fixed(self, url, full_path):\n",
        "        \"\"\"Try aria2c download with enhanced error handling\"\"\"\n",
        "        try:\n",
        "            print(\"ğŸ”„ Trying aria2c (16 connections)...\")\n",
        "\n",
        "            # Enhanced aria2c command with better error handling\n",
        "            aria2c_cmd = [\n",
        "                \"aria2c\",\n",
        "                \"--max-connection-per-server=8\",  # Reduced connections\n",
        "                \"--split=8\",\n",
        "                \"--min-split-size=1M\",\n",
        "                \"--continue=true\",\n",
        "                \"--timeout=120\",  # Increased timeout\n",
        "                \"--retry-wait=5\",\n",
        "                \"--max-tries=2\",  # Reduced retries for faster fallback\n",
        "                \"--user-agent=Mozilla/5.0 (compatible; WAN2GP)\",\n",
        "                \"--out\", os.path.basename(full_path),\n",
        "                \"--dir\", os.path.dirname(full_path) or \".\",\n",
        "                url\n",
        "            ]\n",
        "\n",
        "            # Add authorization header if token is available\n",
        "            if 'HF_TOKEN' in os.environ:\n",
        "                aria2c_cmd.extend([\"--header\", f\"Authorization: Bearer {os.environ['HF_TOKEN']}\"])\n",
        "\n",
        "            result = subprocess.run(aria2c_cmd, capture_output=True, text=True, timeout=300)\n",
        "\n",
        "            if result.returncode == 0 and os.path.exists(full_path):\n",
        "                print(\"âœ… aria2c download successful\")\n",
        "                return True\n",
        "            else:\n",
        "                error_msg = result.stderr[:200] if result.stderr else \"Connection failed\"\n",
        "                print(f\"âŒ aria2c failed: {error_msg}\")\n",
        "                return False\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(\"âŒ aria2c not found - installing...\")\n",
        "            try:\n",
        "                subprocess.run([\"sudo\", \"apt-get\", \"update\", \"-qq\"], timeout=30)\n",
        "                subprocess.run([\"sudo\", \"apt-get\", \"install\", \"-y\", \"aria2\"], timeout=60)\n",
        "                print(\"âœ… aria2c installed - retrying download\")\n",
        "                return self.try_aria2c_download_fixed(url, full_path)  # Retry once\n",
        "            except:\n",
        "                print(\"âŒ aria2c installation failed\")\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ aria2c error: {e}\")\n",
        "            return False\n",
        "\n",
        "    def try_wget_download_fixed(self, url, full_path):\n",
        "        \"\"\"Try wget download with enhanced authentication\"\"\"\n",
        "        try:\n",
        "            print(\"ğŸ”„ Trying wget with enhanced auth...\")\n",
        "\n",
        "            # Build command with enhanced options\n",
        "            cmd = [\n",
        "                \"wget\",\n",
        "                \"--continue\",\n",
        "                \"--timeout=120\",\n",
        "                \"--tries=2\",\n",
        "                \"--retry-connrefused\",\n",
        "                \"--waitretry=5\",\n",
        "                \"--user-agent=Mozilla/5.0 (compatible; WAN2GP)\",\n",
        "                \"--no-check-certificate\"  # Skip SSL verification for problematic certificates\n",
        "            ]\n",
        "\n",
        "            # Add authorization header if token is available\n",
        "            if 'HF_TOKEN' in os.environ:\n",
        "                cmd.extend([\"--header\", f\"Authorization: Bearer {os.environ['HF_TOKEN']}\"])\n",
        "\n",
        "            cmd.extend([\"-O\", full_path, url])\n",
        "\n",
        "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
        "\n",
        "            if result.returncode == 0 and os.path.exists(full_path):\n",
        "                print(\"âœ… wget download successful\")\n",
        "                return True\n",
        "            else:\n",
        "                error_msg = result.stderr[:200] if result.stderr else \"Download failed\"\n",
        "                print(f\"âŒ wget failed: {error_msg}\")\n",
        "                return False\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(\"âŒ wget not found\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ wget error: {e}\")\n",
        "            return False\n",
        "\n",
        "    def try_urllib_download_fixed(self, url, full_path):\n",
        "        \"\"\"Try Python urllib download with enhanced authentication and error handling\"\"\"\n",
        "        try:\n",
        "            print(\"ğŸ”„ Trying Python urllib with enhanced auth...\")\n",
        "\n",
        "            def download_with_enhanced_auth(url, filepath):\n",
        "                try:\n",
        "                    # Ensure directory exists\n",
        "                    Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                    # Create request with comprehensive headers\n",
        "                    import urllib.request\n",
        "                    import ssl\n",
        "\n",
        "                    request = urllib.request.Request(url)\n",
        "                    request.add_header('User-Agent', 'Mozilla/5.0 (compatible; WAN2GP)')\n",
        "\n",
        "                    # Add authorization header if token is available\n",
        "                    if 'HF_TOKEN' in os.environ:\n",
        "                        request.add_header('Authorization', f'Bearer {os.environ[\"HF_TOKEN\"]}')\n",
        "\n",
        "                    # Create SSL context that's less strict\n",
        "                    ssl_context = ssl.create_default_context()\n",
        "                    ssl_context.check_hostname = False\n",
        "                    ssl_context.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "                    # Download file with enhanced error handling\n",
        "                    try:\n",
        "                        with urllib.request.urlopen(request, context=ssl_context, timeout=120) as response:\n",
        "                            total_size = int(response.headers.get('Content-Length', 0))\n",
        "                            downloaded = 0\n",
        "\n",
        "                            with open(filepath, 'wb') as f:\n",
        "                                while True:\n",
        "                                    chunk = response.read(8192)\n",
        "                                    if not chunk:\n",
        "                                        break\n",
        "                                    f.write(chunk)\n",
        "                                    downloaded += len(chunk)\n",
        "\n",
        "                                    # Show progress for large files\n",
        "                                    if total_size > 0 and downloaded % (1024*1024*10) == 0:  # Every 10MB\n",
        "                                        progress = (downloaded / total_size) * 100\n",
        "                                        print(f\"   Progress: {progress:.1f}% ({downloaded:,}/{total_size:,} bytes)\")\n",
        "\n",
        "                    except urllib.error.HTTPError as e:\n",
        "                        print(f\"HTTP Error {e.code}: {e.reason}\")\n",
        "                        return False\n",
        "                    except urllib.error.URLError as e:\n",
        "                        print(f\"URL Error: {e.reason}\")\n",
        "                        return False\n",
        "\n",
        "                    # Verify download\n",
        "                    return os.path.exists(filepath) and os.path.getsize(filepath) > 1024\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"urllib error: {e}\")\n",
        "                    return False\n",
        "\n",
        "            if download_with_enhanced_auth(url, full_path):\n",
        "                print(\"âœ… Python urllib download successful\")\n",
        "                return True\n",
        "            else:\n",
        "                print(\"âŒ Python urllib download failed\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Python urllib error: {e}\")\n",
        "            return False\n",
        "\n",
        "    def download_essential_only(self):\n",
        "        \"\"\"Download only truly essential files for basic functionality\"\"\"\n",
        "        print(\"ğŸ¯ Downloading Essential Files Only (Network Issues Detected)...\")\n",
        "        print(\"=\" * 70)\n",
        "        print(\"ğŸ“‹ Due to network issues, downloading only critical files:\")\n",
        "        print(\"   â€¢ Focus on files not already present\")\n",
        "        print(\"   â€¢ Skip files that may have network issues\")\n",
        "        print(\"   â€¢ Prioritize basic functionality\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Check what's already available\n",
        "        existing_models = list(Path(\"ckpts\").glob(\"*.safetensors\"))\n",
        "        existing_vaes = list(Path(\"vae\").glob(\"*.safetensors\"))\n",
        "        existing_encoders = list(Path(\"text_encoders\").glob(\"*.safetensors\"))\n",
        "\n",
        "        print(f\"ğŸ“„ Existing models: {len(existing_models)}\")\n",
        "        print(f\"ğŸ­ Existing VAEs: {len(existing_vaes)}\")\n",
        "        print(f\"ğŸ“ Existing encoders: {len(existing_encoders)}\")\n",
        "\n",
        "        # Essential items to try downloading (focus on missing files)\n",
        "        essential_priority = []\n",
        "\n",
        "        # Only add items if they don't exist\n",
        "        if len(existing_encoders) == 0:\n",
        "            essential_priority.append((\"umt5_encoder\", \"essential\"))\n",
        "\n",
        "        if len(existing_vaes) < 2:\n",
        "            if not any(\"bf16\" in str(vae) for vae in existing_vaes):\n",
        "                essential_priority.append((\"wan_vae_bf16\", \"essential\"))\n",
        "            if not any(\"fp32\" in str(vae) for vae in existing_vaes):\n",
        "                essential_priority.append((\"wan_vae_fp32\", \"high\"))\n",
        "\n",
        "        if len(existing_models) < 2:\n",
        "            if not any(\"bf16\" in str(model) for model in existing_models):\n",
        "                essential_priority.append((\"wan_t2v_1_3b\", \"essential\"))\n",
        "            if not any(\"fp32\" in str(model) for model in existing_models):\n",
        "                essential_priority.append((\"wan_t2v_1_3b_fp32\", \"high\"))\n",
        "\n",
        "        if not essential_priority:\n",
        "            print(\"âœ… All essential files already present!\")\n",
        "            print(\"ğŸ‰ No additional downloads needed\")\n",
        "            return True\n",
        "\n",
        "        successful_downloads = 0\n",
        "        total_items = len(essential_priority)\n",
        "\n",
        "        for i, (item_id, priority) in enumerate(essential_priority, 1):\n",
        "            config = self.all_downloads[item_id]\n",
        "            print(f\"\\n[{i}/{total_items}] Processing {config['name']} (Priority: {priority})\")\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "            if self.download_with_fixed_methods(config, item_id):\n",
        "                successful_downloads += 1\n",
        "                print(f\"âœ… {config['name']} downloaded successfully\")\n",
        "            else:\n",
        "                print(f\"âŒ {config['name']} download failed\")\n",
        "                if priority == \"essential\":\n",
        "                    print(\"âš ï¸ This is an essential file - WAN2GP functionality may be limited\")\n",
        "\n",
        "        success_rate = (successful_downloads / total_items) * 100 if total_items > 0 else 100\n",
        "\n",
        "        print(f\"\\nğŸ“Š Essential Download Summary:\")\n",
        "        print(f\"Total items: {total_items}\")\n",
        "        print(f\"Successful: {successful_downloads}\")\n",
        "        print(f\"Failed: {total_items - successful_downloads}\")\n",
        "        print(f\"Success rate: {success_rate:.1f}%\")\n",
        "\n",
        "        # Verify interface population\n",
        "        self.verify_interface_population()\n",
        "\n",
        "        return success_rate >= 50 or total_items == 0\n",
        "\n",
        "    def verify_interface_population(self):\n",
        "        \"\"\"Verify that downloaded files will populate the Gradio interface\"\"\"\n",
        "        print(f\"\\nğŸ” Verifying Interface Population...\")\n",
        "\n",
        "        # Check models for dropdown population\n",
        "        model_files = list(Path(\"ckpts\").glob(\"*.safetensors\"))\n",
        "        print(f\"ğŸ“„ Models in ckpts/ (will appear in model dropdown): {len(model_files)}\")\n",
        "        for model in model_files[:5]:  # Show first 5\n",
        "            print(f\"   âœ… {model.name}\")\n",
        "        if len(model_files) > 5:\n",
        "            print(f\"   ... and {len(model_files) - 5} more\")\n",
        "\n",
        "        # Check LoRAs for checkbox population\n",
        "        lora_files = list(Path(\"loras\").glob(\"*.safetensors\"))\n",
        "        print(f\"ğŸ¨ LoRAs in loras/ (will appear in Advanced tab): {len(lora_files)}\")\n",
        "        for lora in lora_files[:5]:  # Show first 5\n",
        "            print(f\"   âœ… {lora.name}\")\n",
        "        if len(lora_files) > 5:\n",
        "            print(f\"   ... and {len(lora_files) - 5} more\")\n",
        "\n",
        "        # Check I2V LoRAs\n",
        "        i2v_lora_files = list(Path(\"lorasi2v\").glob(\"*.safetensors\"))\n",
        "        print(f\"ğŸ–¼ï¸ I2V LoRAs in lorasi2v/: {len(i2v_lora_files)}\")\n",
        "        for lora in i2v_lora_files:\n",
        "            print(f\"   âœ… {lora.name}\")\n",
        "\n",
        "        # Check essential components\n",
        "        encoder_files = list(Path(\"text_encoders\").glob(\"*.safetensors\"))\n",
        "        print(f\"ğŸ“ Text encoders (required): {len(encoder_files)}\")\n",
        "\n",
        "        vae_files = list(Path(\"vae\").glob(\"*.safetensors\"))\n",
        "        print(f\"ğŸ­ VAE models (required): {len(vae_files)}\")\n",
        "\n",
        "        # Summary for interface readiness\n",
        "        interface_ready = (len(model_files) >= 1 and len(encoder_files) >= 1 and\n",
        "                          len(vae_files) >= 1)\n",
        "\n",
        "        if interface_ready:\n",
        "            print(f\"\\nâœ… Interface Population Verified!\")\n",
        "            print(f\"ğŸ‰ WAN2GP will show:\")\n",
        "            print(f\"   â€¢ {len(model_files)} model(s) in dropdown menu\")\n",
        "            print(f\"   â€¢ {len(lora_files)} LoRA(s) in Advanced tab checkboxes\")\n",
        "            print(f\"   â€¢ All required components for generation\")\n",
        "            print(f\"\\nğŸš€ READY TO LAUNCH WAN2GP!\")\n",
        "        else:\n",
        "            print(f\"\\nâš ï¸ Interface may have limited options\")\n",
        "            print(f\"ğŸ’¡ Missing: {' ' if len(model_files) >= 1 else 'models '}{' ' if len(encoder_files) >= 1 else 'encoders '}{' ' if len(vae_files) >= 1 else 'VAEs'}\")\n",
        "\n",
        "        return interface_ready\n",
        "\n",
        "# ==========================================\n",
        "# EXECUTE FIXED DOWNLOAD SYSTEM\n",
        "# ==========================================\n",
        "\n",
        "# Setup HuggingFace authentication first\n",
        "token_configured = setup_hf_authentication()\n",
        "\n",
        "# Create and execute the download system\n",
        "downloader = WAN2GPSystemFixDownloader()\n",
        "\n",
        "# Display download interface\n",
        "display(HTML(f\"\"\"\n",
        "<div style=\"background: linear-gradient(135deg, #28a745 0%, #20c997 100%);\n",
        "           color: white; padding: 20px; border-radius: 10px; text-align: center;\n",
        "           margin: 10px 0; box-shadow: 0 4px 15px rgba(0,0,0,0.2);\">\n",
        "    <h2>ğŸ”§ WAN2GP Downloads - Complete System Fix</h2>\n",
        "    <p><strong>Platform:</strong> {downloader.current_platform}</p>\n",
        "    <p><strong>Python Command:</strong> {downloader.python_cmd}</p>\n",
        "    <p><strong>HF Token:</strong> {'âœ… Configured' if token_configured else 'âš ï¸ Not Configured'}</p>\n",
        "    <p><strong>Status:</strong> All bugs fixed, enhanced error handling</p>\n",
        "</div>\n",
        "\"\"\"))\n",
        "\n",
        "print(\"ğŸ”§ WAN2GP System Fix Download Options:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. Essential Files Only (Recommended) - Smart Download\")\n",
        "print(\"   â€¢ Only downloads missing essential files\")\n",
        "print(\"   â€¢ Enhanced error handling and fallback methods\")\n",
        "print(\"   â€¢ Works around network connectivity issues\")\n",
        "print(\"   â€¢ Maximizes success rate with existing files\")\n",
        "print(\"\")\n",
        "print(\"2. Skip Downloads\")\n",
        "print(\"   â€¢ Use existing files only\")\n",
        "print(\"   â€¢ Launch WAN2GP with current file set\")\n",
        "print(\"\")\n",
        "\n",
        "choice = input(\"ğŸ¯ Select option (1/2 or Enter for Essential): \").strip()\n",
        "\n",
        "# Execute download based on choice\n",
        "if choice == \"2\":\n",
        "    print(\"â­ï¸ Skipping downloads - using existing files\")\n",
        "    success = True\n",
        "    downloader.verify_interface_population()\n",
        "else:\n",
        "    # Create directory structure first\n",
        "    if not downloader.create_directory_structure():\n",
        "        print(\"âŒ Failed to create directory structure\")\n",
        "        success = False\n",
        "    else:\n",
        "        # Download essential files only\n",
        "        success = downloader.download_essential_only()\n",
        "\n",
        "if success:\n",
        "    print(\"\\nğŸ‰ System fix completed successfully!\")\n",
        "    print(\"âœ… WAN2GP should be ready to launch with existing files\")\n",
        "    print(\"ğŸš€ Proceed to Cell 4 to launch WAN2GP\")\n",
        "    if token_configured:\n",
        "        print(\"ğŸ” Token authentication available for future downloads\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ Some issues remain, but WAN2GP may still be functional\")\n",
        "    print(\"ğŸ”„ Existing files should provide basic functionality\")\n",
        "    print(\"ğŸš€ Try launching WAN2GP anyway - it may work with current files\")\n"
      ],
      "metadata": {
        "id": "yP38IfT3_Gy9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "76eb3aeb-1edf-4e1b-cc78-6a7a1975360c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸ Token setup warning: Invalid user token. The token from HF_TOKEN environment variable is invalid. Note that HF_TOKEN takes precedence over `huggingface-cli login`.\n",
            "ğŸ”„ Continuing with unauthenticated downloads\n",
            "âœ… VERIFIED: Download system correctly using venv for Colab\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<div style=\"background: linear-gradient(135deg, #28a745 0%, #20c997 100%); \n",
              "           color: white; padding: 20px; border-radius: 10px; text-align: center; \n",
              "           margin: 10px 0; box-shadow: 0 4px 15px rgba(0,0,0,0.2);\">\n",
              "    <h2>ğŸ”§ WAN2GP Downloads - Complete System Fix</h2>\n",
              "    <p><strong>Platform:</strong> Google Colab</p>\n",
              "    <p><strong>Python Command:</strong> .venv/bin/python</p>\n",
              "    <p><strong>HF Token:</strong> âš ï¸ Not Configured</p>\n",
              "    <p><strong>Status:</strong> All bugs fixed, enhanced error handling</p>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ WAN2GP System Fix Download Options:\n",
            "============================================================\n",
            "1. Essential Files Only (Recommended) - Smart Download\n",
            "   â€¢ Only downloads missing essential files\n",
            "   â€¢ Enhanced error handling and fallback methods\n",
            "   â€¢ Works around network connectivity issues\n",
            "   â€¢ Maximizes success rate with existing files\n",
            "\n",
            "2. Skip Downloads\n",
            "   â€¢ Use existing files only\n",
            "   â€¢ Launch WAN2GP with current file set\n",
            "\n",
            "ğŸ¯ Select option (1/2 or Enter for Essential): 1\n",
            "ğŸ“ Creating WAN2GP directory structure...\n",
            "   âœ… ckpts/ - Main model files - populate model dropdown\n",
            "   âœ… text_encoders/ - Text encoding models - required for generation\n",
            "   âœ… vae/ - VAE models - required for generation\n",
            "   âœ… vae_approx/ - Approximate VAE models\n",
            "   âœ… clip_vision/ - CLIP vision models\n",
            "   âœ… loras/ - General T2V LoRAs - populate LoRA checkboxes\n",
            "   âœ… loras1.3B/ - 1.3B specific LoRAs\n",
            "   âœ… loras14B/ - 14B specific LoRAs\n",
            "   âœ… lorasi2v/ - Image-to-video LoRAs\n",
            "   âœ… lorashunyuan/ - Hunyuan Video LoRAs\n",
            "   âœ… lorashunyuani2v/ - Hunyuan I2V LoRAs\n",
            "   âœ… lorasltxv/ - LTX Video LoRAs\n",
            "âœ… Complete directory structure created\n",
            "ğŸ¯ Downloading Essential Files Only (Network Issues Detected)...\n",
            "======================================================================\n",
            "ğŸ“‹ Due to network issues, downloading only critical files:\n",
            "   â€¢ Focus on files not already present\n",
            "   â€¢ Skip files that may have network issues\n",
            "   â€¢ Prioritize basic functionality\n",
            "======================================================================\n",
            "ğŸ“„ Existing models: 2\n",
            "ğŸ­ Existing VAEs: 2\n",
            "ğŸ“ Existing encoders: 1\n",
            "âœ… All essential files already present!\n",
            "ğŸ‰ No additional downloads needed\n",
            "\n",
            "ğŸ‰ System fix completed successfully!\n",
            "âœ… WAN2GP should be ready to launch with existing files\n",
            "ğŸš€ Proceed to Cell 4 to launch WAN2GP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Emergency Fix: Install Missing mmgp Module v1.0\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def fix_mmgp_dependency():\n",
        "    \"\"\"Install missing mmgp module in Google Colab venv\"\"\"\n",
        "\n",
        "    print(\"ğŸš¨ Fixing WAN2GP Missing Dependency: mmgp module\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Use correct venv paths for Google Colab per system instructions\n",
        "    venv_pip = \".venv/bin/pip\"\n",
        "    venv_python = \".venv/bin/python\"\n",
        "\n",
        "    print(f\"ğŸ”§ Platform: Google Colab\")\n",
        "    print(f\"ğŸ“¦ Pip Command: {venv_pip}\")\n",
        "    print(f\"ğŸ Python Command: {venv_python}\")\n",
        "    print(f\"ğŸ“ Working Directory: {os.getcwd()}\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Install mmgp module\n",
        "        print(\"\\nğŸ“¥ Installing mmgp (Memory Management for GPU Poor)...\")\n",
        "        result = subprocess.run([\n",
        "            venv_pip, \"install\", \"mmgp\"\n",
        "        ], capture_output=True, text=True, timeout=120)\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(\"âœ… mmgp module installed successfully!\")\n",
        "        else:\n",
        "            print(f\"âŒ mmgp installation failed: {result.stderr}\")\n",
        "\n",
        "            # Try alternative installation method\n",
        "            print(\"ğŸ”„ Trying alternative installation from GitHub...\")\n",
        "            github_result = subprocess.run([\n",
        "                venv_pip, \"install\", \"git+https://github.com/deepbeepmeep/mmgp.git\"\n",
        "            ], capture_output=True, text=True, timeout=180)\n",
        "\n",
        "            if github_result.returncode == 0:\n",
        "                print(\"âœ… mmgp installed from GitHub successfully!\")\n",
        "            else:\n",
        "                print(f\"âŒ GitHub installation also failed: {github_result.stderr}\")\n",
        "                return False\n",
        "\n",
        "        # Step 2: Verify installation\n",
        "        print(\"\\nğŸ” Verifying mmgp installation...\")\n",
        "        verify_result = subprocess.run([\n",
        "            venv_python, \"-c\",\n",
        "            \"import mmgp; from mmgp import offload, profile_type; print(f'âœ… mmgp imported successfully'); print(f'ğŸ“‹ Available profiles: {[p.name for p in profile_type]}')\"\n",
        "        ], capture_output=True, text=True, timeout=30)\n",
        "\n",
        "        if verify_result.returncode == 0:\n",
        "            print(\"âœ… mmgp verification successful!\")\n",
        "            print(f\"ğŸ“‹ Module info: {verify_result.stdout}\")\n",
        "        else:\n",
        "            print(f\"âŒ mmgp verification failed: {verify_result.stderr}\")\n",
        "            return False\n",
        "\n",
        "        # Step 3: Install any additional dependencies\n",
        "        print(\"\\nğŸ“¦ Installing additional dependencies...\")\n",
        "        additional_deps = [\n",
        "            \"safetensors\", \"accelerate\", \"transformers\"\n",
        "        ]\n",
        "\n",
        "        for dep in additional_deps:\n",
        "            dep_result = subprocess.run([\n",
        "                venv_pip, \"install\", \"--upgrade\", dep\n",
        "            ], capture_output=True, text=True, timeout=60)\n",
        "\n",
        "            if dep_result.returncode == 0:\n",
        "                print(f\"âœ… {dep} installed/upgraded successfully\")\n",
        "            else:\n",
        "                print(f\"âš ï¸ {dep} installation warning: {dep_result.stderr[:100]}\")\n",
        "\n",
        "        print(\"\\nğŸ‰ WAN2GP dependency fix completed successfully!\")\n",
        "        print(\"ğŸš€ You can now re-run Cell 4 to launch WAN2GP\")\n",
        "        print(\"ğŸ’¡ The mmgp module will now provide memory optimization for your hardware\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"â° Installation timed out - try again\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"ğŸ’¥ Installation error: {e}\")\n",
        "        return False\n",
        "\n",
        "# Execute the fix\n",
        "success = fix_mmgp_dependency()\n",
        "\n",
        "if success:\n",
        "    print(\"\\nğŸ”„ Next Steps:\")\n",
        "    print(\"1. Re-run Cell 4 to launch WAN2GP\")\n",
        "    print(\"2. Your selected configuration will be used:\")\n",
        "    print(\"   â€¢ Model: Wan Fun InP 1.3B (Image-to-Video)\")\n",
        "    print(\"   â€¢ Performance: High Performance Mode\")\n",
        "    print(\"   â€¢ Memory Profile: Profile 3 (optimized for your setup)\")\n",
        "else:\n",
        "    print(\"\\nğŸ†˜ Emergency Fallback Options:\")\n",
        "    print(\"1. Try restarting the runtime and re-running all cells\")\n",
        "    print(\"2. Use emergency installation command:\")\n",
        "    print(\"   !.venv/bin/pip install mmgp safetensors accelerate\")\n"
      ],
      "metadata": {
        "id": "JqYkIt654QfE",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66b9ddb8-8f74-4eba-9d45-461e542ba39a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš¨ Fixing WAN2GP Missing Dependency: mmgp module\n",
            "============================================================\n",
            "ğŸ”§ Platform: Google Colab\n",
            "ğŸ“¦ Pip Command: .venv/bin/pip\n",
            "ğŸ Python Command: .venv/bin/python\n",
            "ğŸ“ Working Directory: /content/Wan2GP\n",
            "\n",
            "ğŸ“¥ Installing mmgp (Memory Management for GPU Poor)...\n",
            "âœ… mmgp module installed successfully!\n",
            "\n",
            "ğŸ” Verifying mmgp installation...\n",
            "âœ… mmgp verification successful!\n",
            "ğŸ“‹ Module info: âœ… mmgp imported successfully\n",
            "ğŸ“‹ Available profiles: ['HighRAM_HighVRAM', 'HighRAM_LowVRAM', 'LowRAM_HighVRAM', 'LowRAM_LowVRAM', 'VerylowRAM_LowVRAM']\n",
            "\n",
            "\n",
            "ğŸ“¦ Installing additional dependencies...\n",
            "âœ… safetensors installed/upgraded successfully\n",
            "âœ… accelerate installed/upgraded successfully\n",
            "âœ… transformers installed/upgraded successfully\n",
            "\n",
            "ğŸ‰ WAN2GP dependency fix completed successfully!\n",
            "ğŸš€ You can now re-run Cell 4 to launch WAN2GP\n",
            "ğŸ’¡ The mmgp module will now provide memory optimization for your hardware\n",
            "\n",
            "ğŸ”„ Next Steps:\n",
            "1. Re-run Cell 4 to launch WAN2GP\n",
            "2. Your selected configuration will be used:\n",
            "   â€¢ Model: Wan Fun InP 1.3B (Image-to-Video)\n",
            "   â€¢ Performance: High Performance Mode\n",
            "   â€¢ Memory Profile: Profile 3 (optimized for your setup)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Complete WAN2GP Missing Models Fix - SAM + Pose + Token Refresh v1.0\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "import time\n",
        "\n",
        "def complete_wan2gp_fix():\n",
        "    \"\"\"Complete fix for all WAN2GP missing model issues\"\"\"\n",
        "\n",
        "    print(\"ğŸ”§ WAN2GP Complete Missing Models Fix\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"ğŸ¯ Fixing 3 Critical Issues:\")\n",
        "    print(\"   1. Missing SAM model (sam_vit_h_4b8939_fp16.safetensors)\")\n",
        "    print(\"   2. Missing Pose model (dw-ll_ucoco_384.onnx)\")\n",
        "    print(\"   3. Expired HuggingFace token authentication\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Ensure we're in the correct WAN2GP directory\n",
        "    if not os.path.exists(\"wgp.py\"):\n",
        "        print(\"âŒ Not in WAN2GP directory. Please run this in /content/Wan2GP/\")\n",
        "        return False\n",
        "\n",
        "    # Step 1: Fix HuggingFace Token Authentication\n",
        "    print(\"\\nğŸ” Step 1: Fixing HuggingFace Token Authentication...\")\n",
        "    fix_hf_authentication()\n",
        "\n",
        "    # Step 2: Download missing models\n",
        "    missing_models = {\n",
        "        \"sam_model\": {\n",
        "            \"name\": \"SAM ViT-H Model\",\n",
        "            \"filename\": \"sam_vit_h_4b8939_fp16.safetensors\",\n",
        "            \"url\": \"https://huggingface.co/DeepBeepMeep/Wan2.1/resolve/main/sam_vit_h_4b8939_fp16.safetensors\",\n",
        "            \"alt_url\": \"https://huggingface.co/luca-martial/segment-anything-h/resolve/main/sam_vit_h_4b8939.pth\",\n",
        "            \"size\": \"1.28 GB\",\n",
        "            \"destination\": \"ckpts/mask/\",\n",
        "            \"description\": \"Segment Anything Model for mask editing\"\n",
        "        },\n",
        "        \"pose_model\": {\n",
        "            \"name\": \"DWPose Model\",\n",
        "            \"filename\": \"dw-ll_ucoco_384.onnx\",\n",
        "            \"url\": \"https://huggingface.co/DeepBeepMeep/Wan2.1/resolve/main/pose/dw-ll_ucoco_384.onnx\",\n",
        "            \"alt_url\": \"https://huggingface.co/yzd-v/DWPose/resolve/main/dw-ll_ucoco_384.onnx\",\n",
        "            \"size\": \"134 MB\",\n",
        "            \"destination\": \"ckpts/pose/\",\n",
        "            \"description\": \"DWPose model for pose estimation and ControlNet\"\n",
        "        },\n",
        "        \"yolox_model\": {\n",
        "            \"name\": \"YOLOX Detection Model\",\n",
        "            \"filename\": \"yolox_l.onnx\",\n",
        "            \"url\": \"https://huggingface.co/DeepBeepMeep/Wan2.1/resolve/main/pose/yolox_l.onnx\",\n",
        "            \"alt_url\": \"https://huggingface.co/yzd-v/DWPose/resolve/main/yolox_l.onnx\",\n",
        "            \"size\": \"217 MB\",\n",
        "            \"destination\": \"ckpts/pose/\",\n",
        "            \"description\": \"YOLOX detection model for pose preprocessing\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Create all necessary directories\n",
        "    print(\"\\nğŸ“ Step 2: Creating model directories...\")\n",
        "    for model_config in missing_models.values():\n",
        "        model_dir = Path(model_config[\"destination\"])\n",
        "        model_dir.mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"   âœ… Created: {model_dir}\")\n",
        "\n",
        "    # Download each missing model\n",
        "    successful_downloads = 0\n",
        "    total_models = len(missing_models)\n",
        "\n",
        "    for i, (model_id, config) in enumerate(missing_models.items(), 1):\n",
        "        print(f\"\\n[{i}/{total_models}] Step {i+2}: Downloading {config['name']}...\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        if download_model_with_fallback(config):\n",
        "            successful_downloads += 1\n",
        "            print(f\"âœ… {config['name']} downloaded successfully\")\n",
        "        else:\n",
        "            print(f\"âŒ {config['name']} download failed\")\n",
        "\n",
        "    # Verify all installations\n",
        "    print(f\"\\nğŸ” Step {total_models+3}: Verifying All Model Installations...\")\n",
        "    verification_success = verify_all_models(missing_models)\n",
        "\n",
        "    # Final summary\n",
        "    success_rate = (successful_downloads / total_models) * 100\n",
        "    print(f\"\\nğŸ“Š Complete Fix Summary:\")\n",
        "    print(f\"Total models: {total_models}\")\n",
        "    print(f\"Successful downloads: {successful_downloads}\")\n",
        "    print(f\"Success rate: {success_rate:.1f}%\")\n",
        "\n",
        "    if verification_success and success_rate >= 66:\n",
        "        print(\"\\nğŸ‰ WAN2GP Complete Fix Successful!\")\n",
        "        print(\"âœ… All critical models are now available\")\n",
        "        print(\"ğŸš€ Next Steps:\")\n",
        "        print(\"   1. Refresh your WAN2GP browser tab\")\n",
        "        print(\"   2. All mask editing and pose features should now work\")\n",
        "        print(\"   3. No more 'Unable to find file' errors\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"\\nâš ï¸ Some models may still be missing\")\n",
        "        print(\"ğŸ”„ You can re-run this cell to retry failed downloads\")\n",
        "        return False\n",
        "\n",
        "def fix_hf_authentication():\n",
        "    \"\"\"Fix HuggingFace authentication by clearing expired tokens\"\"\"\n",
        "    try:\n",
        "        # Clear any expired token environment variables\n",
        "        expired_vars = ['HF_TOKEN', 'HUGGINGFACE_HUB_TOKEN', 'HUGGINGFACE_TOKEN']\n",
        "        for var in expired_vars:\n",
        "            if var in os.environ:\n",
        "                del os.environ[var]\n",
        "                print(f\"   ğŸ—‘ï¸ Cleared expired {var}\")\n",
        "\n",
        "        # Clear HuggingFace cache that might contain expired tokens\n",
        "        cache_dirs = [\n",
        "            Path.home() / \".cache\" / \"huggingface\",\n",
        "            Path.home() / \".huggingface\",\n",
        "            Path(\"/content/.cache/huggingface\") if os.path.exists(\"/content\") else None\n",
        "        ]\n",
        "\n",
        "        for cache_dir in cache_dirs:\n",
        "            if cache_dir and cache_dir.exists():\n",
        "                token_file = cache_dir / \"token\"\n",
        "                if token_file.exists():\n",
        "                    token_file.unlink()\n",
        "                    print(f\"   ğŸ—‘ï¸ Cleared cached token: {token_file}\")\n",
        "\n",
        "        print(\"âœ… HuggingFace authentication cleaned\")\n",
        "        print(\"ğŸ’¡ Downloads will now use unauthenticated public access\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Token cleanup warning: {e}\")\n",
        "        return False\n",
        "\n",
        "def download_model_with_fallback(config):\n",
        "    \"\"\"Download model using multiple fallback methods\"\"\"\n",
        "    filename = config[\"filename\"]\n",
        "    destination = config[\"destination\"]\n",
        "    full_path = Path(destination) / filename\n",
        "\n",
        "    print(f\"ğŸ“¥ Downloading: {config['name']}\")\n",
        "    print(f\"ğŸ“Š Size: {config['size']}\")\n",
        "    print(f\"ğŸ“„ File: {filename}\")\n",
        "    print(f\"ğŸ“ Destination: {full_path}\")\n",
        "\n",
        "    # Check if file already exists\n",
        "    if full_path.exists():\n",
        "        file_size = full_path.stat().st_size\n",
        "        if file_size > 1024*1024*10:  # File exists and > 10MB\n",
        "            print(f\"âœ… {filename} already exists ({file_size:,} bytes)\")\n",
        "            return True\n",
        "\n",
        "    # Try primary URL first\n",
        "    urls_to_try = [config[\"url\"]]\n",
        "    if \"alt_url\" in config:\n",
        "        urls_to_try.append(config[\"alt_url\"])\n",
        "\n",
        "    for url_index, url in enumerate(urls_to_try, 1):\n",
        "        print(f\"ğŸ”— Trying URL {url_index}: {url}\")\n",
        "\n",
        "        # Method 1: aria2c (fastest)\n",
        "        if download_with_aria2c(url, full_path):\n",
        "            return True\n",
        "\n",
        "        # Method 2: wget (reliable)\n",
        "        if download_with_wget(url, full_path):\n",
        "            return True\n",
        "\n",
        "        # Method 3: Python urllib (final fallback)\n",
        "        if download_with_urllib(url, full_path):\n",
        "            return True\n",
        "\n",
        "        print(f\"âŒ URL {url_index} failed, trying next...\")\n",
        "\n",
        "    return False\n",
        "\n",
        "def download_with_aria2c(url, full_path):\n",
        "    \"\"\"Download with aria2c\"\"\"\n",
        "    try:\n",
        "        print(\"ğŸ”„ Trying aria2c...\")\n",
        "\n",
        "        result = subprocess.run([\n",
        "            \"aria2c\",\n",
        "            \"--max-connection-per-server=8\",\n",
        "            \"--split=8\",\n",
        "            \"--min-split-size=1M\",\n",
        "            \"--continue=true\",\n",
        "            \"--timeout=120\",\n",
        "            \"--retry-wait=3\",\n",
        "            \"--max-tries=2\",\n",
        "            \"--user-agent=Mozilla/5.0 (compatible; WAN2GP)\",\n",
        "            \"--out\", full_path.name,\n",
        "            \"--dir\", str(full_path.parent),\n",
        "            url\n",
        "        ], capture_output=True, text=True, timeout=1800)\n",
        "\n",
        "        if result.returncode == 0 and full_path.exists():\n",
        "            print(\"âœ… aria2c download successful\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"âŒ aria2c failed: {result.stderr[:100] if result.stderr else 'Unknown error'}\")\n",
        "            return False\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"âŒ aria2c not found\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ aria2c error: {e}\")\n",
        "        return False\n",
        "\n",
        "def download_with_wget(url, full_path):\n",
        "    \"\"\"Download with wget\"\"\"\n",
        "    try:\n",
        "        print(\"ğŸ”„ Trying wget...\")\n",
        "\n",
        "        result = subprocess.run([\n",
        "            \"wget\",\n",
        "            \"--continue\",\n",
        "            \"--timeout=120\",\n",
        "            \"--tries=2\",\n",
        "            \"--retry-connrefused\",\n",
        "            \"--waitretry=5\",\n",
        "            \"--user-agent=Mozilla/5.0 (compatible; WAN2GP)\",\n",
        "            \"--no-check-certificate\",\n",
        "            \"-O\", str(full_path),\n",
        "            url\n",
        "        ], capture_output=True, text=True, timeout=1800)\n",
        "\n",
        "        if result.returncode == 0 and full_path.exists():\n",
        "            print(\"âœ… wget download successful\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"âŒ wget failed: {result.stderr[:100] if result.stderr else 'Unknown error'}\")\n",
        "            return False\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"âŒ wget not found\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ wget error: {e}\")\n",
        "        return False\n",
        "\n",
        "def download_with_urllib(url, full_path):\n",
        "    \"\"\"Download with Python urllib\"\"\"\n",
        "    try:\n",
        "        print(\"ğŸ”„ Trying Python urllib...\")\n",
        "        print(\"â³ This may take several minutes for large files...\")\n",
        "\n",
        "        def show_progress(block_num, block_size, total_size):\n",
        "            downloaded = block_num * block_size\n",
        "            if total_size > 0:\n",
        "                percent = min(100, (downloaded * 100) // total_size)\n",
        "                if downloaded % (1024*1024*25) == 0:  # Every 25MB\n",
        "                    print(f\"   Progress: {percent}% ({downloaded:,}/{total_size:,} bytes)\")\n",
        "\n",
        "        # Create request with headers to avoid bot detection\n",
        "        request = urllib.request.Request(url)\n",
        "        request.add_header('User-Agent', 'Mozilla/5.0 (compatible; WAN2GP)')\n",
        "\n",
        "        urllib.request.urlretrieve(url, str(full_path), reporthook=show_progress)\n",
        "\n",
        "        if full_path.exists() and full_path.stat().st_size > 1024*1024:\n",
        "            print(\"âœ… Python urllib download successful\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"âŒ Python urllib download failed\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Python urllib error: {e}\")\n",
        "        return False\n",
        "\n",
        "def verify_all_models(models_config):\n",
        "    \"\"\"Verify all downloaded models\"\"\"\n",
        "    print(\"ğŸ” Verifying all model installations...\")\n",
        "\n",
        "    all_verified = True\n",
        "\n",
        "    for model_id, config in models_config.items():\n",
        "        model_path = Path(config[\"destination\"]) / config[\"filename\"]\n",
        "\n",
        "        if model_path.exists():\n",
        "            file_size = model_path.stat().st_size\n",
        "            print(f\"âœ… {config['name']}: {file_size:,} bytes\")\n",
        "        else:\n",
        "            print(f\"âŒ {config['name']}: Missing\")\n",
        "            all_verified = False\n",
        "\n",
        "    # Check directory structure\n",
        "    print(f\"\\nğŸ“ Directory verification:\")\n",
        "    required_dirs = [\"ckpts/mask\", \"ckpts/pose\"]\n",
        "    for directory in required_dirs:\n",
        "        dir_path = Path(directory)\n",
        "        if dir_path.exists():\n",
        "            files_count = len(list(dir_path.glob(\"*\")))\n",
        "            print(f\"âœ… {directory}/: {files_count} files\")\n",
        "        else:\n",
        "            print(f\"âŒ {directory}/: Missing\")\n",
        "            all_verified = False\n",
        "\n",
        "    return all_verified\n",
        "\n",
        "# Execute the complete fix\n",
        "print(\"ğŸ¯ Starting Complete WAN2GP Missing Models Fix...\")\n",
        "print(\"ğŸ”— Your WAN2GP is accessible at: https://72332e60d2557f208b.gradio.live\")\n",
        "print(\"ğŸ’¡ This fix will resolve all missing model issues\")\n",
        "print()\n",
        "\n",
        "success = complete_wan2gp_fix()\n",
        "\n",
        "if success:\n",
        "    print(\"\\nğŸ‰ Complete Fix Successfully Applied!\")\n",
        "    print(\"âœ… All missing models have been downloaded\")\n",
        "    print(\"âœ… HuggingFace authentication issues resolved\")\n",
        "    print(\"âœ… WAN2GP should now have full functionality\")\n",
        "    print(\"\\nğŸš€ Action Required:\")\n",
        "    print(\"1. Refresh your WAN2GP browser tab\")\n",
        "    print(\"2. Test mask editing features (should work now)\")\n",
        "    print(\"3. Test pose detection features (should work now)\")\n",
        "    print(\"4. No more 'Unable to find file' errors\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ Some issues may remain\")\n",
        "    print(\"ğŸ’¡ Basic video generation should still work\")\n",
        "    print(\"ğŸ”„ You can re-run this cell to retry failed downloads\")\n",
        "    print(\"ğŸŒ Try refreshing your browser tab anyway\")\n"
      ],
      "metadata": {
        "id": "ofqZOsswA9V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell (4) WAN2GP Launch - Enhanced with Public Gradio Link & Error Handling v5.0\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import signal\n",
        "import time\n",
        "import sys\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def jupyter_lab_title(title):\n",
        "    if \"google.colab\" not in sys.modules:\n",
        "        from IPython.display import display, Markdown\n",
        "        display(Markdown(f\"## {title}\"))\n",
        "\n",
        "jupyter_lab_title(\"WAN2GP Launch - Enhanced with Public Gradio Link & Error Handling\")\n",
        "\n",
        "class WAN2GPLauncher:\n",
        "    def __init__(self):\n",
        "        # Use global platform configuration from Cell 1\n",
        "        try:\n",
        "            self.current_platform = current_platform\n",
        "            self.pip_cmd = pip_cmd\n",
        "            self.python_cmd = python_cmd\n",
        "            self.use_venv = use_venv\n",
        "        except NameError:\n",
        "            print(\"âš ï¸ Platform detection variables not found. Using fallback detection.\")\n",
        "            self.detect_platform_fallback()\n",
        "\n",
        "        # CRITICAL ENFORCEMENT: Ensure venv for Google Colab per system instructions\n",
        "        self.enforce_colab_venv()\n",
        "\n",
        "        # Model configurations from documentation[3][14]\n",
        "        self.model_options = {\n",
        "            \"1\": {\n",
        "                \"flag\": \"--t2v-1-3B\",\n",
        "                \"name\": \"Wan 2.1 Text2Video 1.3B\",\n",
        "                \"vram\": \"6GB\",\n",
        "                \"speed\": \"Fast\",\n",
        "                \"description\": \"Fast generation, lower VRAM\"\n",
        "            },\n",
        "            \"2\": {\n",
        "                \"flag\": \"--t2v-14B\",\n",
        "                \"name\": \"Wan 2.1 Text2Video 14B\",\n",
        "                \"vram\": \"12GB\",\n",
        "                \"speed\": \"Slower\",\n",
        "                \"description\": \"High quality, more VRAM\"\n",
        "            },\n",
        "            \"3\": {\n",
        "                \"flag\": \"--i2v-1-3B\",\n",
        "                \"name\": \"Wan Fun InP 1.3B\",\n",
        "                \"vram\": \"6GB\",\n",
        "                \"speed\": \"Fast\",\n",
        "                \"description\": \"Image-to-video, fast\"\n",
        "            },\n",
        "            \"4\": {\n",
        "                \"flag\": \"--i2v-14B\",\n",
        "                \"name\": \"Wan 2.1 Image2Video 14B\",\n",
        "                \"vram\": \"12GB\",\n",
        "                \"speed\": \"Slower\",\n",
        "                \"description\": \"High quality I2V\"\n",
        "            },\n",
        "            \"5\": {\n",
        "                \"flag\": \"--vace-1-3B\",\n",
        "                \"name\": \"Wan Vace 1.3B\",\n",
        "                \"vram\": \"6GB\",\n",
        "                \"speed\": \"Fast\",\n",
        "                \"description\": \"ControlNet, motion transfer\"\n",
        "            },\n",
        "            \"6\": {\n",
        "                \"flag\": \"--vace-14B\",\n",
        "                \"name\": \"Wan Vace 14B\",\n",
        "                \"vram\": \"12GB\",\n",
        "                \"speed\": \"Slower\",\n",
        "                \"description\": \"Advanced ControlNet features\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Performance configurations from documentation[5][13]\n",
        "        self.performance_options = {\n",
        "            \"A\": {\n",
        "                \"name\": \"Low VRAM Mode\",\n",
        "                \"args\": [\"--profile\", \"4\", \"--attention\", \"sdpa\"],\n",
        "                \"description\": \"Memory efficient, stable (RTX 10XX-20XX)\"\n",
        "            },\n",
        "            \"B\": {\n",
        "                \"name\": \"High Performance Mode\",\n",
        "                \"args\": [\"--compile\", \"--attention\", \"sage\", \"--profile\", \"3\"],\n",
        "                \"description\": \"Fast generation (RTX 30XX-40XX)\"\n",
        "            },\n",
        "            \"C\": {\n",
        "                \"name\": \"Emergency Fallback\",\n",
        "                \"args\": [\"--attention\", \"sdpa\", \"--profile\", \"4\", \"--fp16\"],\n",
        "                \"description\": \"Maximum compatibility\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def detect_platform_fallback(self):\n",
        "        \"\"\"Fallback platform detection - ENFORCES VENV FOR COLAB per system instructions\"\"\"\n",
        "        if \"google.colab\" in sys.modules:\n",
        "            self.current_platform = \"Google Colab\"\n",
        "            self.pip_cmd, self.python_cmd, self.use_venv = \".venv/bin/pip\", \".venv/bin/python\", True\n",
        "        elif any([\"lightning\" in str(sys.executable).lower(), \"teamspace\" in os.getcwd()]):\n",
        "            self.current_platform = \"Lightning AI\"\n",
        "            self.pip_cmd, self.python_cmd, self.use_venv = \"pip\", \"python\", False\n",
        "        else:\n",
        "            self.current_platform = \"Generic\"\n",
        "            self.pip_cmd, self.python_cmd, self.use_venv = \".venv/bin/pip\", \".venv/bin/python\", True\n",
        "\n",
        "    def enforce_colab_venv(self):\n",
        "        \"\"\"CRITICAL: Enforce venv usage for Google Colab per system instructions\"\"\"\n",
        "        if hasattr(self, 'current_platform') and self.current_platform == \"Google Colab\":\n",
        "            if not hasattr(self, 'use_venv') or not self.use_venv or self.python_cmd != \".venv/bin/python\":\n",
        "                print(\"ğŸš¨ ENFORCING: Google Colab MUST use venv per system instructions\")\n",
        "                self.use_venv = True\n",
        "                self.pip_cmd = \".venv/bin/pip\"\n",
        "                self.python_cmd = \".venv/bin/python\"\n",
        "                print(\"âœ… ENFORCED: Launch system using venv commands\")\n",
        "            else:\n",
        "                print(\"âœ… VERIFIED: Launch system correctly using venv for Colab\")\n",
        "\n",
        "    def setup_runtime_environment(self):\n",
        "        \"\"\"Setup runtime environment to prevent common errors\"\"\"\n",
        "        print(\"ğŸ”§ Setting up runtime environment...\")\n",
        "\n",
        "        # Fix environment variables for headless operation\n",
        "        env_fixes = {\n",
        "            'XDG_RUNTIME_DIR': '/tmp/runtime-root',\n",
        "            'ALSA_CARD': 'null',\n",
        "            'PULSE_SERVER': 'unix:/dev/null',\n",
        "            'MPLBACKEND': 'Agg',\n",
        "            'MATPLOTLIB_BACKEND': 'Agg'\n",
        "        }\n",
        "\n",
        "        for key, value in env_fixes.items():\n",
        "            os.environ[key] = value\n",
        "            print(f\"   âœ… {key} = {value}\")\n",
        "\n",
        "        # Create runtime directory\n",
        "        os.makedirs('/tmp/runtime-root', exist_ok=True)\n",
        "\n",
        "        print(\"âœ… Runtime environment configured for headless operation\")\n",
        "        return True\n",
        "\n",
        "    def show_launch_options(self):\n",
        "        \"\"\"Display launch options with public link information\"\"\"\n",
        "        display(HTML(f\"\"\"\n",
        "        <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "                   color: white; padding: 20px; border-radius: 10px; text-align: center;\n",
        "                   margin: 10px 0; box-shadow: 0 4px 15px rgba(0,0,0,0.2);\">\n",
        "            <h2>ğŸš€ WAN2GP Launch Options</h2>\n",
        "            <p><strong>Platform:</strong> {self.current_platform}</p>\n",
        "            <p><strong>Python Command:</strong> {self.python_cmd}</p>\n",
        "            <p>Includes <strong>--share</strong> flag for public Gradio link generation</p>\n",
        "        </div>\n",
        "        \"\"\"))\n",
        "\n",
        "        print(\"ğŸ¯ WAN2GP Model Selection (with Public Gradio Link):\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        for key, config in self.model_options.items():\n",
        "            print(f\"{key}. {config['name']}\")\n",
        "            print(f\"   VRAM: {config['vram']} | Speed: {config['speed']}\")\n",
        "            print(f\"   {config['description']}\")\n",
        "            print()\n",
        "\n",
        "        print(\"ğŸ”§ Performance Optimization Options:\")\n",
        "        print(\"=\" * 40)\n",
        "        for key, config in self.performance_options.items():\n",
        "            print(f\"{key}. {config['name']}\")\n",
        "            print(f\"   {config['description']}\")\n",
        "            print()\n",
        "\n",
        "        # Get user selections\n",
        "        model_choice = input(\"ğŸ¯ Select model (1-6, or Enter for default Wan T2V 1.3B): \").strip()\n",
        "        perf_choice = input(\"âš¡ Select performance (A-C, or Enter for Low VRAM): \").strip().upper()\n",
        "\n",
        "        # Process selections\n",
        "        if model_choice in self.model_options:\n",
        "            selected_model = self.model_options[model_choice]\n",
        "            model_flag = selected_model[\"flag\"]\n",
        "            model_name = selected_model[\"name\"]\n",
        "        else:\n",
        "            model_flag = \"--t2v-1-3B\"\n",
        "            model_name = \"Wan 2.1 Text2Video 1.3B (default)\"\n",
        "\n",
        "        if perf_choice in self.performance_options:\n",
        "            perf_config = self.performance_options[perf_choice]\n",
        "            perf_args = perf_config[\"args\"]\n",
        "            perf_name = perf_config[\"name\"]\n",
        "        else:\n",
        "            perf_args = [\"--profile\", \"4\", \"--attention\", \"sdpa\"]\n",
        "            perf_name = \"Low VRAM Mode (default)\"\n",
        "\n",
        "        return model_flag, model_name, perf_args, perf_name\n",
        "\n",
        "    def launch_wan2gp_with_public_link(self, model_flag, model_name, perf_args, perf_name):\n",
        "        \"\"\"Launch WAN2GP with public Gradio link and comprehensive error handling\"\"\"\n",
        "        print(\"ğŸš€ Launching WAN2GP with Public Gradio Link...\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"Platform: {self.current_platform}\")\n",
        "        print(f\"Python Command: {self.python_cmd}\")\n",
        "        print(f\"Selected Model: {model_name}\")\n",
        "        print(f\"Performance Mode: {perf_name}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Final enforcement check per system instructions\n",
        "        if self.current_platform == \"Google Colab\" and not self.use_venv:\n",
        "            print(\"ğŸš¨ FATAL: Google Colab not using venv per system instructions - ABORTING\")\n",
        "            return False\n",
        "\n",
        "        # Setup runtime environment\n",
        "        self.setup_runtime_environment()\n",
        "\n",
        "        # Verify we're in the correct directory\n",
        "        if not os.path.exists(\"wgp.py\"):\n",
        "            print(\"âŒ wgp.py not found. Make sure you're in the Wan2GP directory.\")\n",
        "            print(f\"ğŸ“ Current directory: {os.getcwd()}\")\n",
        "            return False\n",
        "\n",
        "        # Build launch command with public link support from CLI documentation[5][13]\n",
        "        base_cmd = [\n",
        "            self.python_cmd,\n",
        "            \"wgp.py\",\n",
        "            model_flag,\n",
        "            \"--share\",        # Creates public HuggingFace URL per documentation[5]\n",
        "            \"--listen\",       # Make server accessible on network\n",
        "            \"--server-port\", \"7860\"\n",
        "        ]\n",
        "\n",
        "        # Add performance arguments\n",
        "        base_cmd.extend(perf_args)\n",
        "\n",
        "        # Add stability arguments from troubleshooting guide[8][16]\n",
        "        stability_args = [\n",
        "            \"--verbose\", \"1\"  # Moderate verbosity for monitoring\n",
        "        ]\n",
        "        base_cmd.extend(stability_args)\n",
        "\n",
        "        print(f\"ğŸ¯ Launch command: {' '.join(base_cmd)}\")\n",
        "        print()\n",
        "        print(\"ğŸ”„ Starting WAN2GP...\")\n",
        "        print(\"ğŸŒ Local interface: http://localhost:7860\")\n",
        "        print(\"ğŸ”— Public Gradio link will be generated and displayed below\")\n",
        "        print(\"â¹ï¸ Press Ctrl+C in this cell to stop WAN2GP\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        try:\n",
        "            # Launch WAN2GP with real-time output monitoring\n",
        "            process = subprocess.Popen(\n",
        "                base_cmd,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.STDOUT,\n",
        "                universal_newlines=True,\n",
        "                bufsize=1\n",
        "            )\n",
        "\n",
        "            gradio_link_found = False\n",
        "            start_time = time.time()\n",
        "            output_lines = []\n",
        "\n",
        "            print(\"ğŸ“‹ WAN2GP Output:\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            # Monitor output for important information\n",
        "            while True:\n",
        "                output = process.stdout.readline()\n",
        "                if output == '' and process.poll() is not None:\n",
        "                    break\n",
        "\n",
        "                if output:\n",
        "                    line = output.strip()\n",
        "                    print(line)\n",
        "                    output_lines.append(line)\n",
        "\n",
        "                    # Detect and highlight public Gradio link\n",
        "                    if \"Running on public URL:\" in line and not gradio_link_found:\n",
        "                        gradio_link_found = True\n",
        "                        # Extract URL\n",
        "                        url = line.split(\"Running on public URL:\")[-1].strip()\n",
        "\n",
        "                        # Display prominent link notification\n",
        "                        display(HTML(f\"\"\"\n",
        "                        <div style=\"background: linear-gradient(90deg, #00ff41, #00d4ff);\n",
        "                                    padding: 20px; border-radius: 15px; margin: 15px 0;\n",
        "                                    border: 3px solid #00ff41; text-align: center;\n",
        "                                    box-shadow: 0 8px 25px rgba(0,255,65,0.3);\">\n",
        "                            <h2 style=\"color: #000; margin: 0; font-size: 24px;\">\n",
        "                                ğŸŒ PUBLIC GRADIO LINK READY!\n",
        "                            </h2>\n",
        "                            <p style=\"color: #000; margin: 10px 0; font-size: 18px;\">\n",
        "                                <strong>Click here to access WAN2GP remotely:</strong>\n",
        "                            </p>\n",
        "                            <div style=\"background: white; padding: 15px; border-radius: 10px;\n",
        "                                        margin: 10px 0; border: 2px solid #000;\">\n",
        "                                <a href=\"{url}\" target=\"_blank\"\n",
        "                                   style=\"color: #000; font-weight: bold; font-size: 20px;\n",
        "                                          text-decoration: none;\">\n",
        "                                    {url}\n",
        "                                </a>\n",
        "                            </div>\n",
        "                            <p style=\"color: #000; margin: 5px 0; font-size: 14px;\">\n",
        "                                ğŸ•’ Link expires in 72 hours â€¢ ğŸ”— Right-click to copy\n",
        "                            </p>\n",
        "                        </div>\n",
        "                        \"\"\"))\n",
        "\n",
        "                    # Monitor for successful startup indicators\n",
        "                    if \"Running on local URL:\" in line:\n",
        "                        print(\"âœ… Local server started successfully\")\n",
        "\n",
        "                    # Monitor for model loading\n",
        "                    if \"Loading model\" in line or \"Model loaded\" in line:\n",
        "                        print(\"ğŸ“¦ Model loading detected...\")\n",
        "\n",
        "                    # Filter harmless warnings but continue execution\n",
        "                    harmless_warnings = [\n",
        "                        \"xdg_runtime_dir\", \"alsa lib\", \"unknown pcm\",\n",
        "                        \"no such file or directory\", \"switching to fp16\"\n",
        "                    ]\n",
        "\n",
        "                    if any(warning in line.lower() for warning in harmless_warnings):\n",
        "                        continue  # Don't treat as errors\n",
        "\n",
        "                # Timeout check (10 minutes for initial startup)\n",
        "                if time.time() - start_time > 600:\n",
        "                    print(\"âš ï¸ Startup timeout reached (10 minutes)\")\n",
        "                    print(\"ğŸ” Check output above for any error messages\")\n",
        "                    break\n",
        "\n",
        "            # Final status check\n",
        "            if not gradio_link_found:\n",
        "                print(\"\\nâš ï¸ Public Gradio link not detected in output\")\n",
        "                print(\"ğŸ” WAN2GP may still be accessible locally at: http://localhost:7860\")\n",
        "                print(\"ğŸ’¡ Check the output above for any error messages\")\n",
        "\n",
        "            # Keep process running until interrupted\n",
        "            try:\n",
        "                print(\"\\nâ¸ï¸ WAN2GP is running. Press Ctrl+C to stop...\")\n",
        "                process.wait()\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"\\nâ¹ï¸ Stopping WAN2GP...\")\n",
        "                process.terminate()\n",
        "                # Give process time to clean up\n",
        "                try:\n",
        "                    process.wait(timeout=5)\n",
        "                except subprocess.TimeoutExpired:\n",
        "                    process.kill()\n",
        "                print(\"âœ… WAN2GP stopped successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error launching WAN2GP: {e}\")\n",
        "            print(\"\\nğŸ”§ Troubleshooting suggestions:\")\n",
        "            print(\"1. Verify models were downloaded in Cell 3\")\n",
        "            print(\"2. Check that wgp.py exists in current directory\")\n",
        "            print(\"3. Try the emergency fallback command below\")\n",
        "            print(\"\\nğŸ†˜ Emergency fallback command:\")\n",
        "            print(f\"{self.python_cmd} wgp.py --t2v-1-3B --attention sdpa --profile 4\")\n",
        "            return False\n",
        "\n",
        "        return gradio_link_found\n",
        "\n",
        "# Create launcher instance\n",
        "launcher = WAN2GPLauncher()\n",
        "\n",
        "# Show options and get user selections\n",
        "model_flag, model_name, perf_args, perf_name = launcher.show_launch_options()\n",
        "\n",
        "# Launch WAN2GP with selected configuration\n",
        "success = launcher.launch_wan2gp_with_public_link(model_flag, model_name, perf_args, perf_name)\n",
        "\n",
        "if success:\n",
        "    print(\"\\nğŸ‰ WAN2GP launch completed successfully!\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ Launch encountered issues - check output above for details\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GU-zDsAW_MwT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6e97763a-b893-4388-ae82-8baa98b26ff9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… VERIFIED: Launch system correctly using venv for Colab\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
              "                   color: white; padding: 20px; border-radius: 10px; text-align: center;\n",
              "                   margin: 10px 0; box-shadow: 0 4px 15px rgba(0,0,0,0.2);\">\n",
              "            <h2>ğŸš€ WAN2GP Launch Options</h2>\n",
              "            <p><strong>Platform:</strong> Google Colab</p>\n",
              "            <p><strong>Python Command:</strong> .venv/bin/python</p>\n",
              "            <p>Includes <strong>--share</strong> flag for public Gradio link generation</p>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¯ WAN2GP Model Selection (with Public Gradio Link):\n",
            "======================================================================\n",
            "1. Wan 2.1 Text2Video 1.3B\n",
            "   VRAM: 6GB | Speed: Fast\n",
            "   Fast generation, lower VRAM\n",
            "\n",
            "2. Wan 2.1 Text2Video 14B\n",
            "   VRAM: 12GB | Speed: Slower\n",
            "   High quality, more VRAM\n",
            "\n",
            "3. Wan Fun InP 1.3B\n",
            "   VRAM: 6GB | Speed: Fast\n",
            "   Image-to-video, fast\n",
            "\n",
            "4. Wan 2.1 Image2Video 14B\n",
            "   VRAM: 12GB | Speed: Slower\n",
            "   High quality I2V\n",
            "\n",
            "5. Wan Vace 1.3B\n",
            "   VRAM: 6GB | Speed: Fast\n",
            "   ControlNet, motion transfer\n",
            "\n",
            "6. Wan Vace 14B\n",
            "   VRAM: 12GB | Speed: Slower\n",
            "   Advanced ControlNet features\n",
            "\n",
            "ğŸ”§ Performance Optimization Options:\n",
            "========================================\n",
            "A. Low VRAM Mode\n",
            "   Memory efficient, stable (RTX 10XX-20XX)\n",
            "\n",
            "B. High Performance Mode\n",
            "   Fast generation (RTX 30XX-40XX)\n",
            "\n",
            "C. Emergency Fallback\n",
            "   Maximum compatibility\n",
            "\n",
            "ğŸ¯ Select model (1-6, or Enter for default Wan T2V 1.3B): 1\n",
            "âš¡ Select performance (A-C, or Enter for Low VRAM): \n",
            "ğŸš€ Launching WAN2GP with Public Gradio Link...\n",
            "======================================================================\n",
            "Platform: Google Colab\n",
            "Python Command: .venv/bin/python\n",
            "Selected Model: Wan 2.1 Text2Video 1.3B\n",
            "Performance Mode: Low VRAM Mode (default)\n",
            "======================================================================\n",
            "ğŸ”§ Setting up runtime environment...\n",
            "   âœ… XDG_RUNTIME_DIR = /tmp/runtime-root\n",
            "   âœ… ALSA_CARD = null\n",
            "   âœ… PULSE_SERVER = unix:/dev/null\n",
            "   âœ… MPLBACKEND = Agg\n",
            "   âœ… MATPLOTLIB_BACKEND = Agg\n",
            "âœ… Runtime environment configured for headless operation\n",
            "ğŸ¯ Launch command: .venv/bin/python wgp.py --t2v-1-3B --share --listen --server-port 7860 --profile 4 --attention sdpa --verbose 1\n",
            "\n",
            "ğŸ”„ Starting WAN2GP...\n",
            "ğŸŒ Local interface: http://localhost:7860\n",
            "ğŸ”— Public Gradio link will be generated and displayed below\n",
            "â¹ï¸ Press Ctrl+C in this cell to stop WAN2GP\n",
            "======================================================================\n",
            "ğŸ“‹ WAN2GP Output:\n",
            "--------------------------------------------------\n",
            "ALSA lib confmisc.c:855:(parse_card) cannot find card 'null'\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such device\n",
            "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such device\n",
            "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such device\n",
            "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such device\n",
            "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
            "ALSA lib confmisc.c:855:(parse_card) cannot find card 'null'\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such device\n",
            "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such device\n",
            "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such device\n",
            "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such device\n",
            "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-2265421813.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;31m# Launch WAN2GP with selected configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch_wan2gp_with_public_link\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_flag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperf_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperf_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-6-2265421813.py\u001b[0m in \u001b[0;36mlaunch_wan2gp_with_public_link\u001b[0;34m(self, model_flag, model_name, perf_args, perf_name)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;31m# Monitor output for important information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!.venv/bin/python wgp.py --t2v-1-3B --share --listen --server-port 7860 --profile 4 --attention sdpa --verbose 1"
      ],
      "metadata": {
        "id": "xgWrz0qzDSMc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72f57529-f48b-4ee9-cf60-95e47c359daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Switching to FP16 models when possible as GPU architecture doesn't support optimed BF16 Kernels\n",
            "ALSA lib confmisc.c:855:(parse_card) cannot find card 'null'\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such device\n",
            "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such device\n",
            "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such device\n",
            "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such device\n",
            "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
            "ALSA lib confmisc.c:855:(parse_card) cannot find card 'null'\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such device\n",
            "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such device\n",
            "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such device\n",
            "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such device\n",
            "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
            "Switching to FP16 models when possible as GPU architecture doesn't support optimed BF16 Kernels\n",
            "* Running on local URL:  http://0.0.0.0:7860\n",
            "* Running on public URL: https://72332e60d2557f208b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Initializing BaseSegmenter to cuda\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/gradio/blocks.py\", line 2137, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/gradio/blocks.py\", line 1663, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/gradio/utils.py\", line 890, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/wgp.py\", line 7293, in select_tab\n",
            "    return set_new_tab(tab_state, evt.index)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/wgp.py\", line 7288, in set_new_tab\n",
            "    vmc_event_handler(True)\n",
            "  File \"/content/Wan2GP/preprocessing/matanyone/app.py\", line 498, in load_unload_models\n",
            "    model = MaskGenerator(sam_checkpoint, arg_device)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/preprocessing/matanyone/app.py\", line 36, in __init__\n",
            "    self.samcontroler = SamControler(sam_checkpoint, arg_sam_model_type, arg_device)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/preprocessing/matanyone/tools/interact_tools.py\", line 35, in __init__\n",
            "    self.sam_controler = BaseSegmenter(SAM_checkpoint, model_type, device)\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/preprocessing/matanyone/tools/base_segmenter.py\", line 35, in __init__\n",
            "    offload.load_model_data(self.model, \"ckpts/mask/sam_vit_h_4b8939_fp16.safetensors\")\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/mmgp/offload.py\", line 1366, in load_model_data\n",
            "    raise Exception(\"Unable to find file\")\n",
            "Exception: Unable to find file\n",
            "Initializing BaseSegmenter to cuda\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/gradio/blocks.py\", line 2137, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/gradio/blocks.py\", line 1663, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/gradio/utils.py\", line 890, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/wgp.py\", line 7293, in select_tab\n",
            "    return set_new_tab(tab_state, evt.index)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/wgp.py\", line 7288, in set_new_tab\n",
            "    vmc_event_handler(True)\n",
            "  File \"/content/Wan2GP/preprocessing/matanyone/app.py\", line 498, in load_unload_models\n",
            "    model = MaskGenerator(sam_checkpoint, arg_device)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/preprocessing/matanyone/app.py\", line 36, in __init__\n",
            "    self.samcontroler = SamControler(sam_checkpoint, arg_sam_model_type, arg_device)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/preprocessing/matanyone/tools/interact_tools.py\", line 35, in __init__\n",
            "    self.sam_controler = BaseSegmenter(SAM_checkpoint, model_type, device)\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/preprocessing/matanyone/tools/base_segmenter.py\", line 35, in __init__\n",
            "    offload.load_model_data(self.model, \"ckpts/mask/sam_vit_h_4b8939_fp16.safetensors\")\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/mmgp/offload.py\", line 1366, in load_model_data\n",
            "    raise Exception(\"Unable to find file\")\n",
            "Exception: Unable to find file\n",
            "Initializing BaseSegmenter to cuda\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/gradio/blocks.py\", line 2137, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/gradio/blocks.py\", line 1663, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/gradio/utils.py\", line 890, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/wgp.py\", line 7293, in select_tab\n",
            "    return set_new_tab(tab_state, evt.index)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/wgp.py\", line 7288, in set_new_tab\n",
            "    vmc_event_handler(True)\n",
            "  File \"/content/Wan2GP/preprocessing/matanyone/app.py\", line 498, in load_unload_models\n",
            "    model = MaskGenerator(sam_checkpoint, arg_device)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/preprocessing/matanyone/app.py\", line 36, in __init__\n",
            "    self.samcontroler = SamControler(sam_checkpoint, arg_sam_model_type, arg_device)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/preprocessing/matanyone/tools/interact_tools.py\", line 35, in __init__\n",
            "    self.sam_controler = BaseSegmenter(SAM_checkpoint, model_type, device)\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/preprocessing/matanyone/tools/base_segmenter.py\", line 35, in __init__\n",
            "    offload.load_model_data(self.model, \"ckpts/mask/sam_vit_h_4b8939_fp16.safetensors\")\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/mmgp/offload.py\", line 1366, in load_model_data\n",
            "    raise Exception(\"Unable to find file\")\n",
            "Exception: Unable to find file\n",
            "Initializing BaseSegmenter to cuda\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/gradio/blocks.py\", line 2137, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/gradio/blocks.py\", line 1663, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/gradio/utils.py\", line 890, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/wgp.py\", line 7293, in select_tab\n",
            "    return set_new_tab(tab_state, evt.index)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/wgp.py\", line 7288, in set_new_tab\n",
            "    vmc_event_handler(True)\n",
            "  File \"/content/Wan2GP/preprocessing/matanyone/app.py\", line 498, in load_unload_models\n",
            "    model = MaskGenerator(sam_checkpoint, arg_device)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/preprocessing/matanyone/app.py\", line 36, in __init__\n",
            "    self.samcontroler = SamControler(sam_checkpoint, arg_sam_model_type, arg_device)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/preprocessing/matanyone/tools/interact_tools.py\", line 35, in __init__\n",
            "    self.sam_controler = BaseSegmenter(SAM_checkpoint, model_type, device)\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/preprocessing/matanyone/tools/base_segmenter.py\", line 35, in __init__\n",
            "    offload.load_model_data(self.model, \"ckpts/mask/sam_vit_h_4b8939_fp16.safetensors\")\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/mmgp/offload.py\", line 1366, in load_model_data\n",
            "    raise Exception(\"Unable to find file\")\n",
            "Exception: Unable to find file\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/requests/models.py\", line 1026, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/DeepBeepMeep/Wan2.1/resolve/main/pose/dw-ll_ucoco_384.onnx\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Wan2GP/wgp.py\", line 4669, in generate_video_error_handler\n",
            "    generate_video(task, send_cmd,  **params)\n",
            "  File \"/content/Wan2GP/wgp.py\", line 3688, in generate_video\n",
            "    wan_model, offloadobj, trans = load_models(model_type)\n",
            "                                   ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/wgp.py\", line 2498, in load_models\n",
            "    download_models(filename, file_model_type)\n",
            "  File \"/content/Wan2GP/wgp.py\", line 2184, in download_models\n",
            "    process_files_def(**shared_def)\n",
            "  File \"/content/Wan2GP/wgp.py\", line 2151, in process_files_def\n",
            "    hf_hub_download(repo_id=repoId,  filename=onefile, local_dir = targetRoot, subfolder=sourceFolder)\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 988, in hf_hub_download\n",
            "    return _hf_hub_download_to_local_dir(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1243, in _hf_hub_download_to_local_dir\n",
            "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1645, in _raise_on_head_call_error\n",
            "    raise head_call_error\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1533, in _get_metadata_or_catch_error\n",
            "    metadata = get_hf_file_metadata(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1450, in get_hf_file_metadata\n",
            "    r = _request_wrapper(\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 286, in _request_wrapper\n",
            "    response = _request_wrapper(\n",
            "               ^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 310, in _request_wrapper\n",
            "    hf_raise_for_status(response)\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py\", line 459, in hf_raise_for_status\n",
            "    raise _format(RepositoryNotFoundError, message, response) from e\n",
            "huggingface_hub.errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-6869fa3c-778ee69b6a402f90395e67e7;42e131ab-ea65-48ab-8139-a77dc05ea9b0)\n",
            "\n",
            "Repository Not Found for url: https://huggingface.co/DeepBeepMeep/Wan2.1/resolve/main/pose/dw-ll_ucoco_384.onnx.\n",
            "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
            "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
            "User Access Token \"fred\" is expired\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell (1) - WAN2GP Comprehensive Diagnostic and Auto-Repair System (Latest v6.2 Compatible)\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "from IPython.display import display, HTML, Markdown\n",
        "import importlib.util\n",
        "\n",
        "class WAN2GPDiagnosticAndRepair:\n",
        "    def __init__(self):\n",
        "        self.platform = self.detect_platform()\n",
        "        self.pip_cmd, self.python_cmd, self.use_venv = self.get_platform_commands()\n",
        "        self.issues_found = []\n",
        "        self.repairs_applied = []\n",
        "        self.gpu_info = {}\n",
        "\n",
        "    def detect_platform(self):\n",
        "        \"\"\"Enhanced platform detection with comprehensive indicators\"\"\"\n",
        "        # Lightning AI detection - multiple indicators for reliability\n",
        "        lightning_indicators = [\n",
        "            \"lightning\" in str(sys.executable).lower(),\n",
        "            \"teamspace-studios\" in os.getcwd(),\n",
        "            \"LIGHTNING_CLOUDSPACE_HOST\" in os.environ,\n",
        "            \"LIGHTNING_CLOUDSPACE_ID\" in os.environ,\n",
        "            \"/commands/python\" in str(sys.executable),\n",
        "            \"/home/zeus/miniconda3/envs/cloudspace\" in str(sys.executable),\n",
        "            os.path.exists(\"/teamspace\"),\n",
        "            os.path.exists(\"/commands\")\n",
        "        ]\n",
        "\n",
        "        # Google Colab detection\n",
        "        colab_indicators = [\n",
        "            \"google.colab\" in sys.modules,\n",
        "            \"/content\" in os.getcwd()\n",
        "        ]\n",
        "\n",
        "        # Vast.AI detection\n",
        "        vast_indicators = [\n",
        "            \"VAST_CONTAINER_LABEL\" in os.environ,\n",
        "            \"/workspace\" in os.getcwd(),\n",
        "            \"vast\" in os.environ.get(\"HOSTNAME\", \"\").lower()\n",
        "        ]\n",
        "\n",
        "        if any(lightning_indicators):\n",
        "            return \"Lightning AI\"\n",
        "        elif any(colab_indicators):\n",
        "            return \"Google Colab\"\n",
        "        elif any(vast_indicators):\n",
        "            return \"Vast.AI/Generic\"\n",
        "        else:\n",
        "            return \"Vast.AI/Generic\"\n",
        "\n",
        "    def get_platform_commands(self):\n",
        "        \"\"\"Get platform-specific pip and python commands\"\"\"\n",
        "        if self.platform == \"Lightning AI\":\n",
        "            return \"pip\", \"python\", False  # (pip_cmd, python_cmd, use_venv)\n",
        "        elif self.platform == \"Google Colab\":\n",
        "            return \".venv/bin/pip\", \".venv/bin/python\", True\n",
        "        else:  # Vast.AI/Generic\n",
        "            return \".venv/bin/pip\", \".venv/bin/python\", True\n",
        "\n",
        "    def run_command_safely(self, command, description, timeout=60):\n",
        "        \"\"\"Execute command with comprehensive error handling\"\"\"\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                command, capture_output=True, text=True, timeout=timeout, shell=isinstance(command, str)\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                return True, result.stdout\n",
        "            else:\n",
        "                return False, result.stderr\n",
        "        except subprocess.TimeoutExpired:\n",
        "            return False, f\"Timeout after {timeout}s\"\n",
        "        except Exception as e:\n",
        "            return False, str(e)\n",
        "\n",
        "    def check_gpu_compatibility(self):\n",
        "        \"\"\"Comprehensive GPU detection and compatibility check\"\"\"\n",
        "        print(\"ğŸ” Checking GPU compatibility...\")\n",
        "\n",
        "        # Check NVIDIA GPU presence\n",
        "        success, output = self.run_command_safely(\"nvidia-smi\", \"GPU Detection\")\n",
        "        if not success:\n",
        "            self.issues_found.append(\"No NVIDIA GPU detected or nvidia-smi not available\")\n",
        "            return False\n",
        "\n",
        "        # Parse GPU info\n",
        "        try:\n",
        "            # Extract GPU name and VRAM from nvidia-smi output\n",
        "            lines = output.split('\\n')\n",
        "            for line in lines:\n",
        "                if 'RTX' in line or 'GTX' in line or 'Tesla' in line or 'A100' in line:\n",
        "                    gpu_name = line.split('|')[1].strip() if '|' in line else \"Unknown GPU\"\n",
        "                    self.gpu_info['name'] = gpu_name\n",
        "                    break\n",
        "\n",
        "            # Check VRAM\n",
        "            for line in lines:\n",
        "                if 'MiB' in line and '/' in line:\n",
        "                    vram_info = [part for part in line.split() if 'MiB' in part]\n",
        "                    if len(vram_info) >= 2:\n",
        "                        total_vram = int(vram_info[-1].replace('MiB', ''))\n",
        "                        self.gpu_info['vram_mb'] = total_vram\n",
        "                        self.gpu_info['vram_gb'] = total_vram / 1024\n",
        "                        break\n",
        "\n",
        "            print(f\"âœ… GPU detected: {self.gpu_info.get('name', 'Unknown')}\")\n",
        "            print(f\"âœ… VRAM: {self.gpu_info.get('vram_gb', 0):.1f}GB\")\n",
        "\n",
        "            # Check VRAM adequacy\n",
        "            vram_gb = self.gpu_info.get('vram_gb', 0)\n",
        "            if vram_gb < 6:\n",
        "                self.issues_found.append(f\"Low VRAM detected ({vram_gb:.1f}GB). Minimum 6GB recommended.\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.issues_found.append(f\"GPU info parsing failed: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def check_cuda_pytorch_compatibility(self):\n",
        "        \"\"\"Check CUDA and PyTorch compatibility\"\"\"\n",
        "        print(\"ğŸ” Checking CUDA and PyTorch compatibility...\")\n",
        "\n",
        "        # Check CUDA version\n",
        "        success, cuda_output = self.run_command_safely(\"nvcc --version\", \"CUDA Version Check\")\n",
        "        if not success:\n",
        "            success, cuda_output = self.run_command_safely(\"nvidia-smi\", \"CUDA Runtime Check\")\n",
        "\n",
        "        # Check PyTorch installation\n",
        "        try:\n",
        "            import torch\n",
        "            pytorch_version = torch.__version__\n",
        "            cuda_available = torch.cuda.is_available()\n",
        "\n",
        "            print(f\"âœ… PyTorch version: {pytorch_version}\")\n",
        "            print(f\"âœ… CUDA available in PyTorch: {cuda_available}\")\n",
        "\n",
        "            if not cuda_available:\n",
        "                self.issues_found.append(\"PyTorch cannot detect CUDA\")\n",
        "                return False\n",
        "\n",
        "            # Check for version compatibility\n",
        "            if \"50\" in self.gpu_info.get('name', '') and not pytorch_version.startswith('2.7'):\n",
        "                self.issues_found.append(\"RTX 50XX series requires PyTorch 2.7.0 or newer\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except ImportError:\n",
        "            self.issues_found.append(\"PyTorch not installed\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            self.issues_found.append(f\"PyTorch check failed: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def check_python_version(self):\n",
        "        \"\"\"Check Python version compatibility\"\"\"\n",
        "        print(\"ğŸ” Checking Python version...\")\n",
        "\n",
        "        python_version = sys.version_info\n",
        "        version_string = f\"{python_version.major}.{python_version.minor}.{python_version.micro}\"\n",
        "        print(f\"âœ… Python version: {version_string}\")\n",
        "\n",
        "        if python_version.major != 3 or python_version.minor != 10:\n",
        "            self.issues_found.append(f\"Python {version_string} detected. Python 3.10.9 recommended for best compatibility.\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def check_wan2gp_installation(self):\n",
        "        \"\"\"Check WAN2GP installation and repository\"\"\"\n",
        "        print(\"ğŸ” Checking WAN2GP installation...\")\n",
        "\n",
        "        wan2gp_paths = [\"Wan2GP\", \"wan2gp\", \"WAN2GP\", \"../Wan2GP\", \"../wan2gp\"]\n",
        "        wan2gp_found = False\n",
        "\n",
        "        for path in wan2gp_paths:\n",
        "            if os.path.exists(path):\n",
        "                wan2gp_found = True\n",
        "                wgp_py_path = os.path.join(path, \"wgp.py\")\n",
        "                if os.path.exists(wgp_py_path):\n",
        "                    print(f\"âœ… WAN2GP found at: {path}\")\n",
        "                    return True\n",
        "                break\n",
        "\n",
        "        if not wan2gp_found:\n",
        "            self.issues_found.append(\"WAN2GP repository not found\")\n",
        "            return False\n",
        "\n",
        "        self.issues_found.append(\"WAN2GP repository found but wgp.py missing\")\n",
        "        return False\n",
        "\n",
        "    def check_dependencies(self):\n",
        "        \"\"\"Check critical dependencies\"\"\"\n",
        "        print(\"ğŸ” Checking critical dependencies...\")\n",
        "\n",
        "        critical_deps = {\n",
        "            'torch': 'PyTorch',\n",
        "            'torchvision': 'TorchVision',\n",
        "            'gradio': 'Gradio',\n",
        "            'transformers': 'Transformers',\n",
        "            'accelerate': 'Accelerate',\n",
        "            'diffusers': 'Diffusers'\n",
        "        }\n",
        "\n",
        "        missing_deps = []\n",
        "        for dep, name in critical_deps.items():\n",
        "            try:\n",
        "                importlib.import_module(dep)\n",
        "                print(f\"âœ… {name} installed\")\n",
        "            except ImportError:\n",
        "                missing_deps.append(dep)\n",
        "                print(f\"âŒ {name} missing\")\n",
        "\n",
        "        if missing_deps:\n",
        "            self.issues_found.append(f\"Missing dependencies: {', '.join(missing_deps)}\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def check_attention_mechanisms(self):\n",
        "        \"\"\"Check available attention mechanisms\"\"\"\n",
        "        print(\"ğŸ” Checking attention mechanisms...\")\n",
        "\n",
        "        attention_status = {}\n",
        "\n",
        "        # Check Triton\n",
        "        try:\n",
        "            import triton\n",
        "            attention_status['triton'] = f\"âœ… Triton {triton.__version__}\"\n",
        "        except ImportError:\n",
        "            attention_status['triton'] = \"âŒ Triton not available\"\n",
        "\n",
        "        # Check SageAttention\n",
        "        try:\n",
        "            import sageattention\n",
        "            attention_status['sage'] = \"âœ… SageAttention available\"\n",
        "        except ImportError:\n",
        "            attention_status['sage'] = \"âŒ SageAttention not available\"\n",
        "\n",
        "        # Check Flash Attention\n",
        "        try:\n",
        "            import flash_attn\n",
        "            attention_status['flash'] = \"âœ… Flash Attention available\"\n",
        "        except ImportError:\n",
        "            attention_status['flash'] = \"âŒ Flash Attention not available\"\n",
        "\n",
        "        for mech, status in attention_status.items():\n",
        "            print(f\"  {status}\")\n",
        "\n",
        "        # Recommend fallback if advanced attention not available\n",
        "        if all(\"âŒ\" in status for status in attention_status.values()):\n",
        "            self.issues_found.append(\"No advanced attention mechanisms available. Will use SDPA fallback.\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def check_memory_and_performance(self):\n",
        "        \"\"\"Check memory and performance configuration\"\"\"\n",
        "        print(\"ğŸ” Checking memory and performance...\")\n",
        "\n",
        "        # Check available system RAM\n",
        "        try:\n",
        "            import psutil\n",
        "            ram_gb = psutil.virtual_memory().total / (1024**3)\n",
        "            print(f\"âœ… System RAM: {ram_gb:.1f}GB\")\n",
        "\n",
        "            if ram_gb < 16:\n",
        "                self.issues_found.append(f\"Low system RAM ({ram_gb:.1f}GB). 16GB+ recommended for best performance.\")\n",
        "        except ImportError:\n",
        "            print(\"âš ï¸ Cannot check system RAM (psutil not available)\")\n",
        "\n",
        "        # Check disk space\n",
        "        try:\n",
        "            disk_usage = shutil.disk_usage(os.getcwd())\n",
        "            free_gb = disk_usage.free / (1024**3)\n",
        "            print(f\"âœ… Free disk space: {free_gb:.1f}GB\")\n",
        "\n",
        "            if free_gb < 20:\n",
        "                self.issues_found.append(f\"Low disk space ({free_gb:.1f}GB). 20GB+ recommended.\")\n",
        "        except Exception:\n",
        "            print(\"âš ï¸ Cannot check disk space\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def auto_repair_pytorch(self):\n",
        "        \"\"\"Auto-repair PyTorch installation\"\"\"\n",
        "        print(\"ğŸ”§ Attempting PyTorch repair...\")\n",
        "\n",
        "        # Determine correct PyTorch version based on GPU\n",
        "        if \"50\" in self.gpu_info.get('name', ''):\n",
        "            # RTX 50XX series\n",
        "            pytorch_cmd = [\n",
        "                self.pip_cmd, \"install\", \"--upgrade\",\n",
        "                \"torch==2.7.0\", \"torchvision\", \"torchaudio\",\n",
        "                \"--index-url\", \"https://download.pytorch.org/whl/test/cu128\"\n",
        "            ]\n",
        "        else:\n",
        "            # RTX 10XX-40XX series\n",
        "            pytorch_cmd = [\n",
        "                self.pip_cmd, \"install\", \"--upgrade\",\n",
        "                \"torch==2.6.0\", \"torchvision\", \"torchaudio\",\n",
        "                \"--index-url\", \"https://download.pytorch.org/whl/test/cu124\"\n",
        "            ]\n",
        "\n",
        "        success, output = self.run_command_safely(pytorch_cmd, \"PyTorch Installation\", timeout=300)\n",
        "        if success:\n",
        "            self.repairs_applied.append(\"PyTorch installation repaired\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"âŒ PyTorch repair failed: {output}\")\n",
        "            return False\n",
        "\n",
        "    def auto_repair_dependencies(self):\n",
        "        \"\"\"Auto-repair missing dependencies\"\"\"\n",
        "        print(\"ğŸ”§ Installing missing dependencies...\")\n",
        "\n",
        "        # Install core requirements\n",
        "        requirements_cmd = [self.pip_cmd, \"install\", \"-r\", \"requirements.txt\"]\n",
        "        success, output = self.run_command_safely(requirements_cmd, \"Dependencies Installation\", timeout=300)\n",
        "\n",
        "        if success:\n",
        "            self.repairs_applied.append(\"Dependencies installed\")\n",
        "            return True\n",
        "        else:\n",
        "            # Fallback: install critical packages individually\n",
        "            critical_packages = [\n",
        "                \"gradio>=4.0.0\", \"transformers\", \"accelerate\", \"diffusers\",\n",
        "                \"opencv-python\", \"Pillow\", \"numpy\", \"scipy\"\n",
        "            ]\n",
        "\n",
        "            for package in critical_packages:\n",
        "                cmd = [self.pip_cmd, \"install\", package]\n",
        "                success, _ = self.run_command_safely(cmd, f\"Installing {package}\", timeout=60)\n",
        "                if success:\n",
        "                    print(f\"âœ… Installed {package}\")\n",
        "\n",
        "            self.repairs_applied.append(\"Critical dependencies installed individually\")\n",
        "            return True\n",
        "\n",
        "    def auto_repair_wan2gp_repo(self):\n",
        "        \"\"\"Auto-repair WAN2GP repository\"\"\"\n",
        "        print(\"ğŸ”§ Cloning WAN2GP repository...\")\n",
        "\n",
        "        repo_url = \"https://github.com/deepbeepmeep/Wan2GP.git\"\n",
        "        clone_cmd = [\"git\", \"clone\", \"--depth\", \"1\", repo_url]\n",
        "\n",
        "        success, output = self.run_command_safely(clone_cmd, \"Repository Clone\", timeout=120)\n",
        "        if success:\n",
        "            self.repairs_applied.append(\"WAN2GP repository cloned\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"âŒ Repository clone failed: {output}\")\n",
        "            return False\n",
        "\n",
        "    def auto_repair_attention_mechanisms(self):\n",
        "        \"\"\"Auto-repair attention mechanisms\"\"\"\n",
        "        print(\"ğŸ”§ Installing performance optimizations...\")\n",
        "\n",
        "        repairs = []\n",
        "\n",
        "        # Install Triton for Windows\n",
        "        if os.name == 'nt':  # Windows\n",
        "            triton_cmd = [self.pip_cmd, \"install\", \"triton-windows\"]\n",
        "            success, _ = self.run_command_safely(triton_cmd, \"Triton Installation\", timeout=120)\n",
        "            if success:\n",
        "                repairs.append(\"Triton (Windows)\")\n",
        "\n",
        "        # Install SageAttention\n",
        "        sage_cmd = [self.pip_cmd, \"install\", \"sageattention>=1.0.6\"]\n",
        "        success, _ = self.run_command_safely(sage_cmd, \"SageAttention Installation\", timeout=120)\n",
        "        if success:\n",
        "            repairs.append(\"SageAttention\")\n",
        "\n",
        "        if repairs:\n",
        "            self.repairs_applied.append(f\"Installed: {', '.join(repairs)}\")\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def generate_optimized_launch_commands(self):\n",
        "        \"\"\"Generate optimized launch commands based on hardware\"\"\"\n",
        "        print(\"\\nğŸš€ Generating optimized launch commands...\")\n",
        "\n",
        "        vram_gb = self.gpu_info.get('vram_gb', 8)\n",
        "        gpu_name = self.gpu_info.get('name', 'Unknown')\n",
        "\n",
        "        commands = {}\n",
        "\n",
        "        # Base command components\n",
        "        base_cmd = \"python wgp.py\"\n",
        "\n",
        "        if vram_gb < 8:\n",
        "            # Low VRAM setup\n",
        "            commands['Low VRAM (6-8GB)'] = f\"{base_cmd} --t2v-1-3B --attention sdpa --profile 4 --teacache 1.5\"\n",
        "        elif vram_gb < 12:\n",
        "            # Medium VRAM setup\n",
        "            commands['Medium VRAM (8-12GB)'] = f\"{base_cmd} --t2v-14B --attention sage --profile 4 --teacache 2.0\"\n",
        "        else:\n",
        "            # High VRAM setup\n",
        "            commands['High VRAM (12GB+)'] = f\"{base_cmd} --t2v-14B --attention sage2 --profile 3 --compile --teacache 2.0\"\n",
        "\n",
        "        # GPU-specific optimizations\n",
        "        if \"10\" in gpu_name or \"20\" in gpu_name:\n",
        "            commands['RTX 10XX/20XX Optimized'] = f\"{base_cmd} --attention sdpa --profile 4 --teacache 1.5\"\n",
        "        elif \"30\" in gpu_name or \"40\" in gpu_name:\n",
        "            commands['RTX 30XX/40XX Optimized'] = f\"{base_cmd} --compile --attention sage --profile 3 --teacache 2.0\"\n",
        "        elif \"50\" in gpu_name:\n",
        "            commands['RTX 50XX Optimized'] = f\"{base_cmd} --attention sage --profile 4 --fp16\"\n",
        "\n",
        "        # Fallback command\n",
        "        commands['Safe Fallback'] = f\"{base_cmd} --t2v-1-3B --attention sdpa --profile 4 --teacache 0 --fp16\"\n",
        "\n",
        "        # Debug command\n",
        "        commands['Debug Mode'] = f\"{base_cmd} --verbose 2 --check-loras --attention sdpa --profile 4\"\n",
        "\n",
        "        return commands\n",
        "\n",
        "    def run_full_diagnostic(self):\n",
        "        \"\"\"Run complete diagnostic and repair sequence\"\"\"\n",
        "        print(\"ğŸ¥ WAN2GP Comprehensive Diagnostic and Auto-Repair System\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"ğŸ–¥ï¸ Platform: {self.platform}\")\n",
        "        print(f\"ğŸ Python: {self.python_cmd}\")\n",
        "        print(f\"ğŸ“¦ Pip: {self.pip_cmd}\")\n",
        "        print(f\"ğŸ”§ Virtual Environment: {'Yes' if self.use_venv else 'No'}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Run all diagnostics\n",
        "        checks = [\n",
        "            self.check_python_version,\n",
        "            self.check_gpu_compatibility,\n",
        "            self.check_cuda_pytorch_compatibility,\n",
        "            self.check_wan2gp_installation,\n",
        "            self.check_dependencies,\n",
        "            self.check_attention_mechanisms,\n",
        "            self.check_memory_and_performance\n",
        "        ]\n",
        "\n",
        "        print(\"\\nğŸ“‹ Running Diagnostics...\")\n",
        "        for check in checks:\n",
        "            try:\n",
        "                check()\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Diagnostic error: {str(e)}\")\n",
        "                self.issues_found.append(f\"Diagnostic error: {str(e)}\")\n",
        "\n",
        "        # Auto-repair if issues found\n",
        "        if self.issues_found:\n",
        "            print(f\"\\nâš ï¸ Found {len(self.issues_found)} issues:\")\n",
        "            for i, issue in enumerate(self.issues_found, 1):\n",
        "                print(f\"  {i}. {issue}\")\n",
        "\n",
        "            print(f\"\\nğŸ”§ Attempting automatic repairs...\")\n",
        "\n",
        "            # Apply repairs based on issues found\n",
        "            if any(\"PyTorch\" in issue for issue in self.issues_found):\n",
        "                self.auto_repair_pytorch()\n",
        "\n",
        "            if any(\"dependencies\" in issue.lower() for issue in self.issues_found):\n",
        "                self.auto_repair_dependencies()\n",
        "\n",
        "            if any(\"repository\" in issue.lower() for issue in self.issues_found):\n",
        "                self.auto_repair_wan2gp_repo()\n",
        "\n",
        "            if any(\"attention\" in issue.lower() for issue in self.issues_found):\n",
        "                self.auto_repair_attention_mechanisms()\n",
        "\n",
        "        # Generate launch commands\n",
        "        commands = self.generate_optimized_launch_commands()\n",
        "\n",
        "        # Final report\n",
        "        self.display_final_report(commands)\n",
        "\n",
        "    def display_final_report(self, commands):\n",
        "        \"\"\"Display comprehensive final report\"\"\"\n",
        "\n",
        "        # Create styled HTML report\n",
        "        html_report = f\"\"\"\n",
        "        <div style=\"font-family: Arial, sans-serif; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "                    color: white; padding: 20px; border-radius: 10px; margin: 10px 0;\">\n",
        "            <h2>ğŸ¥ WAN2GP Diagnostic Report</h2>\n",
        "            <div style=\"background-color: rgba(255,255,255,0.1); padding: 15px; border-radius: 8px; margin: 10px 0;\">\n",
        "                <h3>ğŸ“Š System Status</h3>\n",
        "                <p><strong>Platform:</strong> {self.platform}</p>\n",
        "                <p><strong>GPU:</strong> {self.gpu_info.get('name', 'Unknown')} ({self.gpu_info.get('vram_gb', 0):.1f}GB VRAM)</p>\n",
        "                <p><strong>Issues Found:</strong> {len(self.issues_found)}</p>\n",
        "                <p><strong>Repairs Applied:</strong> {len(self.repairs_applied)}</p>\n",
        "            </div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        if self.repairs_applied:\n",
        "            html_report += f\"\"\"\n",
        "            <div style=\"background-color: #28a745; color: white; padding: 15px; border-radius: 8px; margin: 10px 0;\">\n",
        "                <h3>âœ… Repairs Applied Successfully:</h3>\n",
        "                <ul>\n",
        "            \"\"\"\n",
        "            for repair in self.repairs_applied:\n",
        "                html_report += f\"<li>{repair}</li>\"\n",
        "            html_report += \"</ul></div>\"\n",
        "\n",
        "        if self.issues_found and not self.repairs_applied:\n",
        "            html_report += f\"\"\"\n",
        "            <div style=\"background-color: #dc3545; color: white; padding: 15px; border-radius: 8px; margin: 10px 0;\">\n",
        "                <h3>âš ï¸ Unresolved Issues:</h3>\n",
        "                <ul>\n",
        "            \"\"\"\n",
        "            for issue in self.issues_found:\n",
        "                html_report += f\"<li>{issue}</li>\"\n",
        "            html_report += \"</ul></div>\"\n",
        "\n",
        "        display(HTML(html_report))\n",
        "\n",
        "        # Display optimized commands\n",
        "        print(\"\\nğŸš€ Recommended Launch Commands:\")\n",
        "        print(\"=\" * 50)\n",
        "        for name, command in commands.items():\n",
        "            print(f\"\\n{name}:\")\n",
        "            print(f\"  {command}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"âœ… Diagnostic complete! Use the appropriate command above to launch WAN2GP.\")\n",
        "        if self.issues_found and not self.repairs_applied:\n",
        "            print(\"âš ï¸  Some issues require manual intervention. Check the report above.\")\n",
        "\n",
        "# Execute diagnostic system\n",
        "diagnostic_system = WAN2GPDiagnosticAndRepair()\n",
        "diagnostic_system.run_full_diagnostic()\n"
      ],
      "metadata": {
        "id": "ZzJeUGqaCLJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell (2) - Quick Diagnostic Runner and Emergency Repair Tools (Latest v6.2)\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "class QuickDiagnosticRunner:\n",
        "    def __init__(self):\n",
        "        self.platform = self.detect_platform()\n",
        "        self.pip_cmd, self.python_cmd = self.get_platform_commands()\n",
        "\n",
        "    def detect_platform(self):\n",
        "        \"\"\"Quick platform detection\"\"\"\n",
        "        if \"google.colab\" in sys.modules:\n",
        "            return \"Google Colab\"\n",
        "        elif any(indicator in str(sys.executable).lower() for indicator in [\"lightning\", \"teamspace\"]):\n",
        "            return \"Lightning AI\"\n",
        "        else:\n",
        "            return \"Generic/Vast.AI\"\n",
        "\n",
        "    def get_platform_commands(self):\n",
        "        \"\"\"Get platform-specific commands\"\"\"\n",
        "        if self.platform == \"Lightning AI\":\n",
        "            return \"pip\", \"python\"\n",
        "        else:\n",
        "            return \".venv/bin/pip\" if os.path.exists(\".venv\") else \"pip\", \".venv/bin/python\" if os.path.exists(\".venv\") else \"python\"\n",
        "\n",
        "    def run_cmd(self, cmd, timeout=30):\n",
        "        \"\"\"Execute command safely\"\"\"\n",
        "        try:\n",
        "            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout)\n",
        "            return result.returncode == 0, result.stdout, result.stderr\n",
        "        except subprocess.TimeoutExpired:\n",
        "            return False, \"\", \"Timeout\"\n",
        "        except Exception as e:\n",
        "            return False, \"\", str(e)\n",
        "\n",
        "    def emergency_pytorch_fix(self):\n",
        "        \"\"\"Emergency PyTorch installation fix\"\"\"\n",
        "        print(\"ğŸš¨ Emergency PyTorch Fix...\")\n",
        "\n",
        "        # Detect GPU generation for correct PyTorch version\n",
        "        success, gpu_info, _ = self.run_cmd(\"nvidia-smi\")\n",
        "\n",
        "        if \"RTX 50\" in gpu_info:\n",
        "            pytorch_url = \"https://download.pytorch.org/whl/test/cu128\"\n",
        "            torch_version = \"torch==2.7.0\"\n",
        "        else:\n",
        "            pytorch_url = \"https://download.pytorch.org/whl/test/cu124\"\n",
        "            torch_version = \"torch==2.6.0\"\n",
        "\n",
        "        cmd = f\"{self.pip_cmd} install --upgrade {torch_version} torchvision torchaudio --index-url {pytorch_url}\"\n",
        "        success, stdout, stderr = self.run_cmd(cmd, timeout=300)\n",
        "\n",
        "        if success:\n",
        "            print(\"âœ… PyTorch emergency fix applied\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"âŒ PyTorch fix failed: {stderr}\")\n",
        "            return False\n",
        "\n",
        "    def emergency_dependency_fix(self):\n",
        "        \"\"\"Emergency dependency installation\"\"\"\n",
        "        print(\"ğŸš¨ Emergency Dependency Fix...\")\n",
        "\n",
        "        essential_packages = [\n",
        "            \"gradio>=4.0.0\", \"transformers\", \"accelerate\", \"diffusers\",\n",
        "            \"opencv-python\", \"Pillow\", \"numpy\", \"scipy\", \"psutil\"\n",
        "        ]\n",
        "\n",
        "        for package in essential_packages:\n",
        "            cmd = f\"{self.pip_cmd} install {package}\"\n",
        "            success, _, _ = self.run_cmd(cmd, timeout=60)\n",
        "            print(\"âœ…\" if success else \"âŒ\", package)\n",
        "\n",
        "        print(\"âœ… Emergency dependencies installed\")\n",
        "\n",
        "    def quick_system_check(self):\n",
        "        \"\"\"Quick system health check\"\"\"\n",
        "        checks = {}\n",
        "\n",
        "        # GPU Check\n",
        "        success, output, _ = self.run_cmd(\"nvidia-smi\")\n",
        "        checks['GPU'] = \"âœ… Available\" if success and \"RTX\" in output else \"âŒ Issue detected\"\n",
        "\n",
        "        # PyTorch Check\n",
        "        try:\n",
        "            import torch\n",
        "            checks['PyTorch'] = f\"âœ… {torch.__version__}\" if torch.cuda.is_available() else \"âŒ CUDA not available\"\n",
        "        except ImportError:\n",
        "            checks['PyTorch'] = \"âŒ Not installed\"\n",
        "\n",
        "        # WAN2GP Check\n",
        "        wan_exists = any(os.path.exists(path) for path in [\"Wan2GP/wgp.py\", \"wan2gp/wgp.py\", \"WAN2GP/wgp.py\"])\n",
        "        checks['WAN2GP'] = \"âœ… Found\" if wan_exists else \"âŒ Missing\"\n",
        "\n",
        "        # Dependencies Check\n",
        "        try:\n",
        "            import gradio, transformers, accelerate, diffusers\n",
        "            checks['Dependencies'] = \"âœ… Core packages available\"\n",
        "        except ImportError:\n",
        "            checks['Dependencies'] = \"âŒ Missing packages\"\n",
        "\n",
        "        return checks\n",
        "\n",
        "    def generate_emergency_commands(self):\n",
        "        \"\"\"Generate emergency launch commands\"\"\"\n",
        "        commands = {\n",
        "            \"Ultra Safe Mode\": f\"{self.python_cmd} wgp.py --t2v-1-3B --attention sdpa --profile 4 --teacache 0 --fp16\",\n",
        "            \"Memory Emergency\": f\"{self.python_cmd} wgp.py --t2v-1-3B --profile 5 --perc-reserved-mem-max 0.2\",\n",
        "            \"Debug Mode\": f\"{self.python_cmd} wgp.py --verbose 2 --attention sdpa --profile 4\",\n",
        "            \"Network Share\": f\"{self.python_cmd} wgp.py --listen --server-port 7861 --attention sdpa\"\n",
        "        }\n",
        "        return commands\n",
        "\n",
        "    def run_quick_diagnostic(self):\n",
        "        \"\"\"Run quick diagnostic and provide emergency options\"\"\"\n",
        "\n",
        "        print(f\"âš¡ Quick Diagnostic - Platform: {self.platform}\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "        # Quick system check\n",
        "        checks = self.quick_system_check()\n",
        "\n",
        "        issues = []\n",
        "        for component, status in checks.items():\n",
        "            print(f\"{component}: {status}\")\n",
        "            if \"âŒ\" in status:\n",
        "                issues.append(component)\n",
        "\n",
        "        # Emergency repairs\n",
        "        if issues:\n",
        "            print(f\"\\nğŸš¨ {len(issues)} issues detected. Applying emergency fixes...\")\n",
        "\n",
        "            if \"PyTorch\" in issues:\n",
        "                self.emergency_pytorch_fix()\n",
        "\n",
        "            if \"Dependencies\" in issues:\n",
        "                self.emergency_dependency_fix()\n",
        "\n",
        "            if \"WAN2GP\" in issues:\n",
        "                print(\"ğŸ”§ Cloning WAN2GP repository...\")\n",
        "                success, _, _ = self.run_cmd(\"git clone --depth 1 https://github.com/deepbeepmeep/Wan2GP.git\", timeout=120)\n",
        "                print(\"âœ… Repository cloned\" if success else \"âŒ Clone failed\")\n",
        "\n",
        "        # Generate emergency commands\n",
        "        commands = self.generate_emergency_commands()\n",
        "\n",
        "        # Display results\n",
        "        html_display = f\"\"\"\n",
        "        <div style=\"background: linear-gradient(45deg, #ff6b6b, #feca57); color: white;\n",
        "                    padding: 15px; border-radius: 8px; margin: 10px 0;\">\n",
        "            <h3>âš¡ Quick Diagnostic Results</h3>\n",
        "            <p><strong>Platform:</strong> {self.platform}</p>\n",
        "            <p><strong>Issues Found:</strong> {len(issues)}</p>\n",
        "            <p><strong>Status:</strong> {'ğŸš¨ Needs Attention' if issues else 'âœ… System Ready'}</p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        display(HTML(html_display))\n",
        "\n",
        "        print(\"\\nğŸš€ Emergency Launch Commands:\")\n",
        "        print(\"-\" * 40)\n",
        "        for name, command in commands.items():\n",
        "            print(f\"\\n{name}:\")\n",
        "            print(f\"  {command}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 40)\n",
        "        print(\"âš¡ Quick diagnostic complete!\")\n",
        "\n",
        "        if not issues:\n",
        "            print(\"âœ… System appears healthy. Try the Ultra Safe Mode command first.\")\n",
        "        else:\n",
        "            print(\"ğŸš¨ Emergency fixes applied. Test with Ultra Safe Mode.\")\n",
        "\n",
        "# Execute quick diagnostic\n",
        "quick_runner = QuickDiagnosticRunner()\n",
        "quick_runner.run_quick_diagnostic()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0sx8G7LBCNyw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}