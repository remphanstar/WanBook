{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/remphanstar/WanBook/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell (1) WAN2GP Setup Introduction & Enhanced Platform Detection v3.1 (Colab venv Enforced)\n",
        "\n",
        "import sys\n",
        "from IPython.display import display, HTML, Markdown\n",
        "import os\n",
        "\n",
        "def jupyter_lab_title(title):\n",
        "    if \"google.colab\" not in sys.modules:\n",
        "        display(Markdown(f\"## {title}\"))\n",
        "\n",
        "jupyter_lab_title(\"WAN2GP Setup Introduction & Enhanced Platform Detection\")\n",
        "\n",
        "def detect_platform():\n",
        "    \"\"\"Enhanced platform detection with comprehensive indicators\"\"\"\n",
        "\n",
        "    # Lightning AI detection - multiple indicators for reliability\n",
        "    lightning_indicators = [\n",
        "        \"lightning\" in str(sys.executable).lower(),\n",
        "        \"teamspace-studios\" in os.getcwd(),\n",
        "        \"LIGHTNING_CLOUDSPACE_HOST\" in os.environ,\n",
        "        \"LIGHTNING_CLOUDSPACE_ID\" in os.environ,\n",
        "        \"commands/python\" in str(sys.executable),\n",
        "        \"/home/zeus/miniconda3/envs/cloudspace\" in str(sys.executable),\n",
        "        os.path.exists(\"/teamspace\"),\n",
        "        os.path.exists(\"/commands\")\n",
        "    ]\n",
        "\n",
        "    # Google Colab detection\n",
        "    colab_indicators = [\n",
        "        \"google.colab\" in sys.modules,\n",
        "        \"/content\" in os.getcwd()\n",
        "    ]\n",
        "\n",
        "    # Vast.AI detection\n",
        "    vast_indicators = [\n",
        "        \"VAST_CONTAINER_LABEL\" in os.environ,\n",
        "        \"/workspace\" in os.getcwd(),\n",
        "        \"vast\" in os.environ.get(\"HOSTNAME\", \"\").lower()\n",
        "    ]\n",
        "\n",
        "    if any(lightning_indicators):\n",
        "        return \"Lightning AI\"\n",
        "    elif any(colab_indicators):\n",
        "        return \"Google Colab\"\n",
        "    elif any(vast_indicators):\n",
        "        return \"Vast.AI/Generic\"\n",
        "    else:\n",
        "        return \"Vast.AI/Generic\"\n",
        "\n",
        "def get_platform_commands(platform):\n",
        "    \"\"\"Get platform-specific pip and python commands - ENFORCED VENV FOR COLAB\"\"\"\n",
        "    if platform == \"Lightning AI\":\n",
        "        return \"pip\", \"python\", False  # (pip_cmd, python_cmd, use_venv)\n",
        "    elif platform == \"Google Colab\":\n",
        "        # ENFORCED: Always use venv for Google Colab\n",
        "        return \".venv/bin/pip\", \".venv/bin/python\", True\n",
        "    else:  # Vast.AI/Generic\n",
        "        return \".venv/bin/pip\", \".venv/bin/python\", True\n",
        "\n",
        "# Detect current platform\n",
        "current_platform = detect_platform()\n",
        "pip_cmd, python_cmd, use_venv = get_platform_commands(current_platform)\n",
        "\n",
        "# Display platform information\n",
        "display(HTML(f\"\"\"\n",
        "<div style=\"font-family: Arial, sans-serif; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "           color: white; padding: 20px; border-radius: 10px; text-align: center; margin: 10px 0;\n",
        "           box-shadow: 0 4px 15px rgba(0,0,0,0.2);\">\n",
        "    <h1>üöÄ Wan2GP Full Setup Notebook</h1>\n",
        "    <p>A cross-platform Jupyter notebook (Colab, Lightning AI, Vast.ai) that installs Wan2GP,\n",
        "       common LoRA packs, and optional performance extras (FlashAttention 2, SageAttention, xFormers).\n",
        "       It accelerates all downloads with <strong>aria2c</strong> for maximum speed.</p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background-color: #f8f9fa; padding: 20px; border-radius: 8px; border: 1px solid #dee2e6; margin: 15px 0;\">\n",
        "    <div style=\"background-color: #007bff; color: white; padding: 15px; border-radius: 5px; text-align: center; margin-bottom: 20px;\">\n",
        "        <h3 style=\"margin: 0; color: white;\">üéØ Platform Detection Results</h3>\n",
        "        <p style=\"margin: 5px 0 0 0; color: white;\">Currently Running on: <strong style=\"color: #28a745; background-color: white; padding: 2px 6px; border-radius: 3px;\">{current_platform}</strong></p>\n",
        "        <p style=\"margin: 5px 0 0 0; color: white;\">Virtual Environment: <strong>{\"Yes\" if use_venv else \"No (Lightning AI)\"}</strong></p>\n",
        "        <p style=\"margin: 5px 0 0 0; color: white;\">Pip Command: <strong>{pip_cmd}</strong></p>\n",
        "        <p style=\"margin: 5px 0 0 0; color: white;\">Python Command: <strong>{python_cmd}</strong></p>\n",
        "    </div>\n",
        "</div>\n",
        "\"\"\"))\n",
        "\n",
        "print(\"üìã Platform Detection Debug Info:\")\n",
        "print(f\"üîç - Detected Platform: {current_platform}\")\n",
        "print(f\"üêç - Python Executable: {sys.executable}\")\n",
        "print(f\"üìÅ - Current Working Directory: {os.getcwd()}\")\n",
        "print(f\"üåê - Google Colab Check: {'google.colab' in sys.modules}\")\n",
        "print(f\"‚ö° - Lightning Environment Variables: {[key for key in os.environ.keys() if 'LIGHTNING' in key]}\")\n",
        "print(f\"üîß - Virtual Environment Usage: {use_venv}\")\n",
        "print(f\"üì¶ - Pip Command: {pip_cmd}\")\n",
        "print(f\"üêç - Python Command: {python_cmd}\")\n",
        "\n",
        "# CRITICAL: Enforce venv for Google Colab\n",
        "if current_platform == \"Google Colab\" and not use_venv:\n",
        "    print(\"üö® CRITICAL: Detected Google Colab but venv is disabled - FORCE ENABLING\")\n",
        "    use_venv = True\n",
        "    pip_cmd = \".venv/bin/pip\"\n",
        "    python_cmd = \".venv/bin/python\"\n",
        "    print(\"‚úÖ ENFORCED: Google Colab now using venv commands\")\n",
        "elif current_platform == \"Google Colab\":\n",
        "    print(\"‚úÖ CONFIRMED: Google Colab correctly configured for venv usage\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Q7hLV1Oz-aBD",
        "outputId": "b54a9f6e-1766-4c90-c0da-c84722004c02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<div style=\"font-family: Arial, sans-serif; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \n",
              "           color: white; padding: 20px; border-radius: 10px; text-align: center; margin: 10px 0; \n",
              "           box-shadow: 0 4px 15px rgba(0,0,0,0.2);\">\n",
              "    <h1>üöÄ Wan2GP Full Setup Notebook</h1>\n",
              "    <p>A cross-platform Jupyter notebook (Colab, Lightning AI, Vast.ai) that installs Wan2GP, \n",
              "       common LoRA packs, and optional performance extras (FlashAttention 2, SageAttention, xFormers). \n",
              "       It accelerates all downloads with <strong>aria2c</strong> for maximum speed.</p>\n",
              "</div>\n",
              "\n",
              "<div style=\"background-color: #f8f9fa; padding: 20px; border-radius: 8px; border: 1px solid #dee2e6; margin: 15px 0;\">\n",
              "    <div style=\"background-color: #007bff; color: white; padding: 15px; border-radius: 5px; text-align: center; margin-bottom: 20px;\">\n",
              "        <h3 style=\"margin: 0; color: white;\">üéØ Platform Detection Results</h3>\n",
              "        <p style=\"margin: 5px 0 0 0; color: white;\">Currently Running on: <strong style=\"color: #28a745; background-color: white; padding: 2px 6px; border-radius: 3px;\">Google Colab</strong></p>\n",
              "        <p style=\"margin: 5px 0 0 0; color: white;\">Virtual Environment: <strong>Yes</strong></p>\n",
              "        <p style=\"margin: 5px 0 0 0; color: white;\">Pip Command: <strong>.venv/bin/pip</strong></p>\n",
              "        <p style=\"margin: 5px 0 0 0; color: white;\">Python Command: <strong>.venv/bin/python</strong></p>\n",
              "    </div>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìã Platform Detection Debug Info:\n",
            "üîç - Detected Platform: Google Colab\n",
            "üêç - Python Executable: /usr/bin/python3\n",
            "üìÅ - Current Working Directory: /content\n",
            "üåê - Google Colab Check: True\n",
            "‚ö° - Lightning Environment Variables: []\n",
            "üîß - Virtual Environment Usage: True\n",
            "üì¶ - Pip Command: .venv/bin/pip\n",
            "üêç - Python Command: .venv/bin/python\n",
            "‚úÖ CONFIRMED: Google Colab correctly configured for venv usage\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell (2) Complete System + Python Environment Setup - Cross-Platform v3.2 (venv Enforced for Colab)\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "def jupyter_lab_title(title):\n",
        "    if \"google.colab\" not in sys.modules:\n",
        "        from IPython.display import display, Markdown\n",
        "        display(Markdown(f\"## {title}\"))\n",
        "\n",
        "jupyter_lab_title(\"Complete System + Python Environment Setup\")\n",
        "\n",
        "class CompleteEnvironmentSetup:\n",
        "    def __init__(self):\n",
        "        # Use global platform configuration from Cell 1\n",
        "        try:\n",
        "            self.current_platform = current_platform\n",
        "            self.pip_cmd = pip_cmd\n",
        "            self.python_cmd = python_cmd\n",
        "            self.use_venv = use_venv\n",
        "        except NameError:\n",
        "            print(\"‚ö†Ô∏è Platform detection variables not found. Using fallback detection.\")\n",
        "            self.detect_platform_fallback()\n",
        "\n",
        "        # CRITICAL ENFORCEMENT: Ensure venv for Google Colab\n",
        "        self.enforce_colab_venv()\n",
        "\n",
        "        self.setup_phases = [\n",
        "            (\"System Packages\", self.install_system_packages),\n",
        "            (\"Directory Protection\", self.setup_directory_protection),\n",
        "            (\"Repository Clone\", self.clone_repository),\n",
        "            (\"Virtual Environment\", self.setup_virtual_environment),\n",
        "            (\"PyTorch Installation\", self.install_pytorch),\n",
        "            (\"Requirements\", self.install_requirements),\n",
        "            (\"Environment Verification\", self.verify_complete_setup),\n",
        "        ]\n",
        "\n",
        "        self.current_phase = 0\n",
        "        self.total_phases = len(self.setup_phases)\n",
        "\n",
        "    def enforce_colab_venv(self):\n",
        "        \"\"\"CRITICAL: Enforce venv usage for Google Colab\"\"\"\n",
        "        if self.current_platform == \"Google Colab\":\n",
        "            if not self.use_venv or self.pip_cmd != \".venv/bin/pip\" or self.python_cmd != \".venv/bin/python\":\n",
        "                print(\"üö® ENFORCING: Google Colab MUST use venv - correcting configuration\")\n",
        "                self.use_venv = True\n",
        "                self.pip_cmd = \".venv/bin/pip\"\n",
        "                self.python_cmd = \".venv/bin/python\"\n",
        "                print(\"‚úÖ ENFORCED: Google Colab venv configuration corrected\")\n",
        "            else:\n",
        "                print(\"‚úÖ VERIFIED: Google Colab correctly using venv configuration\")\n",
        "\n",
        "    def detect_platform_fallback(self):\n",
        "        \"\"\"Fallback platform detection if Cell 1 wasn't run - ENFORCES VENV FOR COLAB\"\"\"\n",
        "        if \"google.colab\" in sys.modules:\n",
        "            self.current_platform = \"Google Colab\"\n",
        "            # ENFORCED: Always venv for Colab\n",
        "            self.pip_cmd, self.python_cmd, self.use_venv = \".venv/bin/pip\", \".venv/bin/python\", True\n",
        "        elif any([\"lightning\" in str(sys.executable).lower(), \"teamspace\" in os.getcwd()]):\n",
        "            self.current_platform = \"Lightning AI\"\n",
        "            self.pip_cmd, self.python_cmd, self.use_venv = \"pip\", \"python\", False\n",
        "        else:\n",
        "            self.current_platform = \"Generic\"\n",
        "            self.pip_cmd, self.python_cmd, self.use_venv = \".venv/bin/pip\", \".venv/bin/python\", True\n",
        "\n",
        "    def update_progress(self, phase_name, status=\"in_progress\"):\n",
        "        \"\"\"Update setup progress with visual indicators\"\"\"\n",
        "        progress_percent = (self.current_phase / self.total_phases) * 100\n",
        "        status_icons = {\n",
        "            \"in_progress\": \"üîÑ\",\n",
        "            \"success\": \"‚úÖ\",\n",
        "            \"failed\": \"‚ùå\"\n",
        "        }\n",
        "\n",
        "        print(f\"\\n[{self.current_phase + 1}/{self.total_phases}] {status_icons[status]} {phase_name}\")\n",
        "        print(f\"Progress: {progress_percent:.1f}% | Platform: {self.current_platform}\")\n",
        "        print(f\"üîß Using: {self.pip_cmd} | {self.python_cmd} | venv: {self.use_venv}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    def run_command_safely(self, command, description, timeout=300):\n",
        "        \"\"\"Execute command with comprehensive error handling\"\"\"\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                command,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=timeout\n",
        "            )\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print(f\"‚úÖ {description} completed successfully\")\n",
        "                return True, result.stdout\n",
        "            else:\n",
        "                print(f\"‚ùå {description} failed (exit code {result.returncode})\")\n",
        "                if result.stderr:\n",
        "                    # Check for specific ensurepip failure\n",
        "                    if \"ensurepip\" in result.stderr:\n",
        "                        print(f\"‚ö†Ô∏è ensurepip module failure detected\")\n",
        "                        return \"ensurepip_failure\", result.stderr\n",
        "                    print(f\"   Error: {result.stderr.strip()[:200]}\")\n",
        "                return False, result.stderr\n",
        "\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(f\"‚è∞ {description} timed out after {timeout}s\")\n",
        "            return False, f\"Timeout after {timeout}s\"\n",
        "        except Exception as e:\n",
        "            print(f\"üí• {description} crashed: {e}\")\n",
        "            return False, str(e)\n",
        "\n",
        "    def install_system_packages(self):\n",
        "        \"\"\"Install system packages - same for all platforms\"\"\"\n",
        "        print(\"üîß Installing system packages...\")\n",
        "\n",
        "        if not shutil.which(\"apt-get\"):\n",
        "            print(\"üì¶ apt-get not found. Skipping system package installation.\")\n",
        "            return True\n",
        "\n",
        "        try:\n",
        "            print(\"üì¶ Updating package lists...\")\n",
        "            subprocess.run([\"sudo\", \"apt-get\", \"update\", \"-qq\"], check=True, timeout=60)\n",
        "\n",
        "            print(\"üîß Installing aria2, git, build-essential, and wget...\")\n",
        "            subprocess.run([\"sudo\", \"apt-get\", \"install\", \"-y\", \"aria2\", \"git\", \"build-essential\", \"wget\"], check=True, timeout=180)\n",
        "\n",
        "            print(\"‚úÖ System packages installed successfully\")\n",
        "            return True\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå System package installation failed: {e}\")\n",
        "            return False\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"‚è∞ System package installation timed out\")\n",
        "            return False\n",
        "\n",
        "    def setup_directory_protection(self):\n",
        "        \"\"\"Critical directory protection to prevent recursive cloning\"\"\"\n",
        "        print(\"üõ°Ô∏è Setting up directory protection...\")\n",
        "\n",
        "        current_dir = os.getcwd()\n",
        "        print(f\"üìç Current directory: {current_dir}\")\n",
        "\n",
        "        # Check if we're already inside a Wan2GP directory\n",
        "        if \"Wan2GP\" in current_dir:\n",
        "            print(\"‚ö†Ô∏è Already inside Wan2GP directory - navigating to safe location\")\n",
        "\n",
        "            # Navigate to content root (Colab) or appropriate base directory\n",
        "            if self.current_platform == \"Google Colab\":\n",
        "                safe_dir = \"/content\"\n",
        "            elif self.current_platform == \"Lightning AI\":\n",
        "                safe_dir = os.path.expanduser(\"~\")\n",
        "            else:\n",
        "                safe_dir = os.path.expanduser(\"~\")\n",
        "\n",
        "            try:\n",
        "                os.chdir(safe_dir)\n",
        "                print(f\"üìÅ Moved to safe directory: {os.getcwd()}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Failed to navigate to safe directory: {e}\")\n",
        "                return False\n",
        "\n",
        "        # Check for existing Wan2GP directory and handle it\n",
        "        repo_path = os.path.join(os.getcwd(), \"Wan2GP\")\n",
        "        if os.path.exists(repo_path):\n",
        "            print(f\"üìÅ Found existing Wan2GP directory at: {repo_path}\")\n",
        "\n",
        "            # Check if it looks like a valid repository\n",
        "            if os.path.exists(os.path.join(repo_path, \".git\")):\n",
        "                print(\"‚úÖ Existing directory appears to be a valid git repository\")\n",
        "                # Check if it's the correct repository\n",
        "                try:\n",
        "                    os.chdir(repo_path)\n",
        "                    result = subprocess.run([\"git\", \"remote\", \"get-url\", \"origin\"],\n",
        "                                          capture_output=True, text=True, timeout=10)\n",
        "                    if result.returncode == 0 and \"Wan2GP\" in result.stdout:\n",
        "                        print(\"‚úÖ Existing repository is correct - using it\")\n",
        "                        return True\n",
        "                    else:\n",
        "                        print(\"‚ö†Ô∏è Existing repository is not the correct one\")\n",
        "                        os.chdir(\"..\")\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Could not verify existing repository: {e}\")\n",
        "                    os.chdir(\"..\")\n",
        "\n",
        "            # If we reach here, remove the problematic directory\n",
        "            print(\"üóëÔ∏è Removing problematic existing directory...\")\n",
        "            try:\n",
        "                shutil.rmtree(repo_path)\n",
        "                print(\"‚úÖ Problematic directory removed\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Failed to remove existing directory: {e}\")\n",
        "                # Try to rename it instead\n",
        "                try:\n",
        "                    backup_name = f\"Wan2GP_backup_{int(time.time())}\"\n",
        "                    os.rename(repo_path, backup_name)\n",
        "                    print(f\"‚úÖ Renamed problematic directory to {backup_name}\")\n",
        "                except Exception as e2:\n",
        "                    print(f\"‚ùå Could not even rename directory: {e2}\")\n",
        "                    return False\n",
        "\n",
        "        print(\"‚úÖ Directory protection setup complete\")\n",
        "        return True\n",
        "\n",
        "    def clone_repository(self):\n",
        "        \"\"\"Clone WAN2GP repository with enhanced protection\"\"\"\n",
        "        REPO_DIR = \"Wan2GP\"\n",
        "        REPO_URL = \"https://github.com/deepbeepmeep/Wan2GP.git\"\n",
        "\n",
        "        print(\"üìÇ Setting up WAN2GP repository...\")\n",
        "        print(f\"üìç Working from: {os.getcwd()}\")\n",
        "\n",
        "        # Final check - ensure we're not in a nested situation\n",
        "        current_path = os.getcwd()\n",
        "        if current_path.count(\"Wan2GP\") > 0:\n",
        "            print(\"üö® CRITICAL: Still in Wan2GP directory path!\")\n",
        "            print(\"üîÑ Attempting emergency navigation...\")\n",
        "\n",
        "            # Emergency navigation\n",
        "            if self.current_platform == \"Google Colab\":\n",
        "                emergency_path = \"/content\"\n",
        "            else:\n",
        "                emergency_path = os.path.expanduser(\"~\")\n",
        "\n",
        "            try:\n",
        "                os.chdir(emergency_path)\n",
        "                print(f\"üÜò Emergency navigation successful: {os.getcwd()}\")\n",
        "            except Exception as e:\n",
        "                print(f\"üí• Emergency navigation failed: {e}\")\n",
        "                return False\n",
        "\n",
        "        if not os.path.exists(REPO_DIR):\n",
        "            print(f\"üì• Cloning repository from {REPO_URL}...\")\n",
        "            try:\n",
        "                subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", REPO_URL], check=True, timeout=120)\n",
        "                print(\"‚úÖ Repository cloned successfully\")\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                print(f\"‚ùå Failed to clone repository: {e}\")\n",
        "                return False\n",
        "            except subprocess.TimeoutExpired:\n",
        "                print(\"‚è∞ Repository clone timed out\")\n",
        "                return False\n",
        "        else:\n",
        "            print(f\"üìÅ Directory {REPO_DIR} already exists\")\n",
        "\n",
        "        try:\n",
        "            os.chdir(REPO_DIR)\n",
        "            final_path = os.getcwd()\n",
        "            print(f\"üìÅ Changed to directory: {final_path}\")\n",
        "\n",
        "            # Verify we're in the right place\n",
        "            if final_path.count(\"Wan2GP\") > 1:\n",
        "                print(\"üö® WARNING: Detected nested Wan2GP directories!\")\n",
        "                print(f\"üîç Current path: {final_path}\")\n",
        "                return False\n",
        "\n",
        "            print(\"‚úÖ Repository setup verified\")\n",
        "            return True\n",
        "        except FileNotFoundError:\n",
        "            print(f\"‚ùå Directory {REPO_DIR} not found\")\n",
        "            return False\n",
        "\n",
        "    def setup_venv_manual_pip(self):\n",
        "        \"\"\"Create venv without pip, then install pip manually\"\"\"\n",
        "        try:\n",
        "            print(\"üîß Creating venv without pip...\")\n",
        "            subprocess.run([sys.executable, \"-m\", \"venv\", \".venv\", \"--without-pip\"],\n",
        "                          check=True, timeout=60)\n",
        "\n",
        "            print(\"üì• Downloading get-pip.py...\")\n",
        "            subprocess.run([\"wget\", \"https://bootstrap.pypa.io/get-pip.py\"], check=True)\n",
        "\n",
        "            print(\"üîß Installing pip manually...\")\n",
        "            subprocess.run([\".venv/bin/python\", \"get-pip.py\"], check=True)\n",
        "\n",
        "            print(\"‚úÖ Virtual environment with manual pip created successfully\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Manual pip installation failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def setup_virtual_environment(self):\n",
        "        \"\"\"Create virtual environment - ENFORCED for Google Colab\"\"\"\n",
        "        if not self.use_venv:\n",
        "            print(\"‚ö° Lightning AI detected - using system environment (no venv)\")\n",
        "            return True\n",
        "\n",
        "        print(\"üêç Setting up virtual environment...\")\n",
        "        print(f\"üîß Platform: {self.current_platform} - venv REQUIRED\")\n",
        "\n",
        "        # Check if venv already exists\n",
        "        if os.path.exists(\".venv/bin/python\"):\n",
        "            print(\"‚úÖ Virtual environment already exists and appears functional\")\n",
        "            return True\n",
        "\n",
        "        print(\"üî® Creating virtual environment...\")\n",
        "\n",
        "        # Try multiple methods for virtual environment creation\n",
        "        venv_methods = [\n",
        "            ([sys.executable, \"-m\", \"venv\", \"--system-site-packages\", \".venv\"], \"venv with system packages\"),\n",
        "            ([sys.executable, \"-m\", \"venv\", \".venv\"], \"standard venv\"),\n",
        "            ([sys.executable, \"-m\", \"virtualenv\", \".venv\"], \"virtualenv package\"),\n",
        "        ]\n",
        "\n",
        "        for command, method_name in venv_methods:\n",
        "            print(f\"üîÑ Trying: {method_name}\")\n",
        "            success, error_msg = self.run_command_safely(command, f\"Virtual environment creation ({method_name})\", timeout=60)\n",
        "\n",
        "            if success == True:\n",
        "                print(\"‚úÖ Virtual environment created successfully\")\n",
        "                return True\n",
        "            elif success == \"ensurepip_failure\":\n",
        "                print(\"‚ö†Ô∏è ensurepip failure detected - trying manual pip installation...\")\n",
        "                if self.setup_venv_manual_pip():\n",
        "                    return True\n",
        "                break  # Don't try other methods if ensurepip failed\n",
        "            else:\n",
        "                print(f\"‚ùå {method_name} failed\")\n",
        "                if error_msg:\n",
        "                    print(f\"   Error: {error_msg[:200]}\")\n",
        "                continue\n",
        "\n",
        "        # Final fallback ONLY for non-Colab platforms\n",
        "        if self.current_platform != \"Google Colab\":\n",
        "            print(\"üîÑ All virtual environment methods failed - switching to system installation\")\n",
        "            print(\"‚ö° Overriding to system-wide installation\")\n",
        "            self.use_venv = False\n",
        "            self.pip_cmd = \"pip\"\n",
        "            self.python_cmd = \"python\"\n",
        "            return True\n",
        "        else:\n",
        "            print(\"üö® CRITICAL: Google Colab REQUIRES venv - cannot fallback to system\")\n",
        "            return False\n",
        "\n",
        "    def install_pytorch(self):\n",
        "        \"\"\"Install PyTorch using enforced venv commands\"\"\"\n",
        "        print(\"üî• Installing PyTorch...\")\n",
        "        print(f\"üîß Using pip command: {self.pip_cmd}\")\n",
        "\n",
        "        pytorch_cmd = [\n",
        "            self.pip_cmd, \"install\",\n",
        "            \"torch==2.6.0\", \"torchvision\", \"torchaudio\",\n",
        "            \"--index-url\", \"https://download.pytorch.org/whl/test/cu124\"\n",
        "        ]\n",
        "\n",
        "        success, _ = self.run_command_safely(pytorch_cmd, \"PyTorch installation\", timeout=300)\n",
        "        return success == True\n",
        "\n",
        "    def install_requirements(self):\n",
        "        \"\"\"Install requirements.txt using enforced venv commands\"\"\"\n",
        "        print(\"üì¶ Installing requirements...\")\n",
        "        print(f\"üîß Using pip command: {self.pip_cmd}\")\n",
        "\n",
        "        if not os.path.exists(\"requirements.txt\"):\n",
        "            print(\"‚ö†Ô∏è requirements.txt not found, skipping\")\n",
        "            return True\n",
        "\n",
        "        req_cmd = [self.pip_cmd, \"install\", \"-r\", \"requirements.txt\"]\n",
        "        success, _ = self.run_command_safely(req_cmd, \"Requirements installation\", timeout=300)\n",
        "        return success == True\n",
        "\n",
        "    def verify_complete_setup(self):\n",
        "        \"\"\"Verify the complete installation using enforced venv commands\"\"\"\n",
        "        print(\"üîç Verifying installation...\")\n",
        "        print(f\"üîß Using python command: {self.python_cmd}\")\n",
        "\n",
        "        # Verify directory structure\n",
        "        current_dir = os.getcwd()\n",
        "        print(f\"üìç Final directory: {current_dir}\")\n",
        "\n",
        "        if current_dir.count(\"Wan2GP\") > 1:\n",
        "            print(\"üö® CRITICAL: Nested directory structure detected!\")\n",
        "            print(\"‚ùå Setup verification failed due to directory structure\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Test PyTorch installation\n",
        "            result = subprocess.run([self.python_cmd, \"-c\", \"import torch; print(f'PyTorch {torch.__version__} installed'); print(f'CUDA available: {torch.cuda.is_available()}')\"],\n",
        "                                  capture_output=True, text=True, timeout=30)\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print(\"‚úÖ PyTorch verification successful:\")\n",
        "                for line in result.stdout.strip().split('\\n'):\n",
        "                    print(f\"   {line}\")\n",
        "\n",
        "                # Verify main WanGP file exists\n",
        "                if os.path.exists(\"wgp.py\"):\n",
        "                    print(\"‚úÖ WanGP main file found\")\n",
        "                else:\n",
        "                    print(\"‚ö†Ô∏è WanGP main file (wgp.py) not found\")\n",
        "\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"‚ùå PyTorch verification failed: {result.stderr}\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Verification failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def run_complete_setup(self):\n",
        "        \"\"\"Execute the complete setup process with enforced venv\"\"\"\n",
        "        print(\"üöÄ WAN2GP Complete System + Python Environment Setup\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"Platform: {self.current_platform}\")\n",
        "        print(f\"Virtual Environment: {'Yes' if self.use_venv else 'No'}\")\n",
        "        print(f\"Pip Command: {self.pip_cmd}\")\n",
        "        print(f\"Python Command: {self.python_cmd}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Final enforcement check\n",
        "        if self.current_platform == \"Google Colab\" and not self.use_venv:\n",
        "            print(\"üö® FATAL: Google Colab not using venv - ABORTING\")\n",
        "            return False\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for phase_name, phase_func in self.setup_phases:\n",
        "            self.update_progress(phase_name, \"in_progress\")\n",
        "\n",
        "            phase_start = time.time()\n",
        "            success = phase_func()\n",
        "            phase_duration = time.time() - phase_start\n",
        "\n",
        "            if success:\n",
        "                self.update_progress(phase_name, \"success\")\n",
        "                print(f\"‚è±Ô∏è {phase_name} completed in {phase_duration:.1f}s\")\n",
        "                self.current_phase += 1\n",
        "            else:\n",
        "                self.update_progress(phase_name, \"failed\")\n",
        "                print(f\"üí• Setup failed at: {phase_name}\")\n",
        "                print(\"\\nüîÑ Setup Issues - Platform-Specific Troubleshooting:\")\n",
        "                print(\"üì± Colab: Restart runtime and re-run all cells\")\n",
        "                print(\"‚ö° Lightning.AI: Ensure you're using the correct Python version\")\n",
        "                print(\"üåå Vast.AI: Check GPU drivers and CUDA installation\")\n",
        "                return False\n",
        "\n",
        "        total_duration = time.time() - start_time\n",
        "        print(f\"\\nüéâ WAN2GP setup completed successfully in {total_duration:.1f}s!\")\n",
        "        print(\"üöÄ Ready to launch WAN2GP!\")\n",
        "        return True\n",
        "\n",
        "# Execute the complete setup\n",
        "setup = CompleteEnvironmentSetup()\n",
        "setup.run_complete_setup()\n"
      ],
      "metadata": {
        "id": "Wc1mj9zi_ExM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "185541c4-4f39-4be2-fcd8-53777ed9ea51"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ VERIFIED: Google Colab correctly using venv configuration\n",
            "üöÄ WAN2GP Complete System + Python Environment Setup\n",
            "======================================================================\n",
            "Platform: Google Colab\n",
            "Virtual Environment: Yes\n",
            "Pip Command: .venv/bin/pip\n",
            "Python Command: .venv/bin/python\n",
            "======================================================================\n",
            "\n",
            "[1/7] üîÑ System Packages\n",
            "Progress: 0.0% | Platform: Google Colab\n",
            "üîß Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "üîß Installing system packages...\n",
            "üì¶ Updating package lists...\n",
            "üîß Installing aria2, git, build-essential, and wget...\n",
            "‚úÖ System packages installed successfully\n",
            "\n",
            "[1/7] ‚úÖ System Packages\n",
            "Progress: 0.0% | Platform: Google Colab\n",
            "üîß Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "‚è±Ô∏è System Packages completed in 21.8s\n",
            "\n",
            "[2/7] üîÑ Directory Protection\n",
            "Progress: 14.3% | Platform: Google Colab\n",
            "üîß Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "üõ°Ô∏è Setting up directory protection...\n",
            "üìç Current directory: /content\n",
            "‚úÖ Directory protection setup complete\n",
            "\n",
            "[2/7] ‚úÖ Directory Protection\n",
            "Progress: 14.3% | Platform: Google Colab\n",
            "üîß Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "‚è±Ô∏è Directory Protection completed in 0.0s\n",
            "\n",
            "[3/7] üîÑ Repository Clone\n",
            "Progress: 28.6% | Platform: Google Colab\n",
            "üîß Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "üìÇ Setting up WAN2GP repository...\n",
            "üìç Working from: /content\n",
            "üì• Cloning repository from https://github.com/deepbeepmeep/Wan2GP.git...\n",
            "‚úÖ Repository cloned successfully\n",
            "üìÅ Changed to directory: /content/Wan2GP\n",
            "‚úÖ Repository setup verified\n",
            "\n",
            "[3/7] ‚úÖ Repository Clone\n",
            "Progress: 28.6% | Platform: Google Colab\n",
            "üîß Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "‚è±Ô∏è Repository Clone completed in 1.5s\n",
            "\n",
            "[4/7] üîÑ Virtual Environment\n",
            "Progress: 42.9% | Platform: Google Colab\n",
            "üîß Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "üêç Setting up virtual environment...\n",
            "üîß Platform: Google Colab - venv REQUIRED\n",
            "üî® Creating virtual environment...\n",
            "üîÑ Trying: venv with system packages\n",
            "‚ùå Virtual environment creation (venv with system packages) failed (exit code 1)\n",
            "‚ö†Ô∏è ensurepip module failure detected\n",
            "‚ö†Ô∏è ensurepip failure detected - trying manual pip installation...\n",
            "üîß Creating venv without pip...\n",
            "üì• Downloading get-pip.py...\n",
            "üîß Installing pip manually...\n",
            "‚úÖ Virtual environment with manual pip created successfully\n",
            "\n",
            "[4/7] ‚úÖ Virtual Environment\n",
            "Progress: 42.9% | Platform: Google Colab\n",
            "üîß Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "‚è±Ô∏è Virtual Environment completed in 9.9s\n",
            "\n",
            "[5/7] üîÑ PyTorch Installation\n",
            "Progress: 57.1% | Platform: Google Colab\n",
            "üîß Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "üî• Installing PyTorch...\n",
            "üîß Using pip command: .venv/bin/pip\n",
            "‚úÖ PyTorch installation completed successfully\n",
            "\n",
            "[5/7] ‚úÖ PyTorch Installation\n",
            "Progress: 57.1% | Platform: Google Colab\n",
            "üîß Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "‚è±Ô∏è PyTorch Installation completed in 206.5s\n",
            "\n",
            "[6/7] üîÑ Requirements\n",
            "Progress: 71.4% | Platform: Google Colab\n",
            "üîß Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "üì¶ Installing requirements...\n",
            "üîß Using pip command: .venv/bin/pip\n",
            "‚úÖ Requirements installation completed successfully\n",
            "\n",
            "[6/7] ‚úÖ Requirements\n",
            "Progress: 71.4% | Platform: Google Colab\n",
            "üîß Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "‚è±Ô∏è Requirements completed in 109.5s\n",
            "\n",
            "[7/7] üîÑ Environment Verification\n",
            "Progress: 85.7% | Platform: Google Colab\n",
            "üîß Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "üîç Verifying installation...\n",
            "üîß Using python command: .venv/bin/python\n",
            "üìç Final directory: /content/Wan2GP\n",
            "‚úÖ PyTorch verification successful:\n",
            "   PyTorch 2.6.0+cu124 installed\n",
            "   CUDA available: True\n",
            "‚úÖ WanGP main file found\n",
            "\n",
            "[7/7] ‚úÖ Environment Verification\n",
            "Progress: 85.7% | Platform: Google Colab\n",
            "üîß Using: .venv/bin/pip | .venv/bin/python | venv: True\n",
            "============================================================\n",
            "‚è±Ô∏è Environment Verification completed in 4.5s\n",
            "\n",
            "üéâ WAN2GP setup completed successfully in 353.8s!\n",
            "üöÄ Ready to launch WAN2GP!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell (3) Complete Model & LoRA Downloads - Fixed URLs + Permanent HF Token Storage v4.0\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "from IPython.display import display, HTML, Markdown\n",
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "def jupyter_lab_title(title):\n",
        "    if \"google.colab\" not in sys.modules:\n",
        "        display(Markdown(f\"## {title}\"))\n",
        "\n",
        "jupyter_lab_title(\"Complete Model & LoRA Downloads - Fixed URLs + Permanent HF Token Storage\")\n",
        "\n",
        "# Permanent HuggingFace Token Storage System\n",
        "class PermanentHFTokenManager:\n",
        "    def __init__(self):\n",
        "        self.token_locations = [\n",
        "            \"/content/.huggingface_token\",  # Colab persistent location\n",
        "            os.path.expanduser(\"~/.huggingface_token\"),  # User home directory\n",
        "            \"/tmp/.hf_token\",  # Temporary fallback\n",
        "            \"./.hf_token\"  # Local directory\n",
        "        ]\n",
        "        self.token = None\n",
        "        self.authenticated = False\n",
        "        self.setup_token_persistence()\n",
        "\n",
        "    def setup_token_persistence(self):\n",
        "        \"\"\"Setup permanent HuggingFace token storage with rich UI\"\"\"\n",
        "        auth_html = f\"\"\"\n",
        "        <div style=\"font-family: Arial, sans-serif; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%);\n",
        "                    color: white; padding: 15px; border-radius: 10px; margin: 10px 0;\n",
        "                    box-shadow: 0 4px 15px rgba(0,0,0,0.2);\">\n",
        "            <h3 style=\"margin: 0; color: white;\">üîê Permanent HuggingFace Token Manager</h3>\n",
        "            <p style=\"margin: 5px 0;\">Store your HF token permanently for faster downloads and private model access</p>\n",
        "            <div id=\"hf-token-status\" style=\"margin-top: 10px;\">\n",
        "                <div id=\"token-check-result\"></div>\n",
        "                <button onclick=\"checkExistingToken()\"\n",
        "                        style=\"background: #2ed573; color: white; border: none; padding: 8px 16px;\n",
        "                               border-radius: 5px; cursor: pointer; margin-right: 10px;\">\n",
        "                    üîç Check Existing Token\n",
        "                </button>\n",
        "                <button onclick=\"setupNewToken()\"\n",
        "                        style=\"background: #3742fa; color: white; border: none; padding: 8px 16px;\n",
        "                               border-radius: 5px; cursor: pointer; margin-right: 10px;\">\n",
        "                    üîë Setup New Token\n",
        "                </button>\n",
        "                <button onclick=\"skipTokenSetup()\"\n",
        "                        style=\"background: #57606f; color: white; border: none; padding: 8px 16px;\n",
        "                               border-radius: 5px; cursor: pointer;\">\n",
        "                    ‚è≠Ô∏è Skip Token Setup\n",
        "                </button>\n",
        "            </div>\n",
        "            <div id=\"token-input-area\" style=\"margin-top: 15px; display: none;\">\n",
        "                <input type=\"password\" id=\"hf-token-input\" placeholder=\"Enter HuggingFace Token\"\n",
        "                       style=\"padding: 8px; border-radius: 5px; border: 1px solid #ccc; margin-right: 10px; width: 300px;\">\n",
        "                <button onclick=\"saveToken()\"\n",
        "                        style=\"background: #1dd1a1; color: white; border: none; padding: 8px 16px;\n",
        "                               border-radius: 5px; cursor: pointer; margin-right: 10px;\">\n",
        "                    üíæ Save Permanently\n",
        "                </button>\n",
        "                <button onclick=\"testToken()\"\n",
        "                        style=\"background: #feca57; color: black; border: none; padding: 8px 16px;\n",
        "                               border-radius: 5px; cursor: pointer;\">\n",
        "                    üß™ Test Token\n",
        "                </button>\n",
        "                <p style=\"font-size: 12px; margin-top: 5px;\">\n",
        "                    Get your token from: <a href=\"https://huggingface.co/settings/tokens\" target=\"_blank\"\n",
        "                    style=\"color: #ffc048;\">https://huggingface.co/settings/tokens</a>\n",
        "                </p>\n",
        "            </div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        auth_js = \"\"\"\n",
        "        <script>\n",
        "        function checkExistingToken() {\n",
        "            google.colab.kernel.invokeFunction('check_existing_hf_token', [], {});\n",
        "        }\n",
        "\n",
        "        function setupNewToken() {\n",
        "            document.getElementById('token-input-area').style.display = 'block';\n",
        "            document.getElementById('hf-token-input').focus();\n",
        "        }\n",
        "\n",
        "        function saveToken() {\n",
        "            const token = document.getElementById('hf-token-input').value;\n",
        "            if (token && token.startsWith('hf_')) {\n",
        "                google.colab.kernel.invokeFunction('save_hf_token_permanently', [token], {});\n",
        "                document.getElementById('token-check-result').innerHTML =\n",
        "                    '<p style=\"color: #2ed573;\">üíæ Saving token permanently...</p>';\n",
        "            } else {\n",
        "                document.getElementById('token-check-result').innerHTML =\n",
        "                    '<p style=\"color: #ff3838;\">‚ùå Invalid token format. Must start with \"hf_\"</p>';\n",
        "            }\n",
        "        }\n",
        "\n",
        "        function testToken() {\n",
        "            const token = document.getElementById('hf-token-input').value;\n",
        "            if (token) {\n",
        "                google.colab.kernel.invokeFunction('test_hf_token', [token], {});\n",
        "                document.getElementById('token-check-result').innerHTML =\n",
        "                    '<p style=\"color: #feca57;\">üß™ Testing token...</p>';\n",
        "            }\n",
        "        }\n",
        "\n",
        "        function skipTokenSetup() {\n",
        "            google.colab.kernel.invokeFunction('skip_hf_token_setup', [], {});\n",
        "            document.getElementById('hf-token-status').innerHTML =\n",
        "                '<p style=\"color: #ffc048;\">‚è≠Ô∏è Skipped token setup - using public access only</p>';\n",
        "        }\n",
        "        </script>\n",
        "        \"\"\"\n",
        "\n",
        "        display(HTML(auth_html + auth_js))\n",
        "\n",
        "    def check_existing_token(self):\n",
        "        \"\"\"Check for existing stored tokens\"\"\"\n",
        "        for location in self.token_locations:\n",
        "            try:\n",
        "                if os.path.exists(location):\n",
        "                    with open(location, 'r') as f:\n",
        "                        potential_token = f.read().strip()\n",
        "                    if potential_token.startswith('hf_') and len(potential_token) > 20:\n",
        "                        self.token = potential_token\n",
        "                        self.show_existing_token_found(location)\n",
        "                        return True\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        self.show_no_existing_token()\n",
        "        return False\n",
        "\n",
        "    def save_token_permanently(self, token):\n",
        "        \"\"\"Save token to multiple persistent locations\"\"\"\n",
        "        saved_locations = []\n",
        "\n",
        "        for location in self.token_locations:\n",
        "            try:\n",
        "                # Create directory if needed\n",
        "                os.makedirs(os.path.dirname(location) if os.path.dirname(location) else \".\", exist_ok=True)\n",
        "\n",
        "                with open(location, 'w') as f:\n",
        "                    f.write(token)\n",
        "\n",
        "                # Set restrictive permissions\n",
        "                os.chmod(location, 0o600)\n",
        "                saved_locations.append(location)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Could not save to {location}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if saved_locations:\n",
        "            self.token = token\n",
        "            self.authenticated = True\n",
        "            self.show_token_saved_success(saved_locations)\n",
        "            return True\n",
        "        else:\n",
        "            self.show_token_save_failed()\n",
        "            return False\n",
        "\n",
        "    def test_token_validity(self, token):\n",
        "        \"\"\"Test if token is valid by making API call\"\"\"\n",
        "        try:\n",
        "            import requests\n",
        "            headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "            response = requests.get(\"https://huggingface.co/api/whoami\", headers=headers, timeout=10)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                user_info = response.json()\n",
        "                self.show_token_test_success(user_info.get('name', 'Unknown'))\n",
        "                return True\n",
        "            else:\n",
        "                self.show_token_test_failed(\"Invalid token\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            self.show_token_test_failed(f\"Network error: {e}\")\n",
        "            return False\n",
        "\n",
        "    def show_existing_token_found(self, location):\n",
        "        \"\"\"Show existing token found message\"\"\"\n",
        "        success_html = f\"\"\"\n",
        "        <div style=\"background: #d4edda; color: #155724; padding: 10px; border-radius: 5px;\n",
        "                    border: 1px solid #c3e6cb; margin: 5px 0;\">\n",
        "            <strong>‚úÖ Existing HuggingFace Token Found!</strong><br>\n",
        "            üìç Location: {location}<br>\n",
        "            üöÄ Ready for authenticated downloads\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        display(HTML(success_html))\n",
        "\n",
        "    def show_no_existing_token(self):\n",
        "        \"\"\"Show no existing token message\"\"\"\n",
        "        info_html = \"\"\"\n",
        "        <div style=\"background: #fff3cd; color: #856404; padding: 10px; border-radius: 5px;\n",
        "                    border: 1px solid #ffeaa7; margin: 5px 0;\">\n",
        "            <strong>‚ÑπÔ∏è No Existing Token Found</strong><br>\n",
        "            üí° You can setup a new token for faster downloads and private model access\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        display(HTML(info_html))\n",
        "\n",
        "    def show_token_saved_success(self, locations):\n",
        "        \"\"\"Show token saved successfully\"\"\"\n",
        "        locations_str = \"<br>\".join([f\"üìÅ {loc}\" for loc in locations])\n",
        "        success_html = f\"\"\"\n",
        "        <div style=\"background: #d4edda; color: #155724; padding: 10px; border-radius: 5px;\n",
        "                    border: 1px solid #c3e6cb; margin: 5px 0;\">\n",
        "            <strong>üíæ HuggingFace Token Saved Permanently!</strong><br>\n",
        "            {locations_str}<br>\n",
        "            üîê Token will persist across sessions\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        display(HTML(success_html))\n",
        "\n",
        "    def show_token_save_failed(self):\n",
        "        \"\"\"Show token save failed\"\"\"\n",
        "        error_html = \"\"\"\n",
        "        <div style=\"background: #f8d7da; color: #721c24; padding: 10px; border-radius: 5px;\n",
        "                    border: 1px solid #f5c6cb; margin: 5px 0;\">\n",
        "            <strong>‚ùå Failed to Save Token</strong><br>\n",
        "            ‚ö†Ô∏è Continuing with public access only\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        display(HTML(error_html))\n",
        "\n",
        "    def show_token_test_success(self, username):\n",
        "        \"\"\"Show token test success\"\"\"\n",
        "        success_html = f\"\"\"\n",
        "        <div style=\"background: #d4edda; color: #155724; padding: 10px; border-radius: 5px;\n",
        "                    border: 1px solid #c3e6cb; margin: 5px 0;\">\n",
        "            <strong>üß™ Token Test Successful!</strong><br>\n",
        "            üë§ Authenticated as: {username}<br>\n",
        "            ‚úÖ Token is valid and working\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        display(HTML(success_html))\n",
        "\n",
        "    def show_token_test_failed(self, error):\n",
        "        \"\"\"Show token test failed\"\"\"\n",
        "        error_html = f\"\"\"\n",
        "        <div style=\"background: #f8d7da; color: #721c24; padding: 10px; border-radius: 5px;\n",
        "                    border: 1px solid #f5c6cb; margin: 5px 0;\">\n",
        "            <strong>üß™ Token Test Failed</strong><br>\n",
        "            ‚ùå Error: {error}<br>\n",
        "            üí° Please check your token and try again\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        display(HTML(error_html))\n",
        "\n",
        "# Enhanced Model Downloader with Fixed URLs\n",
        "class FixedModelDownloader:\n",
        "    def __init__(self):\n",
        "        # Use global platform configuration from previous cells\n",
        "        try:\n",
        "            self.current_platform = current_platform\n",
        "            self.pip_cmd = pip_cmd\n",
        "            self.python_cmd = python_cmd\n",
        "            self.use_venv = use_venv\n",
        "        except NameError:\n",
        "            print(\"‚ö†Ô∏è Platform detection variables not found. Using fallback detection.\")\n",
        "            self.detect_platform_fallback()\n",
        "\n",
        "        # CRITICAL ENFORCEMENT: Ensure venv for Google Colab\n",
        "        self.enforce_colab_venv()\n",
        "\n",
        "        # Initialize permanent HF token manager\n",
        "        self.hf_token_manager = PermanentHFTokenManager()\n",
        "\n",
        "        # Fixed repository URL with correct naming\n",
        "        self.base_repo_url = \"https://huggingface.co/Kijai/WanVideo_comfy\"\n",
        "        self.resolve_url = f\"{self.base_repo_url}/resolve/main\"\n",
        "\n",
        "        # Download methods with HF Hub integration\n",
        "        self.download_methods = [\n",
        "            (\"hf_hub\", self.download_with_hf_hub),\n",
        "            (\"aria2c\", self.download_with_aria2c),\n",
        "            (\"wget\", self.download_with_wget),\n",
        "            (\"python\", self.download_with_python)\n",
        "        ]\n",
        "\n",
        "        # CORRECTED model definitions based on actual repository files\n",
        "        self.setup_corrected_model_definitions()\n",
        "\n",
        "        # Download statistics\n",
        "        self.download_stats = {\n",
        "            \"total_files\": 0,\n",
        "            \"downloaded_files\": 0,\n",
        "            \"failed_files\": 0,\n",
        "            \"total_size\": 0,\n",
        "            \"downloaded_size\": 0\n",
        "        }\n",
        "\n",
        "    def enforce_colab_venv(self):\n",
        "        \"\"\"CRITICAL: Enforce venv usage for Google Colab\"\"\"\n",
        "        if hasattr(self, 'current_platform') and self.current_platform == \"Google Colab\":\n",
        "            if not hasattr(self, 'use_venv') or not self.use_venv or self.pip_cmd != \".venv/bin/pip\":\n",
        "                print(\"üö® ENFORCING: Google Colab MUST use venv for downloads\")\n",
        "                self.use_venv = True\n",
        "                self.pip_cmd = \".venv/bin/pip\"\n",
        "                self.python_cmd = \".venv/bin/python\"\n",
        "                print(\"‚úÖ ENFORCED: Download system using venv commands\")\n",
        "            else:\n",
        "                print(\"‚úÖ VERIFIED: Download system correctly using venv for Colab\")\n",
        "\n",
        "    def detect_platform_fallback(self):\n",
        "        \"\"\"Fallback platform detection - ENFORCES VENV FOR COLAB\"\"\"\n",
        "        if \"google.colab\" in sys.modules:\n",
        "            self.current_platform = \"Google Colab\"\n",
        "            # ENFORCED: Always venv for Colab\n",
        "            self.pip_cmd, self.python_cmd, self.use_venv = \".venv/bin/pip\", \".venv/bin/python\", True\n",
        "        elif any([\"lightning\" in str(sys.executable).lower(), \"teamspace\" in os.getcwd()]):\n",
        "            self.current_platform = \"Lightning AI\"\n",
        "            self.pip_cmd, self.python_cmd, self.use_venv = \"pip\", \"python\", False\n",
        "        else:\n",
        "            self.current_platform = \"Generic\"\n",
        "            self.pip_cmd, self.python_cmd, self.use_venv = \".venv/bin/pip\", \".venv/bin/python\", True\n",
        "\n",
        "    def setup_corrected_model_definitions(self):\n",
        "        \"\"\"Setup model definitions with CORRECTED URLs matching actual repository files\"\"\"\n",
        "\n",
        "        # Core Models - Based on actual repository structure\n",
        "        self.models = {\n",
        "            \"wan_14b_phantom\": {\n",
        "                \"url\": f\"{self.resolve_url}/Phantom-Wan-14B_fp16.safetensors\",\n",
        "                \"filename\": \"Phantom-Wan-14B_fp16.safetensors\",\n",
        "                \"size\": \"29.1 GB\",\n",
        "                \"path\": \"ckpts/\",\n",
        "                \"description\": \"Phantom Wan 14B model - Best for object/person transfer\"\n",
        "            },\n",
        "            \"wan_14b_skyreels\": {\n",
        "                \"url\": f\"{self.resolve_url}/SkyReels-v2-14B_fp16.safetensors\",\n",
        "                \"filename\": \"SkyReels-v2-14B_fp16.safetensors\",\n",
        "                \"size\": \"29.1 GB\",\n",
        "                \"path\": \"ckpts/\",\n",
        "                \"description\": \"SkyReels v2 14B - High quality video generation\"\n",
        "            },\n",
        "            \"wan_1_3b_base\": {\n",
        "                \"url\": f\"{self.resolve_url}/Wan2_1-T2V-1_3B_bf16.safetensors\",\n",
        "                \"filename\": \"Wan2_1-T2V-1_3B_bf16.safetensors\",\n",
        "                \"size\": \"2.87 GB\",\n",
        "                \"path\": \"ckpts/\",\n",
        "                \"description\": \"Wan 2.1 1.3B model - Fast generation for lower hardware\"\n",
        "            },\n",
        "            \"wan_1_3b_fp32\": {\n",
        "                \"url\": f\"{self.resolve_url}/Wan2_1-T2V-1_3B_fp32.safetensors\",\n",
        "                \"filename\": \"Wan2_1-T2V-1_3B_fp32.safetensors\",\n",
        "                \"size\": \"5.68 GB\",\n",
        "                \"path\": \"ckpts/\",\n",
        "                \"description\": \"Wan 2.1 1.3B FP32 - Higher precision variant\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Text Encoders\n",
        "        self.text_encoders = {\n",
        "            \"umt5_xxl_fp16\": {\n",
        "                \"url\": f\"{self.resolve_url}/umt5_xxl_fp16.safetensors\",\n",
        "                \"filename\": \"umt5_xxl_fp16.safetensors\",\n",
        "                \"size\": \"2.98 GB\",\n",
        "                \"path\": \"text_encoders/\",\n",
        "                \"description\": \"T5 text encoder - Required for all video generation\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # VAE Models\n",
        "        self.vaes = {\n",
        "            \"wan_vae_bf16\": {\n",
        "                \"url\": f\"{self.resolve_url}/Wan2_1_VAE_bf16.safetensors\",\n",
        "                \"filename\": \"Wan2_1_VAE_bf16.safetensors\",\n",
        "                \"size\": \"254 MB\",\n",
        "                \"path\": \"vae/\",\n",
        "                \"description\": \"Video VAE - Converts between pixel and latent space\"\n",
        "            },\n",
        "            \"wan_vae_fp32\": {\n",
        "                \"url\": f\"{self.resolve_url}/Wan2_1_VAE_fp32.safetensors\",\n",
        "                \"filename\": \"Wan2_1_VAE_fp32.safetensors\",\n",
        "                \"size\": \"508 MB\",\n",
        "                \"path\": \"vae/\",\n",
        "                \"description\": \"Video VAE FP32 - Higher precision variant\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # CLIP Vision\n",
        "        self.clip_vision = {\n",
        "            \"clip_vision_h\": {\n",
        "                \"url\": f\"{self.resolve_url}/clip_vision_h.safetensors\",\n",
        "                \"filename\": \"clip_vision_h.safetensors\",\n",
        "                \"size\": \"1.47 GB\",\n",
        "                \"path\": \"clip_vision/\",\n",
        "                \"description\": \"CLIP vision encoder for image-to-video tasks\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # LoRAs - Using corrected URLs from documentation\n",
        "        self.loras = {\n",
        "            \"causVid_14b\": {\n",
        "                \"url\": f\"{self.resolve_url}/Wan21_CausVid_14B_T2V_lora_rank32.safetensors\",\n",
        "                \"filename\": \"Wan21_CausVid_14B_T2V_lora_rank32.safetensors\",\n",
        "                \"size\": \"323 MB\",\n",
        "                \"path\": \"loras14B/\",\n",
        "                \"description\": \"CausVid LoRA - 4-12 step generation with 2x speed improvement\"\n",
        "            },\n",
        "            \"accVid_t2v_14b\": {\n",
        "                \"url\": f\"{self.resolve_url}/Wan21AccVidT2V14Blorarank32fp16.safetensors\",\n",
        "                \"filename\": \"Wan21AccVidT2V14Blorarank32fp16.safetensors\",\n",
        "                \"size\": \"317 MB\",\n",
        "                \"path\": \"loras14B/\",\n",
        "                \"description\": \"AccVid T2V LoRA - 2x speed with no classifier-free guidance\"\n",
        "            },\n",
        "            \"accVid_i2v_14b\": {\n",
        "                \"url\": f\"{self.resolve_url}/Wan21AccVidI2V480P14Blorarank32fp16.safetensors\",\n",
        "                \"filename\": \"Wan21AccVidI2V480P14Blorarank32fp16.safetensors\",\n",
        "                \"size\": \"319 MB\",\n",
        "                \"path\": \"lorasi2v/\",\n",
        "                \"description\": \"AccVid I2V LoRA - Image-to-video acceleration\"\n",
        "            },\n",
        "            \"safe_forcing_14b\": {\n",
        "                \"url\": f\"{self.resolve_url}/Wan21T2V14Blightx2vcfgstepdistilllorarank32.safetensors\",\n",
        "                \"filename\": \"Wan21T2V14Blightx2vcfgstepdistilllorarank32.safetensors\",\n",
        "                \"size\": \"311 MB\",\n",
        "                \"path\": \"loras14B/\",\n",
        "                \"description\": \"Safe-Forcing LoRA - 2-8 step generation with 2x speed\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def create_directory_structure(self):\n",
        "        \"\"\"Create necessary directories for downloads\"\"\"\n",
        "        directories = [\n",
        "            \"ckpts\", \"text_encoders\", \"vae\", \"vae_approx\", \"clip_vision\",\n",
        "            \"loras\", \"loras1.3B\", \"loras14B\", \"lorasi2v\", \"lorashunyuan\", \"lorasltxv\"\n",
        "        ]\n",
        "\n",
        "        print(\"üìÅ Creating directory structure...\")\n",
        "        for directory in directories:\n",
        "            Path(directory).mkdir(parents=True, exist_ok=True)\n",
        "            print(f\"   ‚úÖ {directory}/\")\n",
        "\n",
        "        print(\"‚úÖ Directory structure created successfully\")\n",
        "\n",
        "    def download_with_hf_hub(self, url, filename, output_dir):\n",
        "        \"\"\"Download using HuggingFace Hub API with authentication\"\"\"\n",
        "        try:\n",
        "            # Try installing huggingface_hub if not available\n",
        "            try:\n",
        "                from huggingface_hub import hf_hub_download\n",
        "            except ImportError:\n",
        "                print(\"üì¶ Installing huggingface_hub...\")\n",
        "                subprocess.run([self.pip_cmd, \"install\", \"huggingface_hub\"], check=True)\n",
        "                from huggingface_hub import hf_hub_download\n",
        "\n",
        "            # Parse HuggingFace URL to extract repo and filename\n",
        "            if \"/resolve/main/\" in url:\n",
        "                parts = url.split(\"/resolve/main/\")\n",
        "                repo_path = parts[0].replace(\"https://huggingface.co/\", \"\")\n",
        "                file_path = parts[1]\n",
        "            else:\n",
        "                print(\"‚ùå Invalid HuggingFace URL format\")\n",
        "                return False\n",
        "\n",
        "            print(f\"üì• Downloading with HuggingFace Hub API...\")\n",
        "            print(f\"üìÅ Repo: {repo_path}\")\n",
        "            print(f\"üìÑ File: {file_path}\")\n",
        "\n",
        "            # Use token if available\n",
        "            token = self.hf_token_manager.token if self.hf_token_manager.authenticated else None\n",
        "\n",
        "            downloaded_file = hf_hub_download(\n",
        "                repo_id=repo_path,\n",
        "                filename=file_path,\n",
        "                token=token,\n",
        "                local_dir=output_dir,\n",
        "                local_dir_use_symlinks=False,\n",
        "                resume_download=True\n",
        "            )\n",
        "\n",
        "            print(f\"‚úÖ HuggingFace Hub download successful\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå HuggingFace Hub download failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def download_with_aria2c(self, url, filename, output_dir):\n",
        "        \"\"\"Download using aria2c with authentication headers\"\"\"\n",
        "        try:\n",
        "            output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "            cmd = [\n",
        "                \"aria2c\",\n",
        "                \"-x\", \"16\",        # 16 connections\n",
        "                \"-s\", \"16\",        # 16 segments\n",
        "                \"-k\", \"1M\",        # 1MB per segment\n",
        "                \"--continue\",      # Resume capability\n",
        "                \"--dir\", output_dir,\n",
        "                \"--out\", filename,\n",
        "                url\n",
        "            ]\n",
        "\n",
        "            # Add authentication headers if available\n",
        "            if self.hf_token_manager.authenticated:\n",
        "                cmd.extend([\"--header\", f\"Authorization: Bearer {self.hf_token_manager.token}\"])\n",
        "\n",
        "            print(f\"üì• Downloading with aria2c (16 connections)...\")\n",
        "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=1800)\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print(f\"‚úÖ aria2c download successful\")\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"‚ùå aria2c failed: {result.stderr[:200]}\")\n",
        "                return False\n",
        "\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(f\"‚è∞ aria2c download timed out\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå aria2c error: {e}\")\n",
        "            return False\n",
        "\n",
        "    def download_with_wget(self, url, filename, output_dir):\n",
        "        \"\"\"Download using wget with authentication\"\"\"\n",
        "        try:\n",
        "            output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "            cmd = [\n",
        "                \"wget\",\n",
        "                \"--continue\",              # Resume capability\n",
        "                \"--progress=bar\",          # Progress bar\n",
        "                \"--timeout=30\",            # Connection timeout\n",
        "                \"--tries=3\",               # Retry attempts\n",
        "                \"-O\", output_path,\n",
        "                url\n",
        "            ]\n",
        "\n",
        "            # Add authentication headers if available\n",
        "            if self.hf_token_manager.authenticated:\n",
        "                cmd.extend([\"--header\", f\"Authorization: Bearer {self.hf_token_manager.token}\"])\n",
        "\n",
        "            print(f\"üì• Downloading with wget...\")\n",
        "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=1800)\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print(f\"‚úÖ wget download successful\")\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"‚ùå wget failed: {result.stderr[:200]}\")\n",
        "                return False\n",
        "\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(f\"‚è∞ wget download timed out\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå wget error: {e}\")\n",
        "            return False\n",
        "\n",
        "    def download_with_python(self, url, filename, output_dir):\n",
        "        \"\"\"Download using Python urllib with authentication\"\"\"\n",
        "        try:\n",
        "            import urllib.request\n",
        "            import shutil\n",
        "\n",
        "            output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "            print(f\"üì• Downloading with Python urllib...\")\n",
        "\n",
        "            # Create request with authentication if available\n",
        "            req = urllib.request.Request(url)\n",
        "            if self.hf_token_manager.authenticated:\n",
        "                req.add_header('Authorization', f'Bearer {self.hf_token_manager.token}')\n",
        "\n",
        "            with urllib.request.urlopen(req) as response:\n",
        "                with open(output_path, 'wb') as out_file:\n",
        "                    shutil.copyfileobj(response, out_file)\n",
        "\n",
        "            print(f\"‚úÖ Python download successful\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Python download failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def download_file(self, url, filename, output_dir, size_info=\"Unknown\"):\n",
        "        \"\"\"Download a single file with progressive fallback methods\"\"\"\n",
        "        output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "        # Check if file already exists\n",
        "        if os.path.exists(output_path):\n",
        "            print(f\"üìÑ {filename} already exists - skipping\")\n",
        "            return True\n",
        "\n",
        "        print(f\"\\nüì• Downloading: {filename}\")\n",
        "        print(f\"üìä Size: {size_info}\")\n",
        "        print(f\"üîó URL: {url}\")\n",
        "        print(f\"üìÅ Destination: {output_path}\")\n",
        "\n",
        "        # Try each download method in order\n",
        "        for method_name, method_func in self.download_methods:\n",
        "            print(f\"\\nüîÑ Trying {method_name}...\")\n",
        "\n",
        "            try:\n",
        "                if method_func(url, filename, output_dir):\n",
        "                    print(f\"‚úÖ Download successful with {method_name}\")\n",
        "                    return True\n",
        "                else:\n",
        "                    print(f\"‚ùå {method_name} failed - trying next method\")\n",
        "                    continue\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå {method_name} crashed: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"üí• All download methods failed for {filename}\")\n",
        "        return False\n",
        "\n",
        "    def download_category(self, category_name, items_dict):\n",
        "        \"\"\"Download all items in a category\"\"\"\n",
        "        print(f\"\\nüéØ Downloading {category_name}...\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        success_count = 0\n",
        "        total_count = len(items_dict)\n",
        "\n",
        "        for item_key, item_info in items_dict.items():\n",
        "            print(f\"\\n[{success_count + 1}/{total_count}] Processing {item_key}...\")\n",
        "\n",
        "            if self.download_file(\n",
        "                item_info[\"url\"],\n",
        "                item_info[\"filename\"],\n",
        "                item_info[\"path\"],\n",
        "                item_info[\"size\"]\n",
        "            ):\n",
        "                success_count += 1\n",
        "                self.download_stats[\"downloaded_files\"] += 1\n",
        "            else:\n",
        "                self.download_stats[\"failed_files\"] += 1\n",
        "\n",
        "            self.download_stats[\"total_files\"] += 1\n",
        "\n",
        "        print(f\"\\n‚úÖ {category_name} Downloads Complete: {success_count}/{total_count} successful\")\n",
        "        return success_count == total_count\n",
        "\n",
        "    def run_complete_download_system(self):\n",
        "        \"\"\"Run the complete download system with corrected URLs and permanent token storage\"\"\"\n",
        "        print(\"üöÄ WAN2GP Model and LoRA Download System\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"Platform: {self.current_platform}\")\n",
        "        print(f\"Virtual Environment: {'Yes' if self.use_venv else 'No'}\")\n",
        "        print(f\"Pip Command: {self.pip_cmd}\")\n",
        "        print(f\"Python Command: {self.python_cmd}\")\n",
        "        print(f\"Download Methods: HF Hub ‚Üí aria2c ‚Üí wget ‚Üí python urllib\")\n",
        "        print(f\"Repository: {self.base_repo_url}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Final enforcement check\n",
        "        if hasattr(self, 'current_platform') and self.current_platform == \"Google Colab\" and not self.use_venv:\n",
        "            print(\"üö® FATAL: Google Colab not using venv - ABORTING DOWNLOADS\")\n",
        "            return False\n",
        "\n",
        "        # Initialize token system\n",
        "        print(\"\\nüîê Checking for permanent HuggingFace token...\")\n",
        "        self.hf_token_manager.check_existing_token()\n",
        "\n",
        "        # Create directory structure\n",
        "        self.create_directory_structure()\n",
        "\n",
        "        # Download categories with corrected model definitions\n",
        "        download_categories = [\n",
        "            (\"Essential Models\", self.models),\n",
        "            (\"Text Encoders\", self.text_encoders),\n",
        "            (\"VAE Models\", self.vaes),\n",
        "            (\"CLIP Vision\", self.clip_vision),\n",
        "            (\"Performance LoRAs\", self.loras)\n",
        "        ]\n",
        "\n",
        "        print(\"\\nüéØ Available Download Categories (CORRECTED URLs):\")\n",
        "        for i, (category_name, items) in enumerate(download_categories, 1):\n",
        "            item_count = len(items)\n",
        "            total_size = 0\n",
        "            for item in items.values():\n",
        "                try:\n",
        "                    size_parts = item[\"size\"].split()\n",
        "                    if len(size_parts) >= 2:\n",
        "                        number = float(size_parts[0])\n",
        "                        unit = size_parts[1].upper()\n",
        "                        multipliers = {\"GB\": 1, \"MB\": 0.001}\n",
        "                        total_size += number * multipliers.get(unit, 0)\n",
        "                except:\n",
        "                    continue\n",
        "            print(f\"{i}. {category_name} ({item_count} files, ~{total_size:.1f} GB)\")\n",
        "\n",
        "        print(\"\\nüìã Download Options:\")\n",
        "        print(\"A. Download All (Recommended)\")\n",
        "        print(\"B. Download Essential Only (Models + Text Encoders + VAE)\")\n",
        "        print(\"C. Download LoRAs Only\")\n",
        "        print(\"D. Download Models Only\")\n",
        "\n",
        "        choice = input(\"\\nüéØ Select download option (A/B/C/D): \").strip().upper()\n",
        "\n",
        "        if choice == \"A\":\n",
        "            print(\"\\nüöÄ Downloading All Categories...\")\n",
        "            for category_name, items in download_categories:\n",
        "                self.download_category(category_name, items)\n",
        "\n",
        "        elif choice == \"B\":\n",
        "            print(\"\\nüöÄ Downloading Essential Components...\")\n",
        "            essential_categories = [\n",
        "                (\"Essential Models\", self.models),\n",
        "                (\"Text Encoders\", self.text_encoders),\n",
        "                (\"VAE Models\", self.vaes)\n",
        "            ]\n",
        "            for category_name, items in essential_categories:\n",
        "                self.download_category(category_name, items)\n",
        "\n",
        "        elif choice == \"C\":\n",
        "            print(\"\\nüöÄ Downloading LoRAs Only...\")\n",
        "            self.download_category(\"Performance LoRAs\", self.loras)\n",
        "\n",
        "        elif choice == \"D\":\n",
        "            print(\"\\nüöÄ Downloading Models Only...\")\n",
        "            self.download_category(\"Essential Models\", self.models)\n",
        "\n",
        "        else:\n",
        "            print(\"‚ùå Invalid choice. Defaulting to Essential Only.\")\n",
        "            essential_categories = [\n",
        "                (\"Essential Models\", self.models),\n",
        "                (\"Text Encoders\", self.text_encoders),\n",
        "                (\"VAE Models\", self.vaes)\n",
        "            ]\n",
        "            for category_name, items in essential_categories:\n",
        "                self.download_category(category_name, items)\n",
        "\n",
        "        # Final statistics\n",
        "        print(\"\\nüìä Download Statistics:\")\n",
        "        print(f\"Total files processed: {self.download_stats['total_files']}\")\n",
        "        print(f\"Successfully downloaded: {self.download_stats['downloaded_files']}\")\n",
        "        print(f\"Failed downloads: {self.download_stats['failed_files']}\")\n",
        "\n",
        "        success_rate = (self.download_stats['downloaded_files'] / max(self.download_stats['total_files'], 1)) * 100\n",
        "        print(f\"Success rate: {success_rate:.1f}%\")\n",
        "\n",
        "        if self.download_stats['failed_files'] == 0:\n",
        "            print(\"\\nüéâ All downloads completed successfully!\")\n",
        "            print(\"‚úÖ Ready to launch WAN2GP!\")\n",
        "        else:\n",
        "            print(f\"\\n‚ö†Ô∏è {self.download_stats['failed_files']} files failed to download\")\n",
        "            print(\"üîÑ You can re-run this cell to retry failed downloads\")\n",
        "\n",
        "        return self.download_stats['failed_files'] == 0\n",
        "\n",
        "# Colab function bindings for JavaScript integration\n",
        "def check_existing_hf_token():\n",
        "    \"\"\"Check for existing HF token (called from JavaScript)\"\"\"\n",
        "    global downloader\n",
        "    downloader.hf_token_manager.check_existing_token()\n",
        "\n",
        "def save_hf_token_permanently(token):\n",
        "    \"\"\"Save HF token permanently (called from JavaScript)\"\"\"\n",
        "    global downloader\n",
        "    if downloader.hf_token_manager.save_token_permanently(token):\n",
        "        downloader.hf_token_manager.authenticated = True\n",
        "        print(\"üîë HuggingFace token saved and authenticated successfully!\")\n",
        "    else:\n",
        "        print(\"‚ùå Failed to save HuggingFace token\")\n",
        "\n",
        "def test_hf_token(token):\n",
        "    \"\"\"Test HF token validity (called from JavaScript)\"\"\"\n",
        "    global downloader\n",
        "    if downloader.hf_token_manager.test_token_validity(token):\n",
        "        print(\"üß™ Token test successful - ready for authenticated downloads!\")\n",
        "    else:\n",
        "        print(\"‚ùå Token test failed - please check your token\")\n",
        "\n",
        "def skip_hf_token_setup():\n",
        "    \"\"\"Skip HF token setup (called from JavaScript)\"\"\"\n",
        "    print(\"‚è≠Ô∏è Skipped HuggingFace token setup - using public access only\")\n",
        "\n",
        "# Execute the enhanced download system with fixed URLs and permanent token storage\n",
        "downloader = FixedModelDownloader()\n",
        "downloader.run_complete_download_system()\n"
      ],
      "metadata": {
        "id": "yP38IfT3_Gy9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ebf4269c-e996-491a-b943-fbed389ba87f",
        "cellView": "form"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ VERIFIED: Download system correctly using venv for Colab\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <div style=\"font-family: Arial, sans-serif; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); \n",
              "                    color: white; padding: 15px; border-radius: 10px; margin: 10px 0; \n",
              "                    box-shadow: 0 4px 15px rgba(0,0,0,0.2);\">\n",
              "            <h3 style=\"margin: 0; color: white;\">üîê Permanent HuggingFace Token Manager</h3>\n",
              "            <p style=\"margin: 5px 0;\">Store your HF token permanently for faster downloads and private model access</p>\n",
              "            <div id=\"hf-token-status\" style=\"margin-top: 10px;\">\n",
              "                <div id=\"token-check-result\"></div>\n",
              "                <button onclick=\"checkExistingToken()\" \n",
              "                        style=\"background: #2ed573; color: white; border: none; padding: 8px 16px; \n",
              "                               border-radius: 5px; cursor: pointer; margin-right: 10px;\">\n",
              "                    üîç Check Existing Token\n",
              "                </button>\n",
              "                <button onclick=\"setupNewToken()\" \n",
              "                        style=\"background: #3742fa; color: white; border: none; padding: 8px 16px; \n",
              "                               border-radius: 5px; cursor: pointer; margin-right: 10px;\">\n",
              "                    üîë Setup New Token\n",
              "                </button>\n",
              "                <button onclick=\"skipTokenSetup()\" \n",
              "                        style=\"background: #57606f; color: white; border: none; padding: 8px 16px; \n",
              "                               border-radius: 5px; cursor: pointer;\">\n",
              "                    ‚è≠Ô∏è Skip Token Setup\n",
              "                </button>\n",
              "            </div>\n",
              "            <div id=\"token-input-area\" style=\"margin-top: 15px; display: none;\">\n",
              "                <input type=\"password\" id=\"hf-token-input\" placeholder=\"Enter HuggingFace Token\" \n",
              "                       style=\"padding: 8px; border-radius: 5px; border: 1px solid #ccc; margin-right: 10px; width: 300px;\">\n",
              "                <button onclick=\"saveToken()\" \n",
              "                        style=\"background: #1dd1a1; color: white; border: none; padding: 8px 16px; \n",
              "                               border-radius: 5px; cursor: pointer; margin-right: 10px;\">\n",
              "                    üíæ Save Permanently\n",
              "                </button>\n",
              "                <button onclick=\"testToken()\" \n",
              "                        style=\"background: #feca57; color: black; border: none; padding: 8px 16px; \n",
              "                               border-radius: 5px; cursor: pointer;\">\n",
              "                    üß™ Test Token\n",
              "                </button>\n",
              "                <p style=\"font-size: 12px; margin-top: 5px;\">\n",
              "                    Get your token from: <a href=\"https://huggingface.co/settings/tokens\" target=\"_blank\" \n",
              "                    style=\"color: #ffc048;\">https://huggingface.co/settings/tokens</a>\n",
              "                </p>\n",
              "            </div>\n",
              "        </div>\n",
              "        \n",
              "        <script>\n",
              "        function checkExistingToken() {\n",
              "            google.colab.kernel.invokeFunction('check_existing_hf_token', [], {});\n",
              "        }\n",
              "        \n",
              "        function setupNewToken() {\n",
              "            document.getElementById('token-input-area').style.display = 'block';\n",
              "            document.getElementById('hf-token-input').focus();\n",
              "        }\n",
              "        \n",
              "        function saveToken() {\n",
              "            const token = document.getElementById('hf-token-input').value;\n",
              "            if (token && token.startsWith('hf_')) {\n",
              "                google.colab.kernel.invokeFunction('save_hf_token_permanently', [token], {});\n",
              "                document.getElementById('token-check-result').innerHTML = \n",
              "                    '<p style=\"color: #2ed573;\">üíæ Saving token permanently...</p>';\n",
              "            } else {\n",
              "                document.getElementById('token-check-result').innerHTML = \n",
              "                    '<p style=\"color: #ff3838;\">‚ùå Invalid token format. Must start with \"hf_\"</p>';\n",
              "            }\n",
              "        }\n",
              "        \n",
              "        function testToken() {\n",
              "            const token = document.getElementById('hf-token-input').value;\n",
              "            if (token) {\n",
              "                google.colab.kernel.invokeFunction('test_hf_token', [token], {});\n",
              "                document.getElementById('token-check-result').innerHTML = \n",
              "                    '<p style=\"color: #feca57;\">üß™ Testing token...</p>';\n",
              "            }\n",
              "        }\n",
              "        \n",
              "        function skipTokenSetup() {\n",
              "            google.colab.kernel.invokeFunction('skip_hf_token_setup', [], {});\n",
              "            document.getElementById('hf-token-status').innerHTML = \n",
              "                '<p style=\"color: #ffc048;\">‚è≠Ô∏è Skipped token setup - using public access only</p>';\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ WAN2GP Model and LoRA Download System\n",
            "======================================================================\n",
            "Platform: Google Colab\n",
            "Virtual Environment: Yes\n",
            "Pip Command: .venv/bin/pip\n",
            "Python Command: .venv/bin/python\n",
            "Download Methods: HF Hub ‚Üí aria2c ‚Üí wget ‚Üí python urllib\n",
            "Repository: https://huggingface.co/Kijai/WanVideo_comfy\n",
            "======================================================================\n",
            "\n",
            "üîê Checking for permanent HuggingFace token...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <div style=\"background: #fff3cd; color: #856404; padding: 10px; border-radius: 5px; \n",
              "                    border: 1px solid #ffeaa7; margin: 5px 0;\">\n",
              "            <strong>‚ÑπÔ∏è No Existing Token Found</strong><br>\n",
              "            üí° You can setup a new token for faster downloads and private model access\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Creating directory structure...\n",
            "   ‚úÖ ckpts/\n",
            "   ‚úÖ text_encoders/\n",
            "   ‚úÖ vae/\n",
            "   ‚úÖ vae_approx/\n",
            "   ‚úÖ clip_vision/\n",
            "   ‚úÖ loras/\n",
            "   ‚úÖ loras1.3B/\n",
            "   ‚úÖ loras14B/\n",
            "   ‚úÖ lorasi2v/\n",
            "   ‚úÖ lorashunyuan/\n",
            "   ‚úÖ lorasltxv/\n",
            "‚úÖ Directory structure created successfully\n",
            "\n",
            "üéØ Available Download Categories (CORRECTED URLs):\n",
            "1. Essential Models (4 files, ~66.8 GB)\n",
            "2. Text Encoders (1 files, ~3.0 GB)\n",
            "3. VAE Models (2 files, ~0.8 GB)\n",
            "4. CLIP Vision (1 files, ~1.5 GB)\n",
            "5. Performance LoRAs (4 files, ~1.3 GB)\n",
            "\n",
            "üìã Download Options:\n",
            "A. Download All (Recommended)\n",
            "B. Download Essential Only (Models + Text Encoders + VAE)\n",
            "C. Download LoRAs Only\n",
            "D. Download Models Only\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-12-1251885429.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;31m# Execute the enhanced download system with fixed URLs and permanent token storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0mdownloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFixedModelDownloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m \u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_complete_download_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-12-1251885429.py\u001b[0m in \u001b[0;36mrun_complete_download_system\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"D. Download Models Only\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüéØ Select download option (A/B/C/D): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"A\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell (4) WAN2GP Launch - Enhanced with venv Enforcement v3.2\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def jupyter_lab_title(title):\n",
        "    if \"google.colab\" not in sys.modules:\n",
        "        from IPython.display import display, Markdown\n",
        "        display(Markdown(f\"## {title}\"))\n",
        "\n",
        "jupyter_lab_title(\"WAN2GP Launch - Enhanced with venv Enforcement\")\n",
        "\n",
        "class WAN2GPLauncher:\n",
        "    def __init__(self):\n",
        "        # Use global platform configuration from previous cells\n",
        "        try:\n",
        "            self.current_platform = current_platform\n",
        "            self.pip_cmd = pip_cmd\n",
        "            self.python_cmd = python_cmd\n",
        "            self.use_venv = use_venv\n",
        "        except NameError:\n",
        "            print(\"‚ö†Ô∏è Platform detection variables not found. Using fallback detection.\")\n",
        "            self.detect_platform_fallback()\n",
        "\n",
        "        # CRITICAL ENFORCEMENT: Ensure venv for Google Colab\n",
        "        self.enforce_colab_venv()\n",
        "\n",
        "    def enforce_colab_venv(self):\n",
        "        \"\"\"CRITICAL: Enforce venv usage for Google Colab\"\"\"\n",
        "        if self.current_platform == \"Google Colab\":\n",
        "            if not self.use_venv or self.python_cmd != \".venv/bin/python\":\n",
        "                print(\"üö® ENFORCING: Google Colab MUST use venv for WAN2GP launch\")\n",
        "                self.use_venv = True\n",
        "                self.pip_cmd = \".venv/bin/pip\"\n",
        "                self.python_cmd = \".venv/bin/python\"\n",
        "                print(\"‚úÖ ENFORCED: Launch system using venv python command\")\n",
        "            else:\n",
        "                print(\"‚úÖ VERIFIED: Launch system correctly using venv for Colab\")\n",
        "\n",
        "    def detect_platform_fallback(self):\n",
        "        \"\"\"Fallback platform detection - ENFORCES VENV FOR COLAB\"\"\"\n",
        "        if \"google.colab\" in sys.modules:\n",
        "            self.current_platform = \"Google Colab\"\n",
        "            # ENFORCED: Always venv for Colab\n",
        "            self.pip_cmd, self.python_cmd, self.use_venv = \".venv/bin/pip\", \".venv/bin/python\", True\n",
        "        elif any([\"lightning\" in str(sys.executable).lower(), \"teamspace\" in os.getcwd()]):\n",
        "            self.current_platform = \"Lightning AI\"\n",
        "            self.pip_cmd, self.python_cmd, self.use_venv = \"pip\", \"python\", False\n",
        "        else:\n",
        "            self.current_platform = \"Generic\"\n",
        "            self.pip_cmd, self.python_cmd, self.use_venv = \".venv/bin/pip\", \".venv/bin/python\", True\n",
        "\n",
        "    def launch_wangp(self, model_type=\"t2v-1-3B\", additional_args=None):\n",
        "        \"\"\"Launch WAN2GP with enforced venv python command\"\"\"\n",
        "        print(\"üöÄ Launching WAN2GP...\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Platform: {self.current_platform}\")\n",
        "        print(f\"Python Command: {self.python_cmd}\")\n",
        "        print(f\"Model Type: {model_type}\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Final enforcement check\n",
        "        if self.current_platform == \"Google Colab\" and not self.use_venv:\n",
        "            print(\"üö® FATAL: Google Colab not using venv - ABORTING LAUNCH\")\n",
        "            return False\n",
        "\n",
        "        # Verify we're in the correct directory\n",
        "        if not os.path.exists(\"wgp.py\"):\n",
        "            print(\"‚ùå wgp.py not found. Make sure you're in the Wan2GP directory.\")\n",
        "            print(f\"üìç Current directory: {os.getcwd()}\")\n",
        "            return False\n",
        "\n",
        "        # Build launch command\n",
        "        launch_cmd = [self.python_cmd, \"wgp.py --share\", f\"--{model_type}\"]\n",
        "\n",
        "        # Add additional arguments if provided\n",
        "        if additional_args:\n",
        "            launch_cmd.extend(additional_args)\n",
        "\n",
        "        print(f\"üéØ Launch command: {' '.join(launch_cmd)}\")\n",
        "        print(\"\\nüîÑ Starting WAN2GP...\")\n",
        "        print(\"üåê Once launched, access the interface at: http://localhost:7860\")\n",
        "        print(\"‚èπÔ∏è Press Ctrl+C to stop WAN2GP\")\n",
        "\n",
        "        try:\n",
        "            # Launch WAN2GP\n",
        "            subprocess.run(launch_cmd)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n‚èπÔ∏è WAN2GP stopped by user\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Launch failed: {e}\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def show_launch_options(self):\n",
        "        \"\"\"Show available launch options\"\"\"\n",
        "        print(\"üéØ WAN2GP Launch Options:\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        options = {\n",
        "            \"1\": (\"t2v-1-3B\", \"Wan 2.1 Text2Video 1.3B (Fast, 6GB VRAM)\"),\n",
        "            \"2\": (\"t2v-14B\", \"Wan 2.1 Text2Video 14B (High Quality, 12GB VRAM)\"),\n",
        "            \"3\": (\"i2v-1-3B\", \"Wan Fun InP 1.3B (Image2Video, 6GB VRAM)\"),\n",
        "            \"4\": (\"i2v-14B\", \"Wan 2.1 Image2Video 14B (High Quality I2V, 12GB VRAM)\"),\n",
        "            \"5\": (\"vace-1-3B\", \"Wan Vace 1.3B (ControlNet, 6GB VRAM)\"),\n",
        "            \"6\": (\"vace-14B\", \"Wan Vace 14B (ControlNet High Quality, 12GB VRAM)\")\n",
        "        }\n",
        "\n",
        "        for key, (model, description) in options.items():\n",
        "            print(f\"{key}. {description}\")\n",
        "\n",
        "        print(\"\\nüîß Performance Options:\")\n",
        "        print(\"A. Low VRAM Mode (--profile 4 --attention sdpa)\")\n",
        "        print(\"B. High Performance Mode (--compile --attention sage)\")\n",
        "        print(\"C. Custom Launch (enter your own arguments)\")\n",
        "\n",
        "        choice = input(\"\\nüéØ Select option (1-6, A-C): \").strip()\n",
        "\n",
        "        additional_args = []\n",
        "\n",
        "        if choice in options:\n",
        "            model_type = options[choice][0]\n",
        "        elif choice.upper() == \"A\":\n",
        "            model_type = \"t2v-1-3B\"\n",
        "            additional_args = [\"--profile\", \"4\", \"--attention\", \"sdpa\"]\n",
        "        elif choice.upper() == \"B\":\n",
        "            model_type = \"t2v-14B\"\n",
        "            additional_args = [\"--compile\", \"--attention\", \"sage\"]\n",
        "        elif choice.upper() == \"C\":\n",
        "            model_type = input(\"Enter model type (e.g., t2v-1-3B): \").strip()\n",
        "            custom_args = input(\"Enter additional arguments (optional): \").strip()\n",
        "            if custom_args:\n",
        "                additional_args = custom_args.split()\n",
        "        else:\n",
        "            print(\"‚ùå Invalid choice. Using default t2v-1-3B\")\n",
        "            model_type = \"t2v-1-3B\"\n",
        "\n",
        "        return model_type, additional_args\n",
        "\n",
        "# Create launcher instance\n",
        "launcher = WAN2GPLauncher()\n",
        "\n",
        "# Show options and launch\n",
        "model_type, additional_args = launcher.show_launch_options()\n",
        "launcher.launch_wangp(model_type, additional_args)\n"
      ],
      "metadata": {
        "id": "GU-zDsAW_MwT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "166d7ee5-4308-48b1-9243-4589452cef70"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ VERIFIED: Launch system correctly using venv for Colab\n",
            "üéØ WAN2GP Launch Options:\n",
            "==================================================\n",
            "1. Wan 2.1 Text2Video 1.3B (Fast, 6GB VRAM)\n",
            "2. Wan 2.1 Text2Video 14B (High Quality, 12GB VRAM)\n",
            "3. Wan Fun InP 1.3B (Image2Video, 6GB VRAM)\n",
            "4. Wan 2.1 Image2Video 14B (High Quality I2V, 12GB VRAM)\n",
            "5. Wan Vace 1.3B (ControlNet, 6GB VRAM)\n",
            "6. Wan Vace 14B (ControlNet High Quality, 12GB VRAM)\n",
            "\n",
            "üîß Performance Options:\n",
            "A. Low VRAM Mode (--profile 4 --attention sdpa)\n",
            "B. High Performance Mode (--compile --attention sage)\n",
            "C. Custom Launch (enter your own arguments)\n",
            "\n",
            "üéØ Select option (1-6, A-C): 1\n",
            "üöÄ Launching WAN2GP...\n",
            "==================================================\n",
            "Platform: Google Colab\n",
            "Python Command: .venv/bin/python\n",
            "Model Type: t2v-1-3B\n",
            "==================================================\n",
            "üéØ Launch command: .venv/bin/python wgp.py --share --t2v-1-3B\n",
            "\n",
            "üîÑ Starting WAN2GP...\n",
            "üåê Once launched, access the interface at: http://localhost:7860\n",
            "‚èπÔ∏è Press Ctrl+C to stop WAN2GP\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!.venv/bin/python wgp.py --share --t2v-1-3B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vkaq4q-wv2_3",
        "outputId": "7880a6bd-4243-4f1d-eb13-2e0617fd7b98"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/Wan2GP/wgp.py\", line 16, in <module>\n",
            "    import wan\n",
            "  File \"/content/Wan2GP/wan/__init__.py\", line 3, in <module>\n",
            "    from .text2video import WanT2V\n",
            "  File \"/content/Wan2GP/wan/text2video.py\", line 31, in <module>\n",
            "    from wgp import update_loras_slists\n",
            "  File \"/content/Wan2GP/wgp.py\", line 41, in <module>\n",
            "    from preprocessing.matanyone  import app as matanyone_app\n",
            "  File \"/content/Wan2GP/preprocessing/matanyone/app.py\", line 16, in <module>\n",
            "    from .tools.interact_tools import SamControler\n",
            "  File \"/content/Wan2GP/preprocessing/matanyone/tools/interact_tools.py\", line 8, in <module>\n",
            "    import matplotlib.pyplot as plt\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/matplotlib/__init__.py\", line 1296, in <module>\n",
            "    rcParams['backend'] = os.environ.get('MPLBACKEND')\n",
            "    ~~~~~~~~^^^^^^^^^^^\n",
            "  File \"/content/Wan2GP/.venv/lib/python3.11/site-packages/matplotlib/__init__.py\", line 771, in __setitem__\n",
            "    raise ValueError(f\"Key {key}: {ve}\") from None\n",
            "ValueError: Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell (1) - WAN2GP Comprehensive Diagnostic and Auto-Repair System (Latest v6.2 Compatible)\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "from IPython.display import display, HTML, Markdown\n",
        "import importlib.util\n",
        "\n",
        "class WAN2GPDiagnosticAndRepair:\n",
        "    def __init__(self):\n",
        "        self.platform = self.detect_platform()\n",
        "        self.pip_cmd, self.python_cmd, self.use_venv = self.get_platform_commands()\n",
        "        self.issues_found = []\n",
        "        self.repairs_applied = []\n",
        "        self.gpu_info = {}\n",
        "\n",
        "    def detect_platform(self):\n",
        "        \"\"\"Enhanced platform detection with comprehensive indicators\"\"\"\n",
        "        # Lightning AI detection - multiple indicators for reliability\n",
        "        lightning_indicators = [\n",
        "            \"lightning\" in str(sys.executable).lower(),\n",
        "            \"teamspace-studios\" in os.getcwd(),\n",
        "            \"LIGHTNING_CLOUDSPACE_HOST\" in os.environ,\n",
        "            \"LIGHTNING_CLOUDSPACE_ID\" in os.environ,\n",
        "            \"/commands/python\" in str(sys.executable),\n",
        "            \"/home/zeus/miniconda3/envs/cloudspace\" in str(sys.executable),\n",
        "            os.path.exists(\"/teamspace\"),\n",
        "            os.path.exists(\"/commands\")\n",
        "        ]\n",
        "\n",
        "        # Google Colab detection\n",
        "        colab_indicators = [\n",
        "            \"google.colab\" in sys.modules,\n",
        "            \"/content\" in os.getcwd()\n",
        "        ]\n",
        "\n",
        "        # Vast.AI detection\n",
        "        vast_indicators = [\n",
        "            \"VAST_CONTAINER_LABEL\" in os.environ,\n",
        "            \"/workspace\" in os.getcwd(),\n",
        "            \"vast\" in os.environ.get(\"HOSTNAME\", \"\").lower()\n",
        "        ]\n",
        "\n",
        "        if any(lightning_indicators):\n",
        "            return \"Lightning AI\"\n",
        "        elif any(colab_indicators):\n",
        "            return \"Google Colab\"\n",
        "        elif any(vast_indicators):\n",
        "            return \"Vast.AI/Generic\"\n",
        "        else:\n",
        "            return \"Vast.AI/Generic\"\n",
        "\n",
        "    def get_platform_commands(self):\n",
        "        \"\"\"Get platform-specific pip and python commands\"\"\"\n",
        "        if self.platform == \"Lightning AI\":\n",
        "            return \"pip\", \"python\", False  # (pip_cmd, python_cmd, use_venv)\n",
        "        elif self.platform == \"Google Colab\":\n",
        "            return \".venv/bin/pip\", \".venv/bin/python\", True\n",
        "        else:  # Vast.AI/Generic\n",
        "            return \".venv/bin/pip\", \".venv/bin/python\", True\n",
        "\n",
        "    def run_command_safely(self, command, description, timeout=60):\n",
        "        \"\"\"Execute command with comprehensive error handling\"\"\"\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                command, capture_output=True, text=True, timeout=timeout, shell=isinstance(command, str)\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                return True, result.stdout\n",
        "            else:\n",
        "                return False, result.stderr\n",
        "        except subprocess.TimeoutExpired:\n",
        "            return False, f\"Timeout after {timeout}s\"\n",
        "        except Exception as e:\n",
        "            return False, str(e)\n",
        "\n",
        "    def check_gpu_compatibility(self):\n",
        "        \"\"\"Comprehensive GPU detection and compatibility check\"\"\"\n",
        "        print(\"üîç Checking GPU compatibility...\")\n",
        "\n",
        "        # Check NVIDIA GPU presence\n",
        "        success, output = self.run_command_safely(\"nvidia-smi\", \"GPU Detection\")\n",
        "        if not success:\n",
        "            self.issues_found.append(\"No NVIDIA GPU detected or nvidia-smi not available\")\n",
        "            return False\n",
        "\n",
        "        # Parse GPU info\n",
        "        try:\n",
        "            # Extract GPU name and VRAM from nvidia-smi output\n",
        "            lines = output.split('\\n')\n",
        "            for line in lines:\n",
        "                if 'RTX' in line or 'GTX' in line or 'Tesla' in line or 'A100' in line:\n",
        "                    gpu_name = line.split('|')[1].strip() if '|' in line else \"Unknown GPU\"\n",
        "                    self.gpu_info['name'] = gpu_name\n",
        "                    break\n",
        "\n",
        "            # Check VRAM\n",
        "            for line in lines:\n",
        "                if 'MiB' in line and '/' in line:\n",
        "                    vram_info = [part for part in line.split() if 'MiB' in part]\n",
        "                    if len(vram_info) >= 2:\n",
        "                        total_vram = int(vram_info[-1].replace('MiB', ''))\n",
        "                        self.gpu_info['vram_mb'] = total_vram\n",
        "                        self.gpu_info['vram_gb'] = total_vram / 1024\n",
        "                        break\n",
        "\n",
        "            print(f\"‚úÖ GPU detected: {self.gpu_info.get('name', 'Unknown')}\")\n",
        "            print(f\"‚úÖ VRAM: {self.gpu_info.get('vram_gb', 0):.1f}GB\")\n",
        "\n",
        "            # Check VRAM adequacy\n",
        "            vram_gb = self.gpu_info.get('vram_gb', 0)\n",
        "            if vram_gb < 6:\n",
        "                self.issues_found.append(f\"Low VRAM detected ({vram_gb:.1f}GB). Minimum 6GB recommended.\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.issues_found.append(f\"GPU info parsing failed: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def check_cuda_pytorch_compatibility(self):\n",
        "        \"\"\"Check CUDA and PyTorch compatibility\"\"\"\n",
        "        print(\"üîç Checking CUDA and PyTorch compatibility...\")\n",
        "\n",
        "        # Check CUDA version\n",
        "        success, cuda_output = self.run_command_safely(\"nvcc --version\", \"CUDA Version Check\")\n",
        "        if not success:\n",
        "            success, cuda_output = self.run_command_safely(\"nvidia-smi\", \"CUDA Runtime Check\")\n",
        "\n",
        "        # Check PyTorch installation\n",
        "        try:\n",
        "            import torch\n",
        "            pytorch_version = torch.__version__\n",
        "            cuda_available = torch.cuda.is_available()\n",
        "\n",
        "            print(f\"‚úÖ PyTorch version: {pytorch_version}\")\n",
        "            print(f\"‚úÖ CUDA available in PyTorch: {cuda_available}\")\n",
        "\n",
        "            if not cuda_available:\n",
        "                self.issues_found.append(\"PyTorch cannot detect CUDA\")\n",
        "                return False\n",
        "\n",
        "            # Check for version compatibility\n",
        "            if \"50\" in self.gpu_info.get('name', '') and not pytorch_version.startswith('2.7'):\n",
        "                self.issues_found.append(\"RTX 50XX series requires PyTorch 2.7.0 or newer\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except ImportError:\n",
        "            self.issues_found.append(\"PyTorch not installed\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            self.issues_found.append(f\"PyTorch check failed: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def check_python_version(self):\n",
        "        \"\"\"Check Python version compatibility\"\"\"\n",
        "        print(\"üîç Checking Python version...\")\n",
        "\n",
        "        python_version = sys.version_info\n",
        "        version_string = f\"{python_version.major}.{python_version.minor}.{python_version.micro}\"\n",
        "        print(f\"‚úÖ Python version: {version_string}\")\n",
        "\n",
        "        if python_version.major != 3 or python_version.minor != 10:\n",
        "            self.issues_found.append(f\"Python {version_string} detected. Python 3.10.9 recommended for best compatibility.\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def check_wan2gp_installation(self):\n",
        "        \"\"\"Check WAN2GP installation and repository\"\"\"\n",
        "        print(\"üîç Checking WAN2GP installation...\")\n",
        "\n",
        "        wan2gp_paths = [\"Wan2GP\", \"wan2gp\", \"WAN2GP\", \"../Wan2GP\", \"../wan2gp\"]\n",
        "        wan2gp_found = False\n",
        "\n",
        "        for path in wan2gp_paths:\n",
        "            if os.path.exists(path):\n",
        "                wan2gp_found = True\n",
        "                wgp_py_path = os.path.join(path, \"wgp.py\")\n",
        "                if os.path.exists(wgp_py_path):\n",
        "                    print(f\"‚úÖ WAN2GP found at: {path}\")\n",
        "                    return True\n",
        "                break\n",
        "\n",
        "        if not wan2gp_found:\n",
        "            self.issues_found.append(\"WAN2GP repository not found\")\n",
        "            return False\n",
        "\n",
        "        self.issues_found.append(\"WAN2GP repository found but wgp.py missing\")\n",
        "        return False\n",
        "\n",
        "    def check_dependencies(self):\n",
        "        \"\"\"Check critical dependencies\"\"\"\n",
        "        print(\"üîç Checking critical dependencies...\")\n",
        "\n",
        "        critical_deps = {\n",
        "            'torch': 'PyTorch',\n",
        "            'torchvision': 'TorchVision',\n",
        "            'gradio': 'Gradio',\n",
        "            'transformers': 'Transformers',\n",
        "            'accelerate': 'Accelerate',\n",
        "            'diffusers': 'Diffusers'\n",
        "        }\n",
        "\n",
        "        missing_deps = []\n",
        "        for dep, name in critical_deps.items():\n",
        "            try:\n",
        "                importlib.import_module(dep)\n",
        "                print(f\"‚úÖ {name} installed\")\n",
        "            except ImportError:\n",
        "                missing_deps.append(dep)\n",
        "                print(f\"‚ùå {name} missing\")\n",
        "\n",
        "        if missing_deps:\n",
        "            self.issues_found.append(f\"Missing dependencies: {', '.join(missing_deps)}\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def check_attention_mechanisms(self):\n",
        "        \"\"\"Check available attention mechanisms\"\"\"\n",
        "        print(\"üîç Checking attention mechanisms...\")\n",
        "\n",
        "        attention_status = {}\n",
        "\n",
        "        # Check Triton\n",
        "        try:\n",
        "            import triton\n",
        "            attention_status['triton'] = f\"‚úÖ Triton {triton.__version__}\"\n",
        "        except ImportError:\n",
        "            attention_status['triton'] = \"‚ùå Triton not available\"\n",
        "\n",
        "        # Check SageAttention\n",
        "        try:\n",
        "            import sageattention\n",
        "            attention_status['sage'] = \"‚úÖ SageAttention available\"\n",
        "        except ImportError:\n",
        "            attention_status['sage'] = \"‚ùå SageAttention not available\"\n",
        "\n",
        "        # Check Flash Attention\n",
        "        try:\n",
        "            import flash_attn\n",
        "            attention_status['flash'] = \"‚úÖ Flash Attention available\"\n",
        "        except ImportError:\n",
        "            attention_status['flash'] = \"‚ùå Flash Attention not available\"\n",
        "\n",
        "        for mech, status in attention_status.items():\n",
        "            print(f\"  {status}\")\n",
        "\n",
        "        # Recommend fallback if advanced attention not available\n",
        "        if all(\"‚ùå\" in status for status in attention_status.values()):\n",
        "            self.issues_found.append(\"No advanced attention mechanisms available. Will use SDPA fallback.\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def check_memory_and_performance(self):\n",
        "        \"\"\"Check memory and performance configuration\"\"\"\n",
        "        print(\"üîç Checking memory and performance...\")\n",
        "\n",
        "        # Check available system RAM\n",
        "        try:\n",
        "            import psutil\n",
        "            ram_gb = psutil.virtual_memory().total / (1024**3)\n",
        "            print(f\"‚úÖ System RAM: {ram_gb:.1f}GB\")\n",
        "\n",
        "            if ram_gb < 16:\n",
        "                self.issues_found.append(f\"Low system RAM ({ram_gb:.1f}GB). 16GB+ recommended for best performance.\")\n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è Cannot check system RAM (psutil not available)\")\n",
        "\n",
        "        # Check disk space\n",
        "        try:\n",
        "            disk_usage = shutil.disk_usage(os.getcwd())\n",
        "            free_gb = disk_usage.free / (1024**3)\n",
        "            print(f\"‚úÖ Free disk space: {free_gb:.1f}GB\")\n",
        "\n",
        "            if free_gb < 20:\n",
        "                self.issues_found.append(f\"Low disk space ({free_gb:.1f}GB). 20GB+ recommended.\")\n",
        "        except Exception:\n",
        "            print(\"‚ö†Ô∏è Cannot check disk space\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def auto_repair_pytorch(self):\n",
        "        \"\"\"Auto-repair PyTorch installation\"\"\"\n",
        "        print(\"üîß Attempting PyTorch repair...\")\n",
        "\n",
        "        # Determine correct PyTorch version based on GPU\n",
        "        if \"50\" in self.gpu_info.get('name', ''):\n",
        "            # RTX 50XX series\n",
        "            pytorch_cmd = [\n",
        "                self.pip_cmd, \"install\", \"--upgrade\",\n",
        "                \"torch==2.7.0\", \"torchvision\", \"torchaudio\",\n",
        "                \"--index-url\", \"https://download.pytorch.org/whl/test/cu128\"\n",
        "            ]\n",
        "        else:\n",
        "            # RTX 10XX-40XX series\n",
        "            pytorch_cmd = [\n",
        "                self.pip_cmd, \"install\", \"--upgrade\",\n",
        "                \"torch==2.6.0\", \"torchvision\", \"torchaudio\",\n",
        "                \"--index-url\", \"https://download.pytorch.org/whl/test/cu124\"\n",
        "            ]\n",
        "\n",
        "        success, output = self.run_command_safely(pytorch_cmd, \"PyTorch Installation\", timeout=300)\n",
        "        if success:\n",
        "            self.repairs_applied.append(\"PyTorch installation repaired\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ùå PyTorch repair failed: {output}\")\n",
        "            return False\n",
        "\n",
        "    def auto_repair_dependencies(self):\n",
        "        \"\"\"Auto-repair missing dependencies\"\"\"\n",
        "        print(\"üîß Installing missing dependencies...\")\n",
        "\n",
        "        # Install core requirements\n",
        "        requirements_cmd = [self.pip_cmd, \"install\", \"-r\", \"requirements.txt\"]\n",
        "        success, output = self.run_command_safely(requirements_cmd, \"Dependencies Installation\", timeout=300)\n",
        "\n",
        "        if success:\n",
        "            self.repairs_applied.append(\"Dependencies installed\")\n",
        "            return True\n",
        "        else:\n",
        "            # Fallback: install critical packages individually\n",
        "            critical_packages = [\n",
        "                \"gradio>=4.0.0\", \"transformers\", \"accelerate\", \"diffusers\",\n",
        "                \"opencv-python\", \"Pillow\", \"numpy\", \"scipy\"\n",
        "            ]\n",
        "\n",
        "            for package in critical_packages:\n",
        "                cmd = [self.pip_cmd, \"install\", package]\n",
        "                success, _ = self.run_command_safely(cmd, f\"Installing {package}\", timeout=60)\n",
        "                if success:\n",
        "                    print(f\"‚úÖ Installed {package}\")\n",
        "\n",
        "            self.repairs_applied.append(\"Critical dependencies installed individually\")\n",
        "            return True\n",
        "\n",
        "    def auto_repair_wan2gp_repo(self):\n",
        "        \"\"\"Auto-repair WAN2GP repository\"\"\"\n",
        "        print(\"üîß Cloning WAN2GP repository...\")\n",
        "\n",
        "        repo_url = \"https://github.com/deepbeepmeep/Wan2GP.git\"\n",
        "        clone_cmd = [\"git\", \"clone\", \"--depth\", \"1\", repo_url]\n",
        "\n",
        "        success, output = self.run_command_safely(clone_cmd, \"Repository Clone\", timeout=120)\n",
        "        if success:\n",
        "            self.repairs_applied.append(\"WAN2GP repository cloned\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ùå Repository clone failed: {output}\")\n",
        "            return False\n",
        "\n",
        "    def auto_repair_attention_mechanisms(self):\n",
        "        \"\"\"Auto-repair attention mechanisms\"\"\"\n",
        "        print(\"üîß Installing performance optimizations...\")\n",
        "\n",
        "        repairs = []\n",
        "\n",
        "        # Install Triton for Windows\n",
        "        if os.name == 'nt':  # Windows\n",
        "            triton_cmd = [self.pip_cmd, \"install\", \"triton-windows\"]\n",
        "            success, _ = self.run_command_safely(triton_cmd, \"Triton Installation\", timeout=120)\n",
        "            if success:\n",
        "                repairs.append(\"Triton (Windows)\")\n",
        "\n",
        "        # Install SageAttention\n",
        "        sage_cmd = [self.pip_cmd, \"install\", \"sageattention>=1.0.6\"]\n",
        "        success, _ = self.run_command_safely(sage_cmd, \"SageAttention Installation\", timeout=120)\n",
        "        if success:\n",
        "            repairs.append(\"SageAttention\")\n",
        "\n",
        "        if repairs:\n",
        "            self.repairs_applied.append(f\"Installed: {', '.join(repairs)}\")\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def generate_optimized_launch_commands(self):\n",
        "        \"\"\"Generate optimized launch commands based on hardware\"\"\"\n",
        "        print(\"\\nüöÄ Generating optimized launch commands...\")\n",
        "\n",
        "        vram_gb = self.gpu_info.get('vram_gb', 8)\n",
        "        gpu_name = self.gpu_info.get('name', 'Unknown')\n",
        "\n",
        "        commands = {}\n",
        "\n",
        "        # Base command components\n",
        "        base_cmd = \"python wgp.py\"\n",
        "\n",
        "        if vram_gb < 8:\n",
        "            # Low VRAM setup\n",
        "            commands['Low VRAM (6-8GB)'] = f\"{base_cmd} --t2v-1-3B --attention sdpa --profile 4 --teacache 1.5\"\n",
        "        elif vram_gb < 12:\n",
        "            # Medium VRAM setup\n",
        "            commands['Medium VRAM (8-12GB)'] = f\"{base_cmd} --t2v-14B --attention sage --profile 4 --teacache 2.0\"\n",
        "        else:\n",
        "            # High VRAM setup\n",
        "            commands['High VRAM (12GB+)'] = f\"{base_cmd} --t2v-14B --attention sage2 --profile 3 --compile --teacache 2.0\"\n",
        "\n",
        "        # GPU-specific optimizations\n",
        "        if \"10\" in gpu_name or \"20\" in gpu_name:\n",
        "            commands['RTX 10XX/20XX Optimized'] = f\"{base_cmd} --attention sdpa --profile 4 --teacache 1.5\"\n",
        "        elif \"30\" in gpu_name or \"40\" in gpu_name:\n",
        "            commands['RTX 30XX/40XX Optimized'] = f\"{base_cmd} --compile --attention sage --profile 3 --teacache 2.0\"\n",
        "        elif \"50\" in gpu_name:\n",
        "            commands['RTX 50XX Optimized'] = f\"{base_cmd} --attention sage --profile 4 --fp16\"\n",
        "\n",
        "        # Fallback command\n",
        "        commands['Safe Fallback'] = f\"{base_cmd} --t2v-1-3B --attention sdpa --profile 4 --teacache 0 --fp16\"\n",
        "\n",
        "        # Debug command\n",
        "        commands['Debug Mode'] = f\"{base_cmd} --verbose 2 --check-loras --attention sdpa --profile 4\"\n",
        "\n",
        "        return commands\n",
        "\n",
        "    def run_full_diagnostic(self):\n",
        "        \"\"\"Run complete diagnostic and repair sequence\"\"\"\n",
        "        print(\"üè• WAN2GP Comprehensive Diagnostic and Auto-Repair System\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"üñ•Ô∏è Platform: {self.platform}\")\n",
        "        print(f\"üêç Python: {self.python_cmd}\")\n",
        "        print(f\"üì¶ Pip: {self.pip_cmd}\")\n",
        "        print(f\"üîß Virtual Environment: {'Yes' if self.use_venv else 'No'}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Run all diagnostics\n",
        "        checks = [\n",
        "            self.check_python_version,\n",
        "            self.check_gpu_compatibility,\n",
        "            self.check_cuda_pytorch_compatibility,\n",
        "            self.check_wan2gp_installation,\n",
        "            self.check_dependencies,\n",
        "            self.check_attention_mechanisms,\n",
        "            self.check_memory_and_performance\n",
        "        ]\n",
        "\n",
        "        print(\"\\nüìã Running Diagnostics...\")\n",
        "        for check in checks:\n",
        "            try:\n",
        "                check()\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Diagnostic error: {str(e)}\")\n",
        "                self.issues_found.append(f\"Diagnostic error: {str(e)}\")\n",
        "\n",
        "        # Auto-repair if issues found\n",
        "        if self.issues_found:\n",
        "            print(f\"\\n‚ö†Ô∏è Found {len(self.issues_found)} issues:\")\n",
        "            for i, issue in enumerate(self.issues_found, 1):\n",
        "                print(f\"  {i}. {issue}\")\n",
        "\n",
        "            print(f\"\\nüîß Attempting automatic repairs...\")\n",
        "\n",
        "            # Apply repairs based on issues found\n",
        "            if any(\"PyTorch\" in issue for issue in self.issues_found):\n",
        "                self.auto_repair_pytorch()\n",
        "\n",
        "            if any(\"dependencies\" in issue.lower() for issue in self.issues_found):\n",
        "                self.auto_repair_dependencies()\n",
        "\n",
        "            if any(\"repository\" in issue.lower() for issue in self.issues_found):\n",
        "                self.auto_repair_wan2gp_repo()\n",
        "\n",
        "            if any(\"attention\" in issue.lower() for issue in self.issues_found):\n",
        "                self.auto_repair_attention_mechanisms()\n",
        "\n",
        "        # Generate launch commands\n",
        "        commands = self.generate_optimized_launch_commands()\n",
        "\n",
        "        # Final report\n",
        "        self.display_final_report(commands)\n",
        "\n",
        "    def display_final_report(self, commands):\n",
        "        \"\"\"Display comprehensive final report\"\"\"\n",
        "\n",
        "        # Create styled HTML report\n",
        "        html_report = f\"\"\"\n",
        "        <div style=\"font-family: Arial, sans-serif; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "                    color: white; padding: 20px; border-radius: 10px; margin: 10px 0;\">\n",
        "            <h2>üè• WAN2GP Diagnostic Report</h2>\n",
        "            <div style=\"background-color: rgba(255,255,255,0.1); padding: 15px; border-radius: 8px; margin: 10px 0;\">\n",
        "                <h3>üìä System Status</h3>\n",
        "                <p><strong>Platform:</strong> {self.platform}</p>\n",
        "                <p><strong>GPU:</strong> {self.gpu_info.get('name', 'Unknown')} ({self.gpu_info.get('vram_gb', 0):.1f}GB VRAM)</p>\n",
        "                <p><strong>Issues Found:</strong> {len(self.issues_found)}</p>\n",
        "                <p><strong>Repairs Applied:</strong> {len(self.repairs_applied)}</p>\n",
        "            </div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        if self.repairs_applied:\n",
        "            html_report += f\"\"\"\n",
        "            <div style=\"background-color: #28a745; color: white; padding: 15px; border-radius: 8px; margin: 10px 0;\">\n",
        "                <h3>‚úÖ Repairs Applied Successfully:</h3>\n",
        "                <ul>\n",
        "            \"\"\"\n",
        "            for repair in self.repairs_applied:\n",
        "                html_report += f\"<li>{repair}</li>\"\n",
        "            html_report += \"</ul></div>\"\n",
        "\n",
        "        if self.issues_found and not self.repairs_applied:\n",
        "            html_report += f\"\"\"\n",
        "            <div style=\"background-color: #dc3545; color: white; padding: 15px; border-radius: 8px; margin: 10px 0;\">\n",
        "                <h3>‚ö†Ô∏è Unresolved Issues:</h3>\n",
        "                <ul>\n",
        "            \"\"\"\n",
        "            for issue in self.issues_found:\n",
        "                html_report += f\"<li>{issue}</li>\"\n",
        "            html_report += \"</ul></div>\"\n",
        "\n",
        "        display(HTML(html_report))\n",
        "\n",
        "        # Display optimized commands\n",
        "        print(\"\\nüöÄ Recommended Launch Commands:\")\n",
        "        print(\"=\" * 50)\n",
        "        for name, command in commands.items():\n",
        "            print(f\"\\n{name}:\")\n",
        "            print(f\"  {command}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"‚úÖ Diagnostic complete! Use the appropriate command above to launch WAN2GP.\")\n",
        "        if self.issues_found and not self.repairs_applied:\n",
        "            print(\"‚ö†Ô∏è  Some issues require manual intervention. Check the report above.\")\n",
        "\n",
        "# Execute diagnostic system\n",
        "diagnostic_system = WAN2GPDiagnosticAndRepair()\n",
        "diagnostic_system.run_full_diagnostic()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZzJeUGqaCLJf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "881c7a11-2b7f-4b87-e166-6054863cea67"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üè• WAN2GP Comprehensive Diagnostic and Auto-Repair System\n",
            "============================================================\n",
            "üñ•Ô∏è Platform: Google Colab\n",
            "üêç Python: .venv/bin/python\n",
            "üì¶ Pip: .venv/bin/pip\n",
            "üîß Virtual Environment: Yes\n",
            "============================================================\n",
            "\n",
            "üìã Running Diagnostics...\n",
            "üîç Checking Python version...\n",
            "‚úÖ Python version: 3.11.13\n",
            "üîç Checking GPU compatibility...\n",
            "‚úÖ GPU detected: 0  Tesla T4                       Off\n",
            "‚úÖ VRAM: 15.0GB\n",
            "üîç Checking CUDA and PyTorch compatibility...\n",
            "‚úÖ PyTorch version: 2.6.0+cu124\n",
            "‚úÖ CUDA available in PyTorch: True\n",
            "üîç Checking WAN2GP installation...\n",
            "‚úÖ WAN2GP found at: ../Wan2GP\n",
            "üîç Checking critical dependencies...\n",
            "‚úÖ PyTorch installed\n",
            "‚úÖ TorchVision installed\n",
            "‚úÖ Gradio installed\n",
            "‚úÖ Transformers installed\n",
            "‚úÖ Accelerate installed\n",
            "‚úÖ Diffusers installed\n",
            "üîç Checking attention mechanisms...\n",
            "  ‚úÖ Triton 3.2.0\n",
            "  ‚ùå SageAttention not available\n",
            "  ‚ùå Flash Attention not available\n",
            "üîç Checking memory and performance...\n",
            "‚úÖ System RAM: 12.7GB\n",
            "‚úÖ Free disk space: 17.8GB\n",
            "\n",
            "‚ö†Ô∏è Found 3 issues:\n",
            "  1. Python 3.11.13 detected. Python 3.10.9 recommended for best compatibility.\n",
            "  2. Low system RAM (12.7GB). 16GB+ recommended for best performance.\n",
            "  3. Low disk space (17.8GB). 20GB+ recommended.\n",
            "\n",
            "üîß Attempting automatic repairs...\n",
            "\n",
            "üöÄ Generating optimized launch commands...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <div style=\"font-family: Arial, sans-serif; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
              "                    color: white; padding: 20px; border-radius: 10px; margin: 10px 0;\">\n",
              "            <h2>üè• WAN2GP Diagnostic Report</h2>\n",
              "            <div style=\"background-color: rgba(255,255,255,0.1); padding: 15px; border-radius: 8px; margin: 10px 0;\">\n",
              "                <h3>üìä System Status</h3>\n",
              "                <p><strong>Platform:</strong> Google Colab</p>\n",
              "                <p><strong>GPU:</strong> 0  Tesla T4                       Off (15.0GB VRAM)</p>\n",
              "                <p><strong>Issues Found:</strong> 3</p>\n",
              "                <p><strong>Repairs Applied:</strong> 0</p>\n",
              "            </div>\n",
              "        </div>\n",
              "        \n",
              "            <div style=\"background-color: #dc3545; color: white; padding: 15px; border-radius: 8px; margin: 10px 0;\">\n",
              "                <h3>‚ö†Ô∏è Unresolved Issues:</h3>\n",
              "                <ul>\n",
              "            <li>Python 3.11.13 detected. Python 3.10.9 recommended for best compatibility.</li><li>Low system RAM (12.7GB). 16GB+ recommended for best performance.</li><li>Low disk space (17.8GB). 20GB+ recommended.</li></ul></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Recommended Launch Commands:\n",
            "==================================================\n",
            "\n",
            "High VRAM (12GB+):\n",
            "  python wgp.py --t2v-14B --attention sage2 --profile 3 --compile --teacache 2.0\n",
            "\n",
            "Safe Fallback:\n",
            "  python wgp.py --t2v-1-3B --attention sdpa --profile 4 --teacache 0 --fp16\n",
            "\n",
            "Debug Mode:\n",
            "  python wgp.py --verbose 2 --check-loras --attention sdpa --profile 4\n",
            "\n",
            "==================================================\n",
            "‚úÖ Diagnostic complete! Use the appropriate command above to launch WAN2GP.\n",
            "‚ö†Ô∏è  Some issues require manual intervention. Check the report above.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell (2) - Quick Diagnostic Runner and Emergency Repair Tools (Latest v6.2)\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "class QuickDiagnosticRunner:\n",
        "    def __init__(self):\n",
        "        self.platform = self.detect_platform()\n",
        "        self.pip_cmd, self.python_cmd = self.get_platform_commands()\n",
        "\n",
        "    def detect_platform(self):\n",
        "        \"\"\"Quick platform detection\"\"\"\n",
        "        if \"google.colab\" in sys.modules:\n",
        "            return \"Google Colab\"\n",
        "        elif any(indicator in str(sys.executable).lower() for indicator in [\"lightning\", \"teamspace\"]):\n",
        "            return \"Lightning AI\"\n",
        "        else:\n",
        "            return \"Generic/Vast.AI\"\n",
        "\n",
        "    def get_platform_commands(self):\n",
        "        \"\"\"Get platform-specific commands\"\"\"\n",
        "        if self.platform == \"Lightning AI\":\n",
        "            return \"pip\", \"python\"\n",
        "        else:\n",
        "            return \".venv/bin/pip\" if os.path.exists(\".venv\") else \"pip\", \".venv/bin/python\" if os.path.exists(\".venv\") else \"python\"\n",
        "\n",
        "    def run_cmd(self, cmd, timeout=30):\n",
        "        \"\"\"Execute command safely\"\"\"\n",
        "        try:\n",
        "            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout)\n",
        "            return result.returncode == 0, result.stdout, result.stderr\n",
        "        except subprocess.TimeoutExpired:\n",
        "            return False, \"\", \"Timeout\"\n",
        "        except Exception as e:\n",
        "            return False, \"\", str(e)\n",
        "\n",
        "    def emergency_pytorch_fix(self):\n",
        "        \"\"\"Emergency PyTorch installation fix\"\"\"\n",
        "        print(\"üö® Emergency PyTorch Fix...\")\n",
        "\n",
        "        # Detect GPU generation for correct PyTorch version\n",
        "        success, gpu_info, _ = self.run_cmd(\"nvidia-smi\")\n",
        "\n",
        "        if \"RTX 50\" in gpu_info:\n",
        "            pytorch_url = \"https://download.pytorch.org/whl/test/cu128\"\n",
        "            torch_version = \"torch==2.7.0\"\n",
        "        else:\n",
        "            pytorch_url = \"https://download.pytorch.org/whl/test/cu124\"\n",
        "            torch_version = \"torch==2.6.0\"\n",
        "\n",
        "        cmd = f\"{self.pip_cmd} install --upgrade {torch_version} torchvision torchaudio --index-url {pytorch_url}\"\n",
        "        success, stdout, stderr = self.run_cmd(cmd, timeout=300)\n",
        "\n",
        "        if success:\n",
        "            print(\"‚úÖ PyTorch emergency fix applied\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ùå PyTorch fix failed: {stderr}\")\n",
        "            return False\n",
        "\n",
        "    def emergency_dependency_fix(self):\n",
        "        \"\"\"Emergency dependency installation\"\"\"\n",
        "        print(\"üö® Emergency Dependency Fix...\")\n",
        "\n",
        "        essential_packages = [\n",
        "            \"gradio>=4.0.0\", \"transformers\", \"accelerate\", \"diffusers\",\n",
        "            \"opencv-python\", \"Pillow\", \"numpy\", \"scipy\", \"psutil\"\n",
        "        ]\n",
        "\n",
        "        for package in essential_packages:\n",
        "            cmd = f\"{self.pip_cmd} install {package}\"\n",
        "            success, _, _ = self.run_cmd(cmd, timeout=60)\n",
        "            print(\"‚úÖ\" if success else \"‚ùå\", package)\n",
        "\n",
        "        print(\"‚úÖ Emergency dependencies installed\")\n",
        "\n",
        "    def quick_system_check(self):\n",
        "        \"\"\"Quick system health check\"\"\"\n",
        "        checks = {}\n",
        "\n",
        "        # GPU Check\n",
        "        success, output, _ = self.run_cmd(\"nvidia-smi\")\n",
        "        checks['GPU'] = \"‚úÖ Available\" if success and \"RTX\" in output else \"‚ùå Issue detected\"\n",
        "\n",
        "        # PyTorch Check\n",
        "        try:\n",
        "            import torch\n",
        "            checks['PyTorch'] = f\"‚úÖ {torch.__version__}\" if torch.cuda.is_available() else \"‚ùå CUDA not available\"\n",
        "        except ImportError:\n",
        "            checks['PyTorch'] = \"‚ùå Not installed\"\n",
        "\n",
        "        # WAN2GP Check\n",
        "        wan_exists = any(os.path.exists(path) for path in [\"Wan2GP/wgp.py\", \"wan2gp/wgp.py\", \"WAN2GP/wgp.py\"])\n",
        "        checks['WAN2GP'] = \"‚úÖ Found\" if wan_exists else \"‚ùå Missing\"\n",
        "\n",
        "        # Dependencies Check\n",
        "        try:\n",
        "            import gradio, transformers, accelerate, diffusers\n",
        "            checks['Dependencies'] = \"‚úÖ Core packages available\"\n",
        "        except ImportError:\n",
        "            checks['Dependencies'] = \"‚ùå Missing packages\"\n",
        "\n",
        "        return checks\n",
        "\n",
        "    def generate_emergency_commands(self):\n",
        "        \"\"\"Generate emergency launch commands\"\"\"\n",
        "        commands = {\n",
        "            \"Ultra Safe Mode\": f\"{self.python_cmd} wgp.py --t2v-1-3B --attention sdpa --profile 4 --teacache 0 --fp16\",\n",
        "            \"Memory Emergency\": f\"{self.python_cmd} wgp.py --t2v-1-3B --profile 5 --perc-reserved-mem-max 0.2\",\n",
        "            \"Debug Mode\": f\"{self.python_cmd} wgp.py --verbose 2 --attention sdpa --profile 4\",\n",
        "            \"Network Share\": f\"{self.python_cmd} wgp.py --listen --server-port 7861 --attention sdpa\"\n",
        "        }\n",
        "        return commands\n",
        "\n",
        "    def run_quick_diagnostic(self):\n",
        "        \"\"\"Run quick diagnostic and provide emergency options\"\"\"\n",
        "\n",
        "        print(f\"‚ö° Quick Diagnostic - Platform: {self.platform}\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "        # Quick system check\n",
        "        checks = self.quick_system_check()\n",
        "\n",
        "        issues = []\n",
        "        for component, status in checks.items():\n",
        "            print(f\"{component}: {status}\")\n",
        "            if \"‚ùå\" in status:\n",
        "                issues.append(component)\n",
        "\n",
        "        # Emergency repairs\n",
        "        if issues:\n",
        "            print(f\"\\nüö® {len(issues)} issues detected. Applying emergency fixes...\")\n",
        "\n",
        "            if \"PyTorch\" in issues:\n",
        "                self.emergency_pytorch_fix()\n",
        "\n",
        "            if \"Dependencies\" in issues:\n",
        "                self.emergency_dependency_fix()\n",
        "\n",
        "            if \"WAN2GP\" in issues:\n",
        "                print(\"üîß Cloning WAN2GP repository...\")\n",
        "                success, _, _ = self.run_cmd(\"git clone --depth 1 https://github.com/deepbeepmeep/Wan2GP.git\", timeout=120)\n",
        "                print(\"‚úÖ Repository cloned\" if success else \"‚ùå Clone failed\")\n",
        "\n",
        "        # Generate emergency commands\n",
        "        commands = self.generate_emergency_commands()\n",
        "\n",
        "        # Display results\n",
        "        html_display = f\"\"\"\n",
        "        <div style=\"background: linear-gradient(45deg, #ff6b6b, #feca57); color: white;\n",
        "                    padding: 15px; border-radius: 8px; margin: 10px 0;\">\n",
        "            <h3>‚ö° Quick Diagnostic Results</h3>\n",
        "            <p><strong>Platform:</strong> {self.platform}</p>\n",
        "            <p><strong>Issues Found:</strong> {len(issues)}</p>\n",
        "            <p><strong>Status:</strong> {'üö® Needs Attention' if issues else '‚úÖ System Ready'}</p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        display(HTML(html_display))\n",
        "\n",
        "        print(\"\\nüöÄ Emergency Launch Commands:\")\n",
        "        print(\"-\" * 40)\n",
        "        for name, command in commands.items():\n",
        "            print(f\"\\n{name}:\")\n",
        "            print(f\"  {command}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 40)\n",
        "        print(\"‚ö° Quick diagnostic complete!\")\n",
        "\n",
        "        if not issues:\n",
        "            print(\"‚úÖ System appears healthy. Try the Ultra Safe Mode command first.\")\n",
        "        else:\n",
        "            print(\"üö® Emergency fixes applied. Test with Ultra Safe Mode.\")\n",
        "\n",
        "# Execute quick diagnostic\n",
        "quick_runner = QuickDiagnosticRunner()\n",
        "quick_runner.run_quick_diagnostic()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0sx8G7LBCNyw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "6c00c143-2c6b-4944-8505-49ffbf2955ed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö° Quick Diagnostic - Platform: Google Colab\n",
            "========================================\n",
            "GPU: ‚ùå Issue detected\n",
            "PyTorch: ‚úÖ 2.6.0+cu124\n",
            "WAN2GP: ‚ùå Missing\n",
            "Dependencies: ‚úÖ Core packages available\n",
            "\n",
            "üö® 2 issues detected. Applying emergency fixes...\n",
            "üîß Cloning WAN2GP repository...\n",
            "‚úÖ Repository cloned\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <div style=\"background: linear-gradient(45deg, #ff6b6b, #feca57); color: white;\n",
              "                    padding: 15px; border-radius: 8px; margin: 10px 0;\">\n",
              "            <h3>‚ö° Quick Diagnostic Results</h3>\n",
              "            <p><strong>Platform:</strong> Google Colab</p>\n",
              "            <p><strong>Issues Found:</strong> 2</p>\n",
              "            <p><strong>Status:</strong> üö® Needs Attention</p>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Emergency Launch Commands:\n",
            "----------------------------------------\n",
            "\n",
            "Ultra Safe Mode:\n",
            "  .venv/bin/python wgp.py --t2v-1-3B --attention sdpa --profile 4 --teacache 0 --fp16\n",
            "\n",
            "Memory Emergency:\n",
            "  .venv/bin/python wgp.py --t2v-1-3B --profile 5 --perc-reserved-mem-max 0.2\n",
            "\n",
            "Debug Mode:\n",
            "  .venv/bin/python wgp.py --verbose 2 --attention sdpa --profile 4\n",
            "\n",
            "Network Share:\n",
            "  .venv/bin/python wgp.py --listen --server-port 7861 --attention sdpa\n",
            "\n",
            "========================================\n",
            "‚ö° Quick diagnostic complete!\n",
            "üö® Emergency fixes applied. Test with Ultra Safe Mode.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}